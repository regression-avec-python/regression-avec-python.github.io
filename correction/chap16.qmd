---
title: "16 Données déséquilibrées"
toc: true
---

::: {.content-hidden}
{{< include ../macros.tex >}}
:::

```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
rng = np.random.default_rng(seed=1234)
```

::: {#exr-15-1 name="Critères pour un exemple de données déséquilibrées"}

1.  

    ```{python}
    n = 500
    p = 0.05
    Y = rng.binomial(1, p=p, size=n)
    ```

2.  
    ```{python}
    rng = np.random.default_rng(seed=123)
    P1 = rng.binomial(1, p=0.005, size=n)
    ```


3.  
    ```{python}
    P2 = np.zeros_like(P1)
    for yy in range(n):
        if Y[yy]==0:
            P2[yy] = rng.binomial(1, p=0.10, size=1)[0]
        else:
            P2[yy] = rng.binomial(1, p=0.85, size=1)[0]
    ```

4.  
    ```{python}
    from sklearn.metrics import confusion_matrix
    print(confusion_matrix(Y, P1))
    ```
    ```{python}
    print(confusion_matrix(Y, P2))
    ```


5.  
    ```{python}
    cm = confusion_matrix(Y, P2)
    acc = cm.diagonal().sum()/cm.sum()
    rec = cm[1,1]/cm[1,:].sum()
    prec = cm[1,1]/cm[:,1].sum()
    print(acc)
    print(rec)
    print(prec)
    ```


6.  
    ```{python}
    F1 = 2*(rec*prec)/(rec+prec)
    print(F1)
    rand = cm[:,0].sum()/n*cm[0,:].sum()/n + cm[:,1].sum()/n*cm[1,:].sum()/n
    kappa = (acc-rand)/(1-rand)
    print(kappa)
    ```

7.  
    ```{python}
    from sklearn.metrics import accuracy_score, recall_score, precision_score
    from sklearn.metrics import f1_score, cohen_kappa_score
    print(accuracy_score(Y, P2), "**", accuracy_score(Y, P1))
    print(recall_score(Y, P2), "**", recall_score(Y, P1))
    print(precision_score(Y, P2), "**", precision_score(Y, P1))
    print(f1_score(Y, P2), "**", f1_score(Y, P1))
    print(cohen_kappa_score(Y, P2), "**", cohen_kappa_score(Y, P1))
    ```

:::


::: {#exr-15-2 name="Échantillonnage rétrospectif"}
On remarque d'abord que $\prob(\tilde y_i=1)=\prob(y_i=1|s_i=1)$. De plus
$$
\logit\, p_\beta(x_i)=\log\frac{\prob(y_i=1)}{\prob(y_i=0)}\quad\text{et}\quad \logit\, p_\gamma(x_i)=\log\frac{\prob(y_i=1|s_i=1)}{\prob(y_i=0|s_i=1)}.
$$
Or
$$
\prob(y_i=1|s_i=1)=\frac{\prob(y_i=1,s_i=1)}{\prob(s_i=1)}=\frac{\prob(s_i=1|y_i=1)\prob(y_i=1)}{\prob(s_i=1)}
$$
et
$$
\prob(y_i=0|s_i=1)=\frac{\prob(y_i=0,s_i=1)}{\prob(s_i=1)}=\frac{\prob(s_i=1|y_i=0)\prob(y_i=0)}{\prob(s_i=1)}.
$$
Donc
$$
\logit\, p_\gamma(x_i)=\log\frac{\prob(y_i=1)}{\prob(y_i=0)}+\log\frac{\prob(s_i=1|y_i=1)}{\prob(s_i=1|y_i=0)}=\logit\,p_\beta(x_i)+\log\left(\frac{\tau_{1i}}{\tau_{0i}}\right).
$$

:::

::: {#exr-15-3 name="Rééquilibrage"}

1.  
    ```{python}
    df1 = pd.read_csv("../donnees/dd_exo3_1.csv", header=0, sep=',')
    df2 = pd.read_csv("../donnees/dd_exo3_2.csv", header=0, sep=',')
    df3 = pd.read_csv("../donnees/dd_exo3_3.csv", header=0, sep=',')
    ```

    ```{python}
    print(df1.describe())
    print(df2.describe())
    print(df3.describe())
    ```

    ```{python}
    colo = ["C1", "C2"]
    mark = ["o", "d"]
    for yy in [0, 1]:
        plt.scatter(df1.loc[df1.Y==yy, "X1"], df1.loc[df1.Y==yy, "X2"], color=colo[yy], marker=mark[yy])
    ```

    ```{python}
    for yy in [0, 1]:
        plt.scatter(df2.loc[df2.Y==yy, "X1"], df2.loc[df2.Y==yy, "X2"], color=colo[yy], marker=mark[yy])
    ```

    ```{python}
    for yy in [0, 1]:
        plt.scatter(df3.loc[df3.Y==yy, "X1"], df3.loc[df3.Y==yy, "X2"], color=colo[yy], marker=mark[yy])
    ```

2.  
    ```{python}
    from sklearn.model_selection import train_test_split
    ## separation en matrice X, Y (et creation du produit=interaction)
    T1 = df1.drop(columns="Y")
    X1 = T1.assign(inter= T1.X1 * T1.X2).to_numpy()
    y1 = df1.Y.to_numpy()
    T2 = df2.drop(columns="Y")
    X2 = T2.assign(inter= T2.X1 * T2.X2).to_numpy()
    y2 = df2.Y.to_numpy()
    T3 = df3.drop(columns="Y")
    X3 = T3.assign(inter= T3.X1 * T3.X2).to_numpy()
    y3 = df3.Y.to_numpy()
    ## separation apprentissage/validation
    X1_app, X1_valid, y1_app, y1_valid = train_test_split(
        X1, y1, test_size=0.33, random_state=1234)
    X2_app, X2_valid, y2_app, y2_valid = train_test_split(
        X2, y2, test_size=0.33, random_state=1234)
    X3_app, X3_valid, y3_app, y3_valid = train_test_split(
        X3, y3, test_size=0.33, random_state=1234)
    ```

3.  
    ```{python}
    from sklearn.linear_model import LogisticRegression
    mod1 = LogisticRegression(penalty=None, solver="newton-cholesky").fit(X1_app, y1_app)
    mod2 = LogisticRegression(penalty=None, solver="newton-cholesky").fit(X2_app, y2_app)
    mod3 = LogisticRegression(penalty=None, solver="newton-cholesky").fit(X3_app, y3_app)
    ```

    ```{python}
    from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, cohen_kappa_score
    P1 = mod1.predict(X1_valid)
    P2 = mod1.predict(X2_valid)
    P3 = mod1.predict(X3_valid)
    s1 = pd.DataFrame({"crit": ["acc", "bal_acc", "F1", "Kappa"]})
    s2 = pd.DataFrame({"crit": ["acc", "bal_acc", "F1", "Kappa"]})
    s3 = pd.DataFrame({"crit": ["acc", "bal_acc", "F1", "Kappa"]})
    print("--- donnees 1 ---")
    s1 = s1.assign(brut=0.0)
    s1.iloc[0,1] = accuracy_score(y1_valid, P1)
    s1.iloc[1,1] = balanced_accuracy_score(y1_valid, P1)
    s1.iloc[2,1] = f1_score(y1_valid, P1)
    s1.iloc[3,1] = cohen_kappa_score(y1_valid, P1)
    print(s1)
    print("--- donnees 2 ---")
    s2 = s2.assign(brut=0.0)
    s2.iloc[0,1] = accuracy_score(y2_valid, P2)
    s2.iloc[1,1] = balanced_accuracy_score(y2_valid, P2)
    s2.iloc[2,1] = f1_score(y2_valid, P2)
    s2.iloc[3,1] = cohen_kappa_score(y2_valid, P2)
    print(s2)
    print("--- donnees 3 ---")
    s3 = s3.assign(brut=0.0)
    s3.iloc[0,1] = accuracy_score(y3_valid, P3)
    s3.iloc[1,1] = balanced_accuracy_score(y3_valid, P3)
    s3.iloc[2,1] = f1_score(y3_valid, P3)
    s3.iloc[3,1] = cohen_kappa_score(y3_valid, P3)
    print(s3)
    ```

4.  
    ```{python}
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.under_sampling import TomekLinks
    from imblearn.over_sampling import RandomOverSampler
    from imblearn.over_sampling import SMOTE
    ## RandomOverSampler
    ros3 = RandomOverSampler(random_state=123)
    X3_app_reech, y3_app_reech = ros3.fit_resample(X3_app, y3_app)
    mod3_ros = LogisticRegression(penalty=None, solver="newton-cholesky").fit(X3_app_reech, y3_app_reech)
    ## Smote
    sm = RandomOverSampler(random_state=123)
    X3_app_reech, y3_app_reech = sm.fit_resample(X3_app, y3_app)
    mod3_sm = LogisticRegression(penalty=None, solver="newton-cholesky").fit(X3_app_reech, y3_app_reech)
    ## RandomUnderSampler
    rus3 = RandomUnderSampler(random_state=123)
    X3_app_reech, y3_app_reech = rus3.fit_resample(X3_app, y3_app)
    mod3_rus = LogisticRegression(penalty=None, solver="newton-cholesky").fit(X3_app_reech, y3_app_reech)
    ## Tomek
    tl = TomekLinks(sampling_strategy='all')
    X3_app_reech, y3_app_reech = tl.fit_resample(X3_app, y3_app)
    mod3_tl = LogisticRegression(penalty=None, solver="newton-cholesky").fit(X3_app_reech, y3_app_reech)
    ```

    ```{python}
    P3_ros = mod3_ros.predict(X3_valid)
    P3_sm = mod3_sm.predict(X3_valid)
    P3_rus = mod3_rus.predict(X3_valid)
    P3_tl = mod3_tl.predict(X3_valid)
    ```

    ```{python}
    s3 = s3.assign(ros=[accuracy_score(y3_valid, P3_ros),
    balanced_accuracy_score(y3_valid, P3_ros),
    f1_score(y3_valid, P3_ros),
    cohen_kappa_score(y3_valid, P3_ros)])
    s3 = s3.assign(sm=[accuracy_score(y3_valid, P3_sm),
    balanced_accuracy_score(y3_valid, P3_sm),
    f1_score(y3_valid, P3_sm),
    cohen_kappa_score(y3_valid, P3_sm)])
    s3 = s3.assign(rus=[accuracy_score(y3_valid, P3_rus),
    balanced_accuracy_score(y3_valid, P3_rus),
    f1_score(y3_valid, P3_rus),
    cohen_kappa_score(y3_valid, P3_rus)])
    s3 = s3.assign(tl=[accuracy_score(y3_valid, P3_tl),
    balanced_accuracy_score(y3_valid, P3_tl),
    f1_score(y3_valid, P3_tl),
    cohen_kappa_score(y3_valid, P3_tl)])
    print(s3)
    ```

:::

