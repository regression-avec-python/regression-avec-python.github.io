---
title: "1 La régression linéaire simple"
toc: true
---


::: {.content-hidden}
{{< include ../macros.tex >}}
:::

::: {#exr-1-1 name="Questions de cours"}
B, A, B, A.
:::

::: {#exr-1-2 name="Biais des estimateurs"}
Les $\hat \beta_j$ sont fonctions de $Y$ (aléatoire), ce sont donc des 
variables aléatoires. Une autre façon d'écrire $\hat \beta_2$ 
en fonction de $\beta_2$ consiste à remplacer $y_i$ 
par sa valeur soit 
\begin{eqnarray*}
\hat \beta_2 &=&\frac{\sum (x_i - \bar x) y_i}{\sum(x_i-\bar x)^2}
=\frac{\beta_1\sum (x_i - \bar x)+\beta_2\sum x_i(x_i - \bar x)+
\sum (x_i - \bar x) \varepsilon_i }{\sum(x_i-\bar x)^2} \\
&=& \beta_2 + \frac{\sum (x_i - \bar x) \varepsilon_i}{\sum(x_i-\bar x)^2}.
\end{eqnarray*}
Par hypothèse $\E(\varepsilon_i)=0$, les autres termes ne sont 
pas aléatoires, le résultat est démontré.


Le résultat est identique pour $\hat \beta_1$ car $\E (\hat \beta_1) =
\E (\bar y) -\bar x \E (\hat \beta_2)= \beta_1 +\bar x \beta_2 - \bar
x \beta_2=\beta_1$, le résultat est démontré.

:::

::: {#exr-1-3 name="Variance des estimateurs"}
Nous avons
\begin{eqnarray*}
\V(\hat \beta_2) 
&=& 
\V\left(\beta_2+\frac{\sum(x_i-\bar x)\varepsilon_i}
{\sum(x_i-\bar x)^2}\right).
\end{eqnarray*}
Or $\beta_2$ est inconnu mais pas aléatoire et les $x_i$ ne sont pas
aléatoires donc 
\begin{eqnarray*}
\V(\hat \beta_2) 
&=&\V\left(\frac{\sum(x_i-\bar x)\varepsilon_i}{\sum(x_i-\bar x)^2}\right)
=\frac{\V\left(\sum(x_i-\bar x)\varepsilon_i\right)}{\left[\sum(x_i-\bar x)^2\right]^2}\\
&=&\frac{\sum_{i,j}(x_i-\bar x)(x_j-\bar x)\C(\varepsilon_i,\varepsilon_j)}{\left[\sum(x_i-\bar x)^2\right]^2}.
\end{eqnarray*}
Or $\C(\varepsilon_i,\varepsilon_j)=\delta_{ij}\sigma^2$ donc
\begin{eqnarray*}
\V(\hat \beta_2) 
&=&\frac{\sum_i(x_i-\bar x)^2\sigma^2}{\left[\sum_i(x_i-\bar x)^2\right]^2}
=\frac{\sigma^2}{\sum(x_i-\bar x)^2}.
\end{eqnarray*}
Plus les mesures $x_i$ sont dispersées autour de leur moyenne, plus
$\V(\hat \beta_2)$ est faible et plus l'estimation est
précise. Bien sûr, plus $\sigma^2$ est faible, c'est-à-dire plus les
$y_i$ sont proches de la droite inconnue, plus l'estimation est précise.\
Puisque $\hat \beta_1=\bar y - \hat \beta_2 \bar x$, nous avons
\begin{eqnarray*}
\V(\hat \beta_1) 
&=& 
\V \left(\bar y-\hat \beta_2 \bar x \right)=\V \left(\bar y\right)+V(\bar x \hat \beta_2)-2\C(\bar y,\hat \beta_2 \bar x)\\
&=&\V \left(\frac{\sum y_i}{n}\right)+\bar x^2\frac{\sigma^2}{\sum(x_i-\bar x)^2}-2 \bar x\C(\bar y,\hat \beta_2)\\
&=&\frac{\sigma^2}{n}+\bar x^2\frac{\sigma^2}{\sum(x_i-\bar x)^2}-2 \bar x\sum_i\C(\bar y,\hat \beta_2).
\end{eqnarray*}

Calculons 
\begin{eqnarray*}
\C(\bar y, \hat \beta_2) 
&=& \frac{1}{n}
\C\left(\sum_{i}\left(\beta_1+\beta_2 x_i+\varepsilon_i\right),\frac{\sum_j(x_j-\bar x)\varepsilon_j}
{\sum_j(x_j-\bar x)^2}\right)\\
&=&\frac{1}{n}\sum_{i}\C\left(\varepsilon_i,\frac{\sum_j(x_j-\bar x)\varepsilon_j}
{\sum_j(x_j-\bar x)^2}\right)\\
&=&\frac{1}{\sum_j(x_j-\bar x)^2}
\sum_{i}\frac{1}{n}\C\left(\varepsilon_i,\sum_j(x_j-\bar x)\varepsilon_j\right)\\
&=&\frac{\sigma^2 \frac{1}{n}\sum_{i}(x_i-\bar x)}{\sum_j(x_j-\bar x)^2}=0.
\end{eqnarray*}
Nous avons donc 
\begin{eqnarray*}
\V(\hat \beta_1) 
&=& \frac{\sigma^2}{n}+\bar x^2\frac{\sigma^2}{\sum(x_i-\bar x)^2}
=\frac{\sigma^2 \sum x_i^2}{n\sum(x_i-\bar x)^2}.
\end{eqnarray*}
Là encore, plus $\sigma^2$ est faible, c'est-à-dire plus les
$y_i$ sont proches de la droite inconnue, plus l'estimation est précise.
Plus les valeurs $x_i$ sont dispersées autour de leur moyenne, plus
la variance de l'estimateur sera faible. De même, une faible moyenne $\bar x$
en valeur absolue contribue à bien estimer $\beta_1$. 

:::

::: {#exr-1-4 name="Covariance de $\hat\beta_1$ et $\hat\beta_2$"}
Nous avons
\begin{eqnarray*}
\C(\hat \beta_1,\hat \beta_2) &=&\C(\bar y-\hat \beta_2 \bar x,\hat \beta_2)
= \C(\bar y,\hat \beta_2)- \bar x \V(\hat \beta_2)=
 -\frac{\sigma^2 \bar x}{\sum(x_i-\bar x)^2}.
\end{eqnarray*}
La covariance entre $\beta_1$ et $\beta_2$ est négative.  L'équation
$\bar y=\hat \beta_1+\hat \beta_2\bar x$ indique que la droite des MC
passe par le centre de gravité du nuage $(\bar x, \bar y)$. Supposons
$\bar x$ positif, nous voyons bien que, si nous augmentons la pente,
l'ordonnée à l'origine va diminuer et vice versa. Nous retrouvons donc
le signe négatif pour la covariance entre $\hat \beta_1$ et $\hat
\beta_2$.
:::

::: {#exr-1-5 name="Théorème de Gauss-Markov"}
L'estimateur des MC s'écrit $\hat \beta_2 = \sum_{i=1}^n p_i y_i,$
avec $p_i=(x_i-\bar x)/\sum(x_i -\bar x)^2$. 

Considérons un autre estimateur $\tilde{\beta_2}$ linéaire en
$y_i$ et sans biais, c'est-à-dire  
$$\tilde{\beta_2} =\sum_{i=1}^n \lambda_i y_i.$$ 


Montrons que $\sum \lambda_i=0$ et $\sum \lambda_i x_i=1$. 
L'égalité $\E (\tilde{\beta_2}) = \beta_1 \sum \lambda_i +
\beta_2 \sum \lambda_i x_i +  \sum \lambda_i \E (\varepsilon_i)$ 
est vraie pour tout $\beta_2$ et $\tilde \beta_2$ est sans biais
donc $\E(\tilde \beta_2)=\beta_2$ pour tout $\beta_2$, c'est-à-dire 
que $\sum \lambda_i=0$ et $\sum \lambda_i x_i=1$. 


Montrons que $\V(\tilde{\beta_2}) \geq \V(\hat
\beta_2)$.
\begin{eqnarray*}
\V(\tilde{\beta_2}) = \V(\tilde{\beta_2}- \hat \beta_2 + \hat \beta_2)
=\V(\tilde{\beta_2}- \hat \beta_2)+\V(\hat \beta_2)+
2\C(\tilde{\beta_2}- \hat \beta_2,\hat \beta_2).
\end{eqnarray*}
\begin{eqnarray*}
\C(\tilde{\beta_2}- \hat \beta_2,\hat \beta_2)
\!=\!\C(\tilde{\beta_2},\hat \beta_2) -\V(-\hat \beta_2)
\!=\!\frac{\sigma^2\sum \lambda_i(x_i-\bar x)}{\sum (x_i-\bar x)^2} - 
\frac{\sigma^2}{\sum (x_i-\bar x)^2} 
\!=\!0,
\end{eqnarray*}
et donc 
\begin{eqnarray*}
\V(\tilde{\beta_2}) =
\V(\tilde{\beta_2}- \hat \beta_2)+\V(\hat \beta_2).
\end{eqnarray*}
Une variance est toujours positive et donc
\begin{eqnarray*}
\V(\tilde{\beta_2}) \geq \V(\hat \beta_2).
\end{eqnarray*}
Le résultat est démontré. On obtiendrait la même chose pour 
$\hat \beta_1$.
:::

::: {#exr-1-6 name="Somme des résidus"}
Il suffit de remplacer les résidus par leur 
définition et de remplacer $\hat \beta_1$ par son expression 
\begin{eqnarray*}
\sum_i \hat \varepsilon_i 
= \sum_i (y_i - \bar y + \hat \beta_2 \bar x - \hat \beta_2 x_i)
= \sum_i (y_i-\bar y) - \hat \beta_2 \sum_i (x_i - \bar x)= 0.
\end{eqnarray*}
:::

::: {#exr-1-7 name="Estimateur de la variance du bruit"}
Récrivons les résidus en constatant que $\hat \beta_1= \bar y- \hat \beta_2 \bar x$ et $\beta_1=\bar y - \beta_2 \bar x - \bar \varepsilon$,
\begin{eqnarray*}
\hat \varepsilon_i&=& \beta_1 + \beta_2 x_i +\varepsilon_i - \hat \beta_1 - \hat \beta_2x_i\\
&=& \bar{y} - \beta_2 \bar{x} - \bar{\varepsilon} + \beta_2 x_i +\varepsilon_i -\bar{y} + \hat \beta_2 \bar{x} - \hat \beta_2x_i\\
&=& (\beta_2-\hat \beta_2)(x_i -\bar{x}) + (\varepsilon_i - \bar{\varepsilon}).
\end{eqnarray*}
En développant et en nous servant de l'écriture de $\hat \beta_2$ donnée 
dans la solution de l'@exr-1-2, nous avons 
\begin{eqnarray*}
\sum \hat \varepsilon_i^2& \!=\!& (\beta_2-\hat \beta_2)^2 \!\sum  \!(x_i \!-\!\bar{x})^2
\!+\!\sum \!(\varepsilon_i \!-\! \bar{\varepsilon})^2\!+\!2 \!(\beta_2\!-\!\hat \beta_2)
\!\sum \!(x_i \!-\!\bar{x})(\varepsilon_i \!- \!\bar{\varepsilon})\\
&=& (\beta_2-\hat \beta_2)^2 \sum  (x_i -\bar{x})^2
+\sum (\varepsilon_i - \bar{\varepsilon})^2 - 2 (\beta_2-\hat \beta_2)^2 \sum  (x_i -\bar{x})^2.
\end{eqnarray*}
Prenons en l'espérance
\begin{eqnarray*}
\E \left( \sum \hat{\varepsilon_i}^2\right)= \E  \left(\sum (\varepsilon_i - \bar{\varepsilon})^2\right) -
\sum  (x_i -\bar{x})^2 \V(\hat \beta_2)
= (n-2) \sigma^2.
\end{eqnarray*}
:::

::: {#exr-1-8 name="Prévision"}
Calculons la variance 
\begin{eqnarray*}
\V\left(\hat y^p_{n+1}\right)
&=&\V\left(\hat \beta_1 + \hat \beta_2x_{n+1}\right)
=\V(\hat \beta_1)+x_{n+1}^2 \V(\hat \beta_2)
+2 x_{n+1} \C\left(\hat \beta_1,\hat \beta_2\right)\\
&=& \frac{\sigma^2}{\sum(x_i-\bar x)^2}\left(
\frac{\sum x_i^2}{n}+x_{n+1}^2-2 x_{n+1}\bar x \right)\\
&=& \frac{\sigma^2}{\sum(x_i-\bar x)^2}\left(
\frac{\sum (x_i-\bar x)^2}{n}+\bar x^2 + x_{n+1}^2-2 x_{n+1}\bar x \right)\\
&=& \sigma^2\left(\frac{1}{n}+
\frac{(x_{n+1}-\bar x)^2}{\sum (x_i-\bar x)^2}\right).
\end{eqnarray*}
Plus la valeur à prévoir s'éloigne du centre de gravité, plus la
valeur prévue sera variable (i.e. de variance élevée).


\textbullet Variance de l'erreur de prévision\\
Nous obtenons la variance de l'erreur de 
prévision en nous servant du fait que $y_{n+1}$ est fonction de 
$\varepsilon_{n+1}$ seulement, alors que $\hat y^p_{n+1}$ est fonction 
des autres $\varepsilon_i$, $i=1,\cdots,n$. Les deux quantités 
ne sont pas corrélées. Nous avons alors
\begin{eqnarray*}
\V(\hat \varepsilon_{n+1}^p) \!=\! \V\left(y_{n+1} \!-\! \hat y_{n+1}^p\right) 
\!=\! \V(y_{n+1})\!+\!\V(\hat y_{n+1}^p)\!=\! \sigma^2\left(1+\frac{1}{n}
\!+\!\frac{(x_{n+1}-\bar x)^2}{\sum (x_i-\bar x)^2}\right).
\end{eqnarray*}

:::

::: {#exr-1-9 name="$R^2$ et coefficient de corrélation"}
Le coefficient $\leR$ s'écrit
\begin{eqnarray*}
\leR&=&%\frac{\|\hat{Y} -\bar{y}\1\|^2}{\|Y-\bar{y}\1\|^2}=
\frac{\sum_{i=1}^{n}{\left(\hat \beta_1 + \hat \beta_2 x_i - \bar{y}\right)^2}}{
\sum_{i=1}^{n}{\left(y_i-\bar{y}\right)^2}}=
\frac{\sum_{i=1}^{n}{\left( \bar{y}-\hat \beta_2 \bar{x}+ \hat \beta_2 x_i - \bar{y}\right)^2}}{
{\sum_{i=1}^{n}(y_i-\bar{y})^2}}\\
&=&
\frac{\hat \beta_2^2\sum_{i=1}^{n}{\left( x_i - \bar{x}\right)^2}}{
\sum_{i=1}^{n}\left(y_i-\bar{y}\right)^2}
=\frac{
\left[\sum_{i=1}^{n}{( x_i - \bar{x})(y_i-\bar{y})}\right]^2
\sum_{i=1}^{n}{( x_i - \bar{x})^2}}{
\left[\sum_{i=1}^{n}{( x_i - \bar{x})^2}\right]^2
\sum_{i=1}^{n}(y_i-\bar{y})^2}\\
&=&\frac{\left[
\sum_{i=1}^{n}{( x_i - \bar{x})(y_i-\bar{y})}\right]^2}{
\sum_{i=1}^{n}{( x_i - \bar{x})^2}\sum_{i=1}^{n}{(y_i-\bar{y})^2}}=\rho^2(X,Y).
\end{eqnarray*}
:::

::: {#exr-1-10 name="Les arbres"}
Le calcul donne 
\begin{eqnarray*}
\hat \beta_1 =\frac{6.26}{28.29}=0.22 \quad \quad 
\hat \beta_0 = 18.34-0.22 \times 34.9=10.662.
\end{eqnarray*}
Nous nous servons de la propriété $\sum_{i=1}^n \hat \varepsilon_i=0$
pour obtenir
\begin{eqnarray*}
\leR2 &=& \frac{\sum_{i=1}^{20} (\hat y_i - \bar y)^2}
{\sum_{i=1}^{20}(y_i - \bar y)^2}=\frac{\sum_{i=1}^{20} 
(\hat \beta_1 x_i - \hat \beta_1 \bar x)^2}
{\sum_{i=1}^{20}(y_i - \bar y)^2}=0.22^2 \times \frac{28.29}{2.85}=0.48.
\end{eqnarray*}
Les statistiques de test valent 5.59 pour $\beta_0$ et 4.11 pour 
$\beta_1$. Elles sont à comparer à un fractile de la loi de Student admettant 
18 ddl, soit 2.1. Nous rejetons dans les deux cas l'hypothèse de nullité
du coefficient. Nous avons modélisé la hauteur par une fonction 
affine de la circonférence, il semblerait évident que la droite passe 
par l'origine (un arbre admettant un diamètre proche de zéro doit 
être petit), or nous rejetons l'hypothèse $\beta_0=0$. Les données 
mesurées indiquent des arbres dont la circonférence varie de 26 à 
43 cm, les estimations des paramètres du modèle sont valides pour 
des données proches de $[26;43]$.

:::

::: {#exr-1-11 name="Modèle quadratique"}
Les modèles sont 
\begin{eqnarray*}
\mathtt{O3} &=& \beta_1 + \beta_2 \mathtt{T12} +\varepsilon \quad 
\hbox{modèle classique,}\\
\mathtt{O3} &=& \gamma_1 + \gamma_2 \mathtt{T12}^2 +\varepsilon \quad
\hbox{modèle demandé}.
\end{eqnarray*}
L'estimation des paramètres donne 
\begin{eqnarray*}
\widehat{\mathtt{O3}} &=& 31.41 + 2.7 \ \mathtt{T12} \quad\quad \leR2=0.28 \quad
\hbox{modèle classique,}\\
\widehat{\mathtt{O3}} &=& 53.74 + 0.075 \ \mathtt{T12}^2 \quad \leR2=0.35 \quad
\hbox{modèle demandé}.
\end{eqnarray*}
Les deux modèles ont le même nombre de paramètres, nous préférons 
le modèle quadratique car le $\leR$ est plus élevé.

:::
