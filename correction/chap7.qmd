---
title: "7 Variables qualitatives : ANCOVA et ANOVA"
toc: true
---

::: {.content-hidden}
{{< include ../macros.tex >}}
:::

::: {#exr-7-1 name="Questions de cours"}
A, A, C, B.
:::

::: {#exr-7-2 name="Analyse de la covariance"}

1.  Nous avons pour le modèle complet la matrice suivante :
$$
X=\begin{bmatrix}
1&\cdots&0     &x_{11}&\cdots&0\\
\vdots&\cdots&\vdots&\vdots&\vdots&\vdots\\
1&\cdots&      0&x_{1n_1}&\cdots&0\\
\cdots&\cdots&\cdots&\cdots&\cdots&\cdots\\
0&\cdots&1&0&\cdots&x_{I1}\\
\vdots&\vdots&\vdots&\vdots&\vdots&\vdots\\
0&\cdots&1&0&\cdots&x_{In_I}
\end{bmatrix}
$$
et pour les deux sous-modèles, nous avons les matrices suivantes :
$$
X=\begin{bmatrix}
1&\cdots&0     &x_{11}\\
\vdots &\cdots&\vdots&\vdots\\
1&\cdots&      0&x_{1n_1}\\
\cdots&\cdots&\cdots&\cdots\\
0&\cdots&1&x_{I1}\\
\vdots&\vdots&\vdots&\vdots\\
0&\cdots&1&x_{In_I}
\end{bmatrix}
\quad
X=\begin{bmatrix}
1&x_{11}&\cdots&0\\
\vdots &\vdots&\vdots&\vdots\\
1&x_{1n_1}&\cdots&0\\
\cdots&\cdots&\cdots&\cdots\\
1&0&\cdots&x_{I1}\\
\vdots&\vdots&\vdots&\vdots\\
1&0&\cdots&x_{In_I}
\end{bmatrix}
$$

2.  Dans le modèle complet, nous obtenons par le calcul
$$
X'X = \begin{bmatrix}
n_1&0&\cdots&\sum x_{i1}&0&\cdots\\
&\ddots&&&\ddots&\\
0&\cdots&n_I&0&\cdots&\sum x_{iI}\\
\sum x_{i1}&0&\cdots&\sum x^2_{i1}&0&\cdots\\
&\ddots&&&\ddots&\\
0&\cdots&\sum x_{iI}&0&\cdots&\sum x^2_{iI}\\
\end{bmatrix}
  \quad
X'Y = \begin{bmatrix}
\sum y_{i1}\\
\vdots\\
\sum y_{iI}\\
\sum x_{i1}y_{i1}\\
\vdots\\
\sum x_{iI}y_{iI}\\
\end{bmatrix}
$$
Une inversion par bloc de $X'X$ et un calcul matriciel donnent le résultat indiqué.

3.  Une autre façon de voir le problème est de partir du
problème de minimisation
\begin{eqnarray*}
&&\min \sum_{i=1}^I\sum_{j=1}^{n_i}\left(y_{ij}-\alpha_{j}-\beta_{j}x_{ij}\right)^2\\
&=& \min \sum_{j=1}^{n_i}\left(y_{j1}-\alpha_1-\beta_{1}x_{j1}\right)^2+\cdots
+\sum_{j=1}^{n_I}\left(y_{jI}-\alpha_I-\beta_{I}x_{JI}\right)^2.
\end{eqnarray*}
Cela revient donc à calculer les estimateurs des MC pour chaque modalité de la variable
    qualitative. Attention tout de même, des régressions pour chaque modalité donnent bien les mêmes coefficients $\alpha_{i}, \beta_{i}$ mais les écarts-types estimés seront différents: un par modalité dans le cas des régressions pour chaque modalité, un seul écart-type estimé dans le cas de l'ANCOVA.

:::

::: {#exr-7-3 name="Estimateurs des MC et ANOVA à 1 facteur"}
La preuve de cette proposition est relativement longue et peu 
difficile. Nous avons toujours $Y$ un vecteur de $\R^n$ à expliquer.
Nous projetons $Y$ sur le sous-espace engendré par les colonnes 
de $A_c$, noté $\M_{A_c}$, de dimension I, et obtenons 
un unique $\hat Y$. Cependant, en fonction des contraintes utilisées, 
le repère de $\M_{A_c}$ va changer. 

Le cas le plus facile se retrouve lorsque $\mu=0$. Nous avons alors 
\begin{eqnarray*}
(A_c'A_c) = 
\begin{bmatrix}
n_1&0&\cdots&0 \\
0&n_2&0&\cdots \\
\vdots&\vdots&\vdots&\vdots\\
0& \cdots & 0 & n_I
\end{bmatrix}
\quad 
(A_c'Y)=\begin{bmatrix}
\sum_{j=1}^{n_1} y_{1j}\\
\sum_{j=1}^{n_2} y_{2j}\\
\vdots\\
\sum_{j=1}^{n_I} y_{Ij}
\end{bmatrix}
\end{eqnarray*}
d'où le résultat.
La variance de $\hat \alpha$ vaut $\sigma^2 (A_c'A_c)^{-1}$ et
cette matrice est bien diagonale.

Pour les autres contraintes, nous utilisons le vecteur $\vec{e}_{ij}$ 
de $\R^n$ dont toutes les coordonnées sont nulles sauf celle 
repérée par le couple $(i,j)$ qui vaut 1 pour repérer un individu.
Nous notons $\vec{e}_{i}$ le vecteur de $\R^n$ dont toutes les coordonnées 
sont nulles sauf celles repérées par les indices $i,j$ pour $j=1,\cdots,n_{i}$ 
qui valent 1. En fait, ce vecteur repère donc les individus qui admettent 
la modalité $i$. La somme des $\vec{e}_{i}$ vaut le vecteur $\1$.
Les vecteurs colonnes de la matrice $A_c$ valent donc 
$\vec{e}_{1},\cdots,\vec{e}_{I}$.


Considérons le modèle 
\begin{eqnarray*}
Y=\mu \1 + \alpha_1 \vec{e_1}+ \alpha_2 \vec{e_2} + 
\cdots + \alpha_I\vec{e_I} + \varepsilon.
\end{eqnarray*}
Voyons comment nous pouvons récrire ce modèle lorsque les 
contraintes sont satisfaites.

1.  $\alpha_1=0$, le modèle devient alors 
\begin{eqnarray*}
Y &=&\mu \1 + 0 \vec{e_1} + \alpha_2 \vec{e_2} + \cdots + 
\alpha_I\vec{e_I} + \varepsilon\\
&=&\mu \1 + \alpha_2 \vec{e_2} + \cdots + \alpha_I\vec{e_I} + \varepsilon\\
&=& [\1, \vec{e_2}, \cdots, \vec{e_I}] \beta + \varepsilon\\
&=& X_{[\alpha_1=0]} \beta_{[\alpha_1=0]} + \varepsilon.
\end{eqnarray*}

2.  $\sum n_i \alpha_i = 0$ cela veut dire que $\alpha_I= - \sum_{j=1}^{I-1} n_j\alpha_j/n_I$, le modèle devient
    $$
    \begin{eqnarray*}
    Y &=&\mu \1+ \alpha_1 \vec{e_1} + \cdots +\alpha_{I-1} \vec{e_{I-1}}   
    - \sum_{j=1}^{I-1}  \frac{n_j\alpha_j}{n_I}\vec{e_I} + \varepsilon\\
    &=& \mu \1 + \alpha_1(\vec{e}_{1}-\frac{n_1}{n_I}\vec{e}_{I}) + \cdots 
    + \alpha_{I-1} (\vec{e}_{I-1}-\frac{n_{I-1}}{n_I}\vec{e}_{I})+\varepsilon\\
    &=&\mu \1 + \alpha_1\vec{v}_{1}+ \cdots + \alpha_{I-1} \vec{v}_{I-1} 
    + \varepsilon \quad \hbox{où} \quad \vec{v}_{i}= (\vec{e}_{i}-\frac{n_i}{n_I}\vec{e}_{I})\\
    &=& X_{[\sum  n_i \alpha_i=0]} \beta_{[\sum  n_i\alpha_i=0]} + \varepsilon.
    \end{eqnarray*}
    $$

3.  $\sum  \alpha_i = 0$ cela veut dire que $\alpha_I= - \sum_{j=1}^{I-1} \alpha_j$, le modèle devient
    $$
    \begin{eqnarray*}
    Y &=&\mu \1 + \alpha_1 \vec{e_1} + \cdots + \alpha_{I-1} \vec{e_{I-1}} -
    \sum_{j=1}^{I-1}  \alpha_j \vec{e_I} + \varepsilon\\
    &=& \mu \1 + \alpha_1(\vec{e}_{1}-\vec{e}_{I}) + \cdots 
    + \alpha_{I-1} (\vec{e}_{I-1}-\vec{e}_{I})+\varepsilon\\
    &=&\mu \1 + \alpha_1\vec{u}_{1}+ \cdots + \alpha_{I-1} \vec{u}_{I-1}
    + \varepsilon \quad \hbox{où} \quad \vec{u}_{i}= (\vec{e}_{i}-\vec{e}_{I})\\
    &=& X_{[\sum  \alpha_i=0]} \beta_{[\sum  \alpha_i=0]} + \varepsilon.
    \end{eqnarray*}
    $$
  
Dans tous les cas, la matrice $X$ est de taille $n \times I$, et de rang $I$. La matrice $X'X$ est donc inversible. Nous pouvons calculer l'estimateur $\hat \beta$ des MC de $\beta$ par la formule $\hat \beta = (X'X)^{-1}X'Y$ et obtenir les valeurs des estimateurs. Cependant ce calcul n'est pas toujours simple et il est plus facile de démontrer les résultats *via* les projections.

Les différentes matrices $X$ et la matrice $A$ engendrent le même 
sous-espace, donc la projection de $Y$, notée $\hat Y$ dans ce sous-espace, 
est toujours la même. La proposition 6.2 indique que
\begin{eqnarray*}
\hat Y = \bar{y_1} \vec{e_1} + \cdots + \bar{y_I} \vec{e_I}. 
\end{eqnarray*}
Avec les différentes contraintes, nous avons les 3 cas suivants :

1.  $\alpha_1=0$, la projection s'écrit
    $$
    \begin{eqnarray*}
    \hat Y &=&\hat \mu \1 + \hat \alpha_2 \vec{e_2} + \cdots + 
    \hat \alpha_I\vec{e_I}.
    \end{eqnarray*}
    $$

2.  $\sum n_i \alpha_i = 0$, la projection s'écrit
    $$
    \begin{eqnarray*}
    \hat Y &=&\hat \mu \1 + \hat \alpha_1 \vec{e_1} + \cdots + \hat \alpha_{I-1} 
    \vec{e_{I-1}}   - \sum_{j=1}^{I-1}  \frac{n_j \hat \alpha_j}{n_I}\vec{e_I}.
    \end{eqnarray*}
    $$

3.  $\sum  \alpha_i = 0$, la projection s'écrit
    $$
    \begin{eqnarray*}
    \hat Y &=& \hat \mu \1 + \hat \alpha_1 \vec{e_1} + \cdots + 
    \hat \alpha_{I-1} \vec{e_{I-1}} - \sum_{j=1}^{I-1} \hat  \alpha_j \vec{e_I}.
    \end{eqnarray*}
    $$

Il suffit maintenant d'écrire que la projection est identique dans chaque cas et de remarquer que le vecteur $\1$ est la somme des vecteurs $\vec{e_i}$ pour $i$ variant de 1~à~$I$. Cela donne

1.  $\alpha_1=0$
    $$
    \begin{eqnarray*}
    &&\bar{y_1} \vec{e_1} + \cdots + \bar{y_I} \vec{e_I} \\
    &=&\hat \mu\1+\hat\alpha_2\vec{e_2}+\cdots+\hat \alpha_I\vec{e_I}\\
    &=& \hat \mu\vec{e_1}+(\hat \mu+\hat \alpha_2)\vec{e_2}
    \cdots (\hat \mu+\hat \alpha_I)\vec{e_I}.
    \end{eqnarray*}
    $$

2.  $\sum n_i \alpha_i = 0$
    $$
    \begin{eqnarray*}
    &&\bar{y_1} \vec{e_1} + \cdots + \bar{y_I} \vec{e_I} \\
    &=& \hat \mu \1 + \hat \alpha_1 \vec{e_1} + \cdots + \hat \alpha_{I-1} 
    \vec{e_{I-1}}   - \sum_{j=1}^{I-1}  \frac{n_j \hat \alpha_j}{n_I}\vec{e_I}\\
    &=& (\hat \mu + \hat \alpha_1) \vec{e_1} + \cdots +
    (\hat \mu  + \hat \alpha_{I-1}) \vec{e_{I-1}} +
    (\hat \mu  - \sum_{i=1}^{I-1} \frac{n_i}{n_I}\hat \alpha_i) \vec{e_I}.
    \end{eqnarray*}
    $$

3.  $\sum \alpha_i = 0$
    $$
    \begin{eqnarray*}
    &&\bar{y_1} \vec{e_1} + \cdots + \bar{y_I} \vec{e_I} \\
    &=&\hat \mu \1 + \hat \alpha_1 \vec{e_1} + \cdots + 
    \hat \alpha_{I-1} \vec{e_{I-1}} - \sum_{j=1}^{I-1} \hat  \alpha_j \vec{e_I}\\
    &=& (\hat \mu + \hat \alpha_1) \vec{e_1} + \cdots +
    (\hat \mu  + \hat \alpha_{I-1})\vec{e_{I-1}} +
    (\hat \mu - \sum_{i=1}^{I-1} \hat \alpha_i) \vec{e_I}.
    \end{eqnarray*}
    $$
    
En identifiant les différents termes, nous obtenons le résultat annoncé.

:::

::: {#exr-7-4 name="Estimateurs des MC et ANOVA à deux facteurs"}
Nous notons $\vec{e}_{ijk}$ le vecteur de $\R^n$ dont toutes les 
coordonnées sont nulles sauf celle indicée par $ijk$ qui vaut 1.
Sous les contraintes de type analyse par cellule, le modèle 
devient 
\begin{eqnarray*}
y_{ijk} &=& \gamma_{ij} + \varepsilon_{ijk},
\end{eqnarray*}
et donc matriciellement 
\begin{eqnarray*}
Y= X \beta +\varepsilon \quad \quad X=(\vec{e_{11}},\vec{e_{12}},\ldots,\vec{e_{IJ}}),
\end{eqnarray*}
où le vecteur $\vec{e}_{ij}= \sum_{k} \vec{e}_{ijk}$. Les vecteurs 
colonnes de la matrice $X$ sont orthogonaux entre eux. Le 
calcul matriciel $(X'X)^{-1}X'Y$ donne alors le résultat annoncé. 
:::

::: {#exr-7-5 name="Estimateurs des MC et ANOVA à deux facteurs, suite"}
Nous notons $\vec{e}_{ijk}$ le vecteur de $\R^n$ dont toutes les 
coordonnées sont nulles sauf celle indicée par $ijk$ qui vaut 1.
Nous définissons ensuite les vecteurs suivants~: 
\begin{eqnarray*}
\vec{e}_{ij} = \sum_{k} \vec{e}_{ijk} \quad
\vec{e}_{i.} = \sum_{j} \vec{e}_{ij}  \quad 
\vec{e}_{.j} = \sum_{i} \vec{e}_{ij}  \quad
\vec{e} = \sum_{i,j,k} \vec{e}_{ijk}.
\end{eqnarray*}
Afin d'effectuer cet exercice, nous définissons les sous-espaces suivants~:
\begin{eqnarray*}
E_1&\!\!:=\!\!&\{m \vec{e},\ m \hbox{ quelconque} \}\\
E_2&\!\!:=\!\!&\{\sum_i a_i \vec{e}_{i.},\ \sum_i a_i=0\}\\
E_3&\!\!:=\!\!&\{\sum_j b_j \vec{e}_{.j},\ \sum_j b_j=0\}\\
E_4&\!\!:=\!\!&\{\sum_{ij} c_{ij} \vec{e}_{ij},
\ \forall j \sum_{i} c_{ij}=0 \hbox{ et } \forall i \sum_{j} c_{ij}=0\}.
\end{eqnarray*}
Ces espaces $E_1$, $E_2$, $E_3$ et $E_4$ sont de dimension respective 
1, $I-1$, $J-1$ et $(I-1)(J-1)$. 
Lorsque le plan est équilibré, tous 
ces sous-espaces sont orthogonaux. Nous avons la décomposition suivante~:
\begin{eqnarray*}
E = E_1 \stackrel{\perp}{\oplus} E_2 \stackrel{\perp}{\oplus} E_3 
\stackrel{\perp}{\oplus} E_4.
\end{eqnarray*}

La projection sur $E$ peut se décomposer en une partie sur $E_1,\cdots,E_4$ 
et l'estimateur des MC est obtenu par projection de $Y$ sur $E$. Notons 
$P_{E^\perp}$, $P_{E},$ $P_{E_1},$ $P_{E_2},$ $P_{E_3}$ et $P_{E_4}$ les 
projections orthogonales sur les sous-espaces  $E^\perp$, $E$, $E_1$, 
$E_2$, $E_3$ et $E_4$, nous avons alors 
\begin{eqnarray*}
P_{E_1} Y &=& \bar{y} \1 ,
\end{eqnarray*}
puis, en remarquant que projeter sur le sous-espace engendré par les 
colonnes de $A=[\vec{e}_{1.},\cdots,\vec{e}_{I.}]$ est identique 
à la projection sur $E_1 \stackrel{\perp}{\oplus} E_2$, nous avons 
alors avec $\1 = \sum_i \vec{e}_{i.}$,
\begin{eqnarray*}
P_{A} Y = \sum_i \bar{y}_{i.} \vec{e}_{i.} \quad \hbox{donc}
\quad P_{E_2} Y =\sum_i (\bar{y}_{i.} - \bar{y})\ \vec{e}_{i.}.\\
\end{eqnarray*}
De la même façon, nous obtenons
\begin{eqnarray*}
P_{E_3}(Y)&=&\sum_j (\bar{y}_{.j} - \bar{y})\ \vec{e}_{.j},\\
P_{E_4}(Y)&=&\sum_{ij} (\bar{y}_{ij}-\bar{y}_{i.}-\bar{y}_{.j}+\bar{y})\  \vec{e}_{i.},\\
P_{E^\perp}(Y)  &=&\sum_{ijk} (y_{ijk}-\bar{y}_{ij})\ \vec{e}_{ijk}, 
\end{eqnarray*}
où $\vec{e}_{ijk}$ est le vecteur dont toutes les coordonnées sont nulles 
sauf celle indicée par ${ijk}$ qui vaut 1. En identifiant terme à terme,
nous retrouvons le résultat énoncé.

:::

::: {#exr-7-6 name="Tableau d'ANOVA à 2 facteurs équilibrés"}
Lorsque le plan est équilibré, nous avons démontré, 
que les sous-espaces $E_1$, $E_2$, $E_3$ et $E_4$
sont orthogonaux (cf. exercice précédent) deux à deux. Nous avons alors
\begin{eqnarray*}
Y &=& P_{E_1}(Y) + P_{E_2}(Y) + P_{E_3}(Y) + P_{E_4}(Y) + P_{E^\perp}(Y).
\end{eqnarray*}
Nous obtenons ensuite par le théorème de Pythagore
\begin{eqnarray*}
\begin{array}{ccccccccccc}
\|Y - \bar Y \|^2 &=&  \| P_{E_2}(Y)\|^2 &+& 
\|P_{E_3}(Y)\|^2 &+& \|P_{E_4}(Y)\|^2 &+& \|P_{E^\perp}(Y)\|^2\\
\SCT &=& \SC_A &+& \SC_B &+& \SC_{AB} &+& \SCR,
\end{array}
\end{eqnarray*}
où 
\begin{eqnarray*}
\SCT &=& \sum_i \sum_j \sum k (y_{ijk} - \bar y)^2\\
\SC_A &=& Jr \sum_i (y_{i..}-\bar y)^2\\
\SC_B &=& Ir \sum_j (y_{.j.} - \bar y)^2\\
\SC_{AB} &=& r \sum_i \sum_j (y_{ij.} - y_{i..} - y_{.j.} +\bar y)^2\\
\SCR &=& \sum_i \sum_j \sum_k (y_{ijk}- \bar{y_{ij}})^2.
\end{eqnarray*}

Afin  de bien visualiser les vecteurs voici un exemple avec $I=2$, $J=3$ et $r=2$ en remplaçant les $0$ par $.$ :

$$
\begin{array}{*{12}c}
      \vec{e}&\vec{e}_{1.}&\vec{e}_{2.}&
      \vec{e}_{.1}&\vec{e}_{.2}&\vec{e}_{.3}&
      \vec{e_{11}}&\vec{e}_{12}&\vec{e}_{13}&
      \vec{e}_{21}&\vec{e}_{22}&\vec{e}_{23}&\\
1&1&.&1&.&.&1&.&.&.&.&.& \\
1&1&.&1&.&.&1&.&.&.&.&.& \\
1&1&.&.&1&.&.&1&.&.&.&.& \\
1&1&.&.&1&.&.&1&.&.&.&.& \\
1&1&.&.&.&1&.&.&1&.&.&.& \\
1&1&.&.&.&1&.&.&1&.&.&.& \\
%%
1&.&1&1&.&.&.&.&.&1&.&.& \\
1&.&1&1&.&.&.&.&.&1&.&.& \\
1&.&1&.&1&.&.&.&.&.&1&.& \\
1&.&1&.&1&.&.&.&.&.&1&.& \\
1&.&1&.&.&1&.&.&.&.&.&1& \\
1&.&1&.&.&1&.&.&.&.&.&1& \\
\end{array}
$$

1.  En écrivant les vecteurs dans le cadre général et en faisant la somme ci-dessous
    $$
    \vec{Y}=\mu \vec{e}+\sum_{i} \alpha_i \vec{e}_{i.}+\sum_{j} \beta_j \vec{e}_{.j}
    +\sum_{ij} (\alpha\beta)_{ij} \vec{e}_{ij} + \vec{\varepsilon},
    $${#eq-anova2somme}
    on a  bien que la ligne $ijk$ vaut
    $$
    y_{ijk} = \mu +\alpha_i+\beta_j+(\alpha\beta)_{ij}+ \varepsilon_{ijk}.
    $$

2.  Montrons que $E_1 \perp E_2$. Pour cela prenons deux vecteurs quelconques de $E_1$ et $E_2 $, ils s'écrivent $m\vec{e}$ et
$\sum_{i=1}^I a_{i} \vec{e}_{i.}$ (avec $\sum_{i=1}^I a_{i}=0$) et leur produit scalaire vaut
$$
<m\vec{e};\sum_{i=1}^I a_{i} \vec{e}_{i.}>=m\sum_{i=1}^I a_{i}<\vec{e};\vec{e}_{i.}> = m I \sum_{i=1}^I a_{i} =0
$$
    De même avec $E_1 \perp E_3$.

    Montrons que $E_1 \perp E_4$. Pour cela prenons deux vecteurs quelconques de $E_1$ et $E_4$, ils s'écrivent $m\vec{e}$ et
$\sum_{i=1}^I\sum_{j=1}^J (ab)_{ij} \vec{e}_{ij}$ (avec pour tout $i$ $\sum_j (ab)_{ij}=0$ et pour tout $j$
$\sum_i (ab)_{ij}=0$). Leur produit scalaire vaut
\begin{align*}
  <m\vec{e};\sum_{i=1}^I\sum_{j=1}^J (ab)_{ij}\vec{e}_{ij}>
  &=m\sum_{i=1}^I \sum_{j=1}^J (ab)_{ij}<\vec{e};\vec{e}_{ij}> = m r \sum_{i=1}^I \sum_{j=1}^J (ab)_{ij} =0
\end{align*}

    Montrons que $E_2 \perp E_4$. Pour cela prenons deux vecteurs quelconques de $E_2$ et $E_4$, ils s'écrivent $\sum_{l=1}^I a_{l} \vec{e}_{l}$ (avec $\sum_{l=1}^I a_{l}=0$) et
$\sum_{i=1}^I\sum_{j=1}^J (ab)_{ij} \vec{e}_{ij}$ (avec $i$ $\sum_j (ab)_{ij}=0$ et pour tout $j$
$\sum_i (ab)_{ij}=0$. Leur produit scalaire vaut
\begin{align*}
  <\sum_{l=1}^I a_{l} \vec{e}_{l.};\sum_{i=1}^I\sum_{j=1}^J (ab)_{ij}\vec{e}_{ij}>
  &=\sum_{l=1}^I a_{l}\sum_{i=1}^I \sum_{j=1}^J (ab)_{ij}<\vec{e}_{l.};\vec{e}_{ij}> \\
  &=\sum_{l=1}^I a_{l} r \sum_{i=1}^I \sum_{j=1}^J (ab)_{ij} =0
\end{align*}
    De même avec $E_3 \perp E_4$.
    
3.  La dimension de $E_{1}$ vaut 1 (car $\vec{e}$ est non nul). Le sous-espace $E_{2}$ est engendré par les $I$ vecteurs non nuls et orthogonaux deux à deux $\{\vec{e}_{i.}\}_{i=1}^{I}$ donc le sous espace engendré est au moins de dimension $I$. Cependant on ajoute une contrainte linéaire donc $dim(E_{2})=I-1$. De même pour $E_{2}$ dont la dimension est donc $J-1$. Enfin sous-espace $E_{4}$ est engendré par les $IJ$ vecteurs non nuls et orthogonaux deux à deux $\{\vec{e}_{ij}\}$ (donc le sous espace engendré est de dimension $IJ$) mais auquel on ajoute plusieurs contraintes linéaires. Il faut donc compter le nombre de contrainte linéaires indépendantes.

    Montrons que les $I+J$ contraintes $ \forall i \sum_{j} (ab)_{ij}=0$ et $\forall j \sum_{i} (ab)_{ij}=0$ ne sont pas indépendantes. En effet quand $I+J-1$ contraintes sont vérifiées, la dernière restante l'est aussi.
$$
  \begin{array}{*{5}c}
  (ab)_{11}&(ab)_{12}&\ldots&(ab)_{1J-1}&(ab)_{1J}&=0\\
  (ab)_{21}&(ab)_{22}&\ldots&(ab)_{2J-1}&(ab)_{2J}&=0\\
  \vdots&\vdots& &\vdots&\vdots&\vdots\\
  (ab)_{I1}&(ab)_{I2}&\ldots&(ab)_{IJ-1}&(ab)_{IJ}&=0\\
  =0&=0&\ldots&=0&c=?&
\end{array}
$$
    Posons que $I+J-1$ contraintes sont vérifiées~: $I$ en ligne et $J-1$ en colonnes (voir ci-dessus). En sommant toute la matrice on sait (somme en ligne) que cela vaut zéro et donc la somme en colonne vaut elle aussi 0 et donc la dernière somme $c$ vaut  0 (voir ci-dessus). Nous avons donc que la dimension de $E_{4}$ est $IJ-(I +J -1)=(I-1)(J-1)$
    
4.  
    -   Calculons $P_{1}Y$
        \begin{align*}
        P_{1}Y &= \un (\un' \un)^{-1} \un'Y=\un (IJr)^{-1} \sum_{ijk}Y_{ijk} = \frac{1}{IJr} Y_{...} \un\\
              &=\bar Y_{...} \vec{e}
        \end{align*}
    -   Calculons $P_{2}Y$. On sait que $F_{2}=E_1 \stackrel{\perp}{\bigoplus} E_2$ et que $F_{2}$ est de dimension $I$. Il est donc engendré par les $I$ vecteurs orthogonaux $\{\vec{e}_{i.}\}_{i=1}^{I}$ qui en forme une base. Du fait de la décomposition on a
        $$
        \begin{align}
        P_{F_{2}}Y &= P_{1}Y  + P_{2}Y
        \end{align}
        $${#eq-pf2}
        Calculons maintenant directement la projection sur $F_{2}$. Ce sous-espace est engendré par les $I$ vecteurs orthogonaux $\{\vec{e}_{i.}\}_{i=1}^{I}$ en posant la matrice concaténant les (coordonnées des) vecteurs:
        \begin{align*}
        F_{2}&=(\vec{e}_{1.}|\vec{e}_{2}|\cdots|\vec{e}_{I.})
        \end{align*}
        on a le projecteur
        \begin{align*}
        P_{F_{2}}&=F_{2}(F_{2}'F_{2})^{-1}F'_{2}
        \end{align*}
        En effectuant le calcul matriciel on a
        \begin{align*}
        (F_{2}'F_{2})&=\diag(I, I, \dotsc, I)
        \end{align*}
        et par  calcul matriciel direct on trouve
        $$
        \begin{align}
        P_{F_{2}}Y&=F_{2}\diag(1/I, 1/I, \dotsc, 1/I)F_{2}'Y\nonumber\\
        &=F_{2}
        \begin{pmatrix}
        \bar Y_{1..}\\
        \bar Y_{2..}\\
        \vdots\\
        \bar Y_{I..}\\
        \end{pmatrix}
        = \bar Y_{1..} \vec{e}_{1.} + \bar Y_{2..}\vec{e}_{2.}
        + \dotsc \bar Y_{I..}\vec{e}_{I.}
        \end{align}
        $${#eq-pf2bis}
        En utilisant les équations @eq-pf2 et @eq-pf2bis on trouve
        \begin{align*}
        P_{2}Y&=\bar Y_{1..} \vec{e}_{1.} + \bar Y_{2..}\vec{e}_{2.}
        + \dotsc + \bar Y_{I..}\vec{e}_{I.} - \bar Y_{...} \vec{e}.
        \end{align*}
        Remarquons que $\vec{e}=\vec{e}_{1.} + \dotsc +\vec{e}_{I.}$ et en remplaçant cela dans l'équation précédente nous avons
        $$
        \begin{align*}
        P_{2}Y&=(\bar Y_{1..} -\bar Y_{...})  \vec{e}_{1.} + (\bar Y_{2..} - \bar Y_{...}) \vec{e}_{2.}
        + \dotsc + (\bar Y_{I..} -\bar Y_{...}) \vec{e}_{I.}.
        \end{align*}
        $$
    -   En calquant ces calculs pour $E_{3}$ on trouve
        \begin{align*}
        P_{3}Y&=(\bar Y_{.1.}-\bar Y_{...}) \vec{e}_{.1} + (\bar Y_{.2.}-\bar Y_{...})\vec{e}_{.2}
        + \dotsc +(\bar Y_{.J.}-\bar Y_{...})\vec{e}_{.J}.
        \end{align*}
    -   Enfin pour $E_{4}$, remarquons que $F_{4}=E=\vect(\vec{e}_{11}, \dotsc, \vec{e}_{IJ})$. La projection sur $E$ identifié à sa matrice $(\vec{e}_{11}| \dotsc |\vec{e}_{IJ})$ peut être calculée de manière directe comme
        \begin{align}
        P_{E}Y&=E\diag(1/r, 1/r, \cdots, 1/r)E'Y\nonumber\\
        &=E
        \begin{pmatrix}
        \bar Y_{11.}\\
        \bar Y_{21.}\\
        \vdots\\
        \bar Y_{IJ.}\\
        \end{pmatrix}
        = \bar Y_{11.} \vec{e}_{11} + \dotsc \bar Y_{IJ.}\vec{e}_{IJ}\label{eq:pebis}
        \end{align}
        En se servant de la décomposition on a
        \begin{align}
        P_{E}Y&=P_{1}Y + P_{2}Y  + P_{3}Y  + P_{4}Y
        \end{align}
        Et en identifiant les deux calculs (avec $\vec{e}_{i.}=\vec{e}_{i1} + \dotsc +\vec{e}_{iJ}$ et $\vec{e}_{.j}=\vec{e}_{1j} + \dotsc +\vec{e}_{Ij}$ )
        \begin{align*}
        P_{4}Y&= (\bar Y_{11.} - \bar Y_{1..} - \bar Y_{.1.} + \bar Y_{...})\vec{e}_{11}+ \dotsc + (\bar Y_{IJ} - \bar Y_{I..} - \bar Y_{.J.} + \bar Y_{...})\vec{e}_{IJ}.
        \end{align*}
    -   La dernière projection s'obtient comme
        \begin{align*}
        QY&=Y- P_{E}Y = Y -  \bar Y_{11.} \vec{e}_{11} + \dotsc \bar Y_{IJ.}\vec{e}_{IJ}
        \end{align*}

5.  En reprenant la décomposition en sous-espace orthogonaux suivante
    $$
    \begin{align}
    \R^{n}&= E  \stackrel{\perp}{\bigoplus} E^{\perp} =E  \stackrel{\perp}{\bigoplus} Q\\
    &= E_1 \stackrel{\perp}{\bigoplus} E_2 \stackrel{\perp}{\bigoplus} E_3 \stackrel{\perp}{\bigoplus} E_4 \stackrel{\perp}{\bigoplus} Q
    \end{align}
    $${#eq-decompEanova2}
    On a donc que
    $$
    \begin{align*}
    Y &= P_{1}Y + P_{2}Y + P_{3}Y + P_{4}Y + P_{Q}Y.
    \end{align*}
    $${#eq-decompPanova2}
    En utilisant toutes les définitions de la question précédente on a
    $$
    \begin{split}
    Y&=\bar Y_{...} \vec{e}\\
    & \ \  +  (\bar Y_{1..} - \bar Y_{...}) \vec{e}_{1.} + \dotsc +\bar (Y_{I..}- \bar Y_{...})\vec{e}_{I.}\\
    & \ \    + (\bar Y_{.1.}- \bar Y_{...}) \vec{e}_{.1} +  \dotsc +\bar (Y_{.J.}- \bar Y_{...})\vec{e}_{.J} \\
    & \ \  + (\bar Y_{11.} - \bar Y_{1..} - \bar Y_{.1.} + \bar Y_{...})\vec{e}_{11}+ \dotsc + (\bar Y_{IJ} - \bar Y_{I..} - \bar Y_{.J.} + \bar Y_{...})\vec{e}_{IJ} \\
    & \ \ + P_{Q}Y.
    \end{split}
    $${#eq-decompPCoefanova2}
    En utilisant l'@eq-anova2somme on identifie terme à terme et nous obtenons les paramètres du modèle :
    \begin{align*}
    \hat \mu&=\bar Y_{...}\\
    \hat \alpha_{i}&=(\bar Y_{i..} - \bar Y_{...})\\
    \hat \beta_{j}&=(\bar Y_{.j.} - \bar Y_{...})\\
    (\widehat{\alpha\beta})_{ij}&=(\bar Y_{ij.} - \bar Y_{i..} - \bar Y_{.j.} + \bar Y_{...})\\
    \end{align*}
    
6.  En utilisant l'@eq-decompPanova2 et en se rappelant de l'orthogonalité (@eq-decompEanova2) on a
    $$
    \begin{align}
    \|Y -\bar Y_{...} \vec{e}\|^{2} &=\| P_{1}Y + P_{2}Y + P_{3}Y + P_{4}Y + P_{Q}Y\|^{2}\nonumber\\
    &=\|P_{1}Y\|^{2} + \|P_{2}Y\|^{2} + \|P_{3}Y\|^{2} + \|P_{4}Y\|^{2} + \|P_{Q}Y\|^{2}
    \end{align}
    $${#eq-decompositonSC1}
    et remplaçant les projections par leur expression (voir par exemple @eq-decompPCoefanova2) et calculant les normes on a
    $$
    \begin{split}
    \sum_{i=1}^I \sum_{j=1}^J\sum_{k=1}^r (Y_{ijk} - \bar Y_{...})^{2}
    &= rJ\sum_{i=1}^I(\bar Y_{i..} - \bar Y_{...})^{2}
    + rI\sum_{j=1}^J(\bar Y_{.j.} - \bar Y_{...})^{2}\\
    & \ \  + r\sum_{i=1}^I\sum_{j=1}^J
    (\bar Y_{ij} - \bar Y_{i..} - \bar Y_{.j.} + \bar Y_{...})^{2}\\
    & \ \  + \sum_{i=1}^I\sum_{j=1}^J\sum_{k=1}^r
    (Y_{ijk} -  \bar Y_{ij})^{2}.
    \end{split}
    $${#eq-decompositonSC2}
    En multipliant par $1/n$ l'équation ci-dessus nous obtenons la décomposition de la variance.

7.  Le vecteur $Y$ grâce à $\HH_{3}$ est un vecteur gaussien de moyenne $\vec{m}=\mu +\alpha_i+\beta_j+(\alpha\beta)_{ij}$ et de variance $\sigma^{2}I_{n}$. On sait que
    \begin{align*}
    P_{E}Y &= P_{1}Y + P_{2}Y + P_{3}Y + P_{4}Y
    \end{align*}
    Grâce au théorème de Cochran on a que
    $$
    \frac{\|P_{i}Y -P_{i}\vec{m} \|^{2}}{\sigma^{2}} \sim \chi^{2} (dim(E_{i}))
    $$
    ou encore que $\frac{\|P_{i}Y\|^{2}}{\sigma^{2}}$ suit un $\chi^{2} (dim(E_{i}))$ décentré de paramètre de décentrage $\|P_{i}\vec{m} \|^{2}$. Pour $Q=E^{\perp}$ on a que $P_{Q}\vec{m}=0$ et il n'y a pas de décentrage.
    
8.  Reprenons l'@eq-decompositonSC1 qui s'écrit aussi avec des sommes (@eq-decompositonSC2) ou encore
    \begin{align*}
    \SCT
    &= \SCE_{a} + \SCE_{b} + \SCE_{ab}  + \SCR
    \end{align*}
    On a donc que
    \begin{align*}
    \SCE_{a}&=\|P_{2}Y\|^{2}\sim \sigma^{2}\chi^{2} (I-1), \|P_{2}\vec{m} \|^{2}),\\
    \SCE_{b}&=\|P_{3}Y\|^{2}\sim \sigma^{2}\chi^{2} (J-1), \|P_{3}\vec{m} \|^{2}),\\
    \SCE_{ab}&=\|P_{4}Y\|^{2}\sim \sigma^{2}\chi^{2} ((I-1)(J-1)), \|P_{4}\vec{m} \|^{2}),\\
    \SCE_{ab}&=\|P_{Q}Y\|^{2}\sim \sigma^{2}\chi^{2} (n - IJ).
    \end{align*}
    Nous voyons donc que chaque terme est la norme carrée d'une projection du vecteur gaussien $Y$ dans un sous-espace et que ces sous-espaces sont orthogonaux 2 à 2. Les vecteurs gaussiens projetés sont donc indépendants ainsi que leur norme au carré. Les lois de ces normes carrées sont donc à $\sigma^{2}$ près des $\chi^{2}$ décentrés (sauf pour $Q$) qui sont indépendants.
    
9.  Notons $\CME_{ab} = \SCE_{ab}/((I-1)(J-1))$ et $\CMR = \SCR/(n-IJ)$ nous avons donc
    \begin{align*}
    \frac{\CME_{ab}}{\CMR}&=\frac{\frac{\SCE_{ab}}{\sigma^{2}(I-1)(J-1)}}{\frac{\SCR}{\sigma^{2}(n-IJ)}}
    \end{align*}
    qui est le rapport de deux $\chi^{2}$ indépendants ramenés à leur degrés de liberté et dont le numérateur est décentré. Nous avons donc une loi de Fisher de paramètres $(I-1)(J-1), n -IJ, \|P_{4}\vec{m} \|^{2}$. Sous $\Hz:$ « il n'y a pas d'interaction » (ou $P_{4}\vec{m}=0$) alors la loi se simplifie et le paramètre de décentrage inconnu (qui dépend de  $\vec{m}$) disparaît et la loi est $F((I-1)(J-1), n -IJ)$.
:::

