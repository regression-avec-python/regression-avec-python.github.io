---
title: "Régression logistique"
toc: true
---

::: {.content-hidden}
{{< include ../macros.tex >}}
:::


```{python}
import numpy as np
import pandas as pd
import statsmodels.api as sm
import statsmodels.formula.api as smf
from statsmodels.stats.anova import anova_lm
import matplotlib.pyplot as plt
from scipy.stats import chi2,norm
from scipy import interpolate
```

::: {#exr-12-1 name="Questions de cours"}
1.  A
2.  A
3.  B
4.  A
5.  A
6.  A
7.  B
8.  A
:::

::: {#exr-12-2 name="Interprétation des coefficients"}
1.  On génère l'échantillon.

    ```{python}
    n = 100
    np.random.seed(48967365)
    X = np.random.choice(['A', 'B', 'C'], size=n, replace=True)
    Y = np.zeros(n, dtype=int)
    np.random.seed(487365)
    Y[X == 'A'] = np.random.binomial(1, 0.95, size=np.sum(X == 'A'))
    np.random.seed(4878365)
    Y[X == 'B'] = np.random.binomial(1, 0.95, size=np.sum(X == 'B'))
    np.random.seed(4653965)
    Y[X == 'C'] = np.random.binomial(1, 0.05, size=np.sum(X == 'C'))
    donnees = pd.DataFrame({'Y': Y, 'X': X})
    print(donnees.head())

    ```

2.  On ajuste le modèle avec les contraintes par défaut.

    ```{python}
    model1 = smf.logit('Y ~ X', data=donnees).fit()
    print(model1.summary())
    ```

    On obtient les résultats du **test de Wald** sur la nullité des paramètres $\beta_0,\beta_2$ et $\beta_3$.

3.  On change la modalité de référence.

    ```{python}
    model2 = smf.logit('Y ~ C(X, Treatment(reference="C"))', data=donnees).fit()
    print(model2.summary())
    ```

    On obtient les résultats du **test de Wald** sur la nullité des paramètres $\beta_0,\beta_1$ et $\beta_2$.

4.  On remarque que dans **model1** on accepte la nullité de $\beta_2$ alors qu'on la rejette dans **model2**. Ceci est logique dans la mesure où ces tests dépendent de la contrainte identifiante choisie. Dans **model1** le test de nullité de $\beta_2$ permet de vérifier si $B$ à un effet similaire à $A$ sur $Y$. Dans **model2**, on compare l'effet de $B$ à celui de $C$. On peut donc conclure $A$ et $B$ ont des effets proches sur $Y$ alors que $B$ et $C$ ont un impact différent. Ceci est logique vu la façon dont les données ont été générées.

5.  Tester l'effet global de $X$ sur $Y$ revient à tester si les coefficients $\beta_1,\beta_2$ et $\beta_3$ sont égaux, ce qui, compte tenu des contraintes revient à considérer les hypothèses nulles :

    -   $\beta_2=\beta_3=0$ dans **model1** ;
    -   $\beta_1=\beta_2=0$ dans **model2**.

    On peut effectuer les tests de **Wald** ou du **rapport de vraisemblance**. On obtient les résultats du **rapport de vraisemblance** avec :

    ```{python}
    lr_test_model1 = model1.llr
    lr_test_model2 = model2.llr
    
    print("Test de rapport de vraisemblance pour model1:")
    print(f"LR stat: {lr_test_model1:.4f}, p-value: {model1.llr_pvalue:.4f}")

    print("\nTest de rapport de vraisemblance pour model2:")
    print(f"LR stat: {lr_test_model2:.4f}, p-value: {model2.llr_pvalue:.4f}")
    ```

    On remarque ici que ces deux tests sont identiques : ils ne dépendent pas de la contrainte identifiante choisie.
:::

::: {#exr-12-3 name="Séparabilité"}
1.  On génère l'échantillon demandé.

    ```{python}
    np.random.seed(1234)
    X = np.concatenate([np.random.uniform(-1, 0, 50), np.random.uniform(0, 1, 50)])
    Y = np.concatenate([np.zeros(50), np.ones(50)])
    df = pd.DataFrame({'X': X, 'Y': Y})
    print(df.head())
    ```


2.  Le graphe s'obtient avec :

    ```{python}
    beta = np.arange(0, 100, 0.01)
    def log_vrais(X, Y, beta):
        LV = np.zeros(len(beta))
        for i in range(len(beta)):
            Pbeta = np.exp(beta[i] * X) / (1 + np.exp(beta[i] * X))
            LV[i] = np.sum(Y * X * beta[i] - np.log(1 + np.exp(X * beta[i])))
        return LV
    LL = log_vrais(df['X'], df['Y'], beta)
    ```


    ```{python}
    plt.plot(beta,LL)
    plt.xlabel('beta')
    plt.ylabel('Log-vraisemblance')
    plt.title('Log-vraisemblance en fonction de beta')
    plt.show()
    ```

3.  On obtient un avertissement qui nous dit que l'algorithme d'optimisation n'a pas convergé.

    ```{python}
    #| warning: true
    #| message: true

    model = smf.logit('Y ~ X - 1', data=df).fit()
    print(model.params)
    ```

4.  Le changement proposé supprime la séparabilité des données. On obtient bien un maximum fini pour cette nouvelle vraisemblance.


    ```{python}
    Y1 = Y.copy()
    Y1[0] = 1
    LL1 = log_vrais(X, Y1, beta)
    plt.plot(beta, LL1)
    plt.xlabel('beta')
    plt.ylabel('Log-vraisemblance')
    plt.title('Log-vraisemblance en fonction de beta pour Y1')
    plt.show()
    ```

:::


::: {#exr-12-4 name="Matrice hessienne"}
Le gradient de la log-vraisemblance en $\beta$ est donné par $\nabla \mathcal L(Y,\beta)=X'(Y-P_\beta)$. Sa $j$ème composante vaut $$\frac{\partial\mathcal L}{\partial\beta_j}(\beta)=\sum_{i=1}^nx_{ij}(y_i-p_\beta(x_i)).$$

On peut donc calculer la drivée par rapport à $\beta_\ell$ : \begin{align*}
\frac{\partial\mathcal L}{\partial\beta_j\partial\beta_\ell}(\beta)= & \frac{\partial}{\partial\beta_\ell}\left[
\sum_{i=1}^nx_{ij}\left(y_i-\frac{\exp(x_i'\beta)}{1+\exp(x_i'\beta)}\right)\right] \\
=& -\sum_{i=1}^nx_{ij}x_{i\ell}\frac{\exp(x_i'\beta)}{[1+\exp(x_i'\beta)]^2} \\
=& -\sum_{i=1}^nx_{ij}x_{i\ell}p_\beta(x_i)(1-p_\beta(x_i)).
\end{align*} Matriciellement on déduit donc que la hessienne vaut $$\nabla^2\mathcal L(Y,\beta)=-X'W_\beta X,$$ où $W_\beta$ est la matrice $n\times n$ diagonale dont le $i$ème terme de la diagonale vaut $p_\beta(x_i)(1-p_\beta(x_i))$. Par ailleurs, comme pour tout $i=1,\dots,n$, on a $p_\beta(x_i)(1-p_\beta(x_i))>0$ et que $X$ est de plein rang, on déduit que $X'W_\beta X$ est définie positive et par conséquent que la hessienne est définie négative.
:::

::: {#exr-12-5 name="Modèles avec R"}
On importe les données :

```{python}
panne = pd.read_csv("../donnees/panne.txt", sep=" ")
print(panne.head())
```

1.  La commande

    ```{python}
    model = smf.logit('etat ~ age+marque', data=panne).fit()
    ```

    ajuste le modèle $$\log\left(\frac{p_\beta(x)}{1-p_\beta(x)}\right)=\beta_0+\beta_1x_1+\beta_2\mathsf{1}_{x_2=B}+\beta_3\mathsf{1}_{x_2=C}$$ où $x_1$ et $x_2$ désigne respectivement les variables **age** et **marque**. On obtient les estimateurs avec

    ```{python}
    print(model.summary())
    ```

2.  Il s'agit des tests de Wald pour tester l'effet des variables dans le modèle. Pour l'effet de marque, on va par exemple tester $$H_0:\beta_2=\beta_3=0\quad\text{contre}\quad H_1:\beta_2\neq 0\text{ ou }\beta_3\neq 0.$$ Sous $H_0$ la statistique de Wald suit une loi du $\chi^2$ à 4-2=2 degrés de liberté. Pour le test de la variable **age** le nombre de degrés de liberté manquant est 1. On retrouve cela dans la sortie

    ```{python}
    wald_tests = model.wald_test_terms()
    print("Tests de Wald pour chaque coefficient:")
    print(wald_tests)
    ```

<!--
3.  Il s'agit cette fois du test du rapport de vraisemblance. Les degrés de liberté manquants sont identiques.

    ```{python}
    def lr_test(full_model, reduced_model):
        lr_stat = 2 * (full_model.llf - reduced_model.llf)
        p_value = chi2.sf(lr_stat, df=full_model.df_model - reduced_model.df_model)
        return lr_stat, p_value

    # Liste des variables explicatives
    variables = panne.columns.drop('etat')

    # Effectuer les tests de rapport de vraisemblance pour chaque variable
    lr_results = {}
    for var in variables:
        formula_reduced = 'etat ~ ' + ' + '.join(variables.drop(var))
        reduced_model = smf.logit(formula_reduced, data=panne).fit(disp=0)
        lr_stat, p_value = lr_test(model, reduced_model)
        lr_results[var] = {'LR stat': lr_stat, 'p-value': p_value}

    # Afficher les résultats des tests de rapport de vraisemblance
    print("Tests de rapport de vraisemblance pour chaque variable:")
    for var, result in lr_results.items():
        print(f"{var}: LR stat = {result['LR stat']:.4f}, p-value = {result['p-value']:.4f}")
    ```
-->

3.  

    a)  Le modèle s'écrit $$\log\left(\frac{p_\beta(x)}{1-p_\beta(x)}\right)=\beta_0+\beta_1\mathsf{1}_{x_2=A}+\beta_2\mathsf{1}_{x_2=B}.$$

    b)  Le modèle ajusté ici est 
        $$
        \log\left(\frac{p_\beta(x)}{1-p_\beta(x)}\right)=\gamma_0+\gamma_1\mathsf{1}_{x_2=B}+\gamma_2\mathsf{1}_{x_2=C}.
        $$ 
        Par identification on a 
        $$
        \begin{cases}
            \beta_0+\beta_1=\gamma_0 \\
            \beta_0+\beta_2=\gamma_0+\gamma_1 \\
            \beta_0=\gamma_0+\gamma_2 \\
            \end{cases}
            \Longleftrightarrow
            \begin{cases}
            \beta_0=\gamma_0+\gamma_2 \\
            \beta_1=-\gamma_2 \\
            \beta_2=\gamma_1-\gamma_2 \\
            \end{cases}
            \Longrightarrow
            \begin{cases}
            \widehat\beta_0=-0.92 \\
            \widehat\beta_1=1.48 \\
            \widehat\beta_2=1.05 \\
            \end{cases}
        $$
        On peut retrouver ces résultats avec



        ```{python}
        model1 = smf.logit('etat ~ C(marque, Treatment(reference="C"))', data=panne).fit()
        model1.summary()
        ```


4.  Il y a interaction si l'age agit différemment sur la panne en fonction de la marque.

5.  Le modèle ajusté sur est 
$$
\log\left(\frac{p_\beta(x)}{1-p_\beta(x)}\right)=\delta_0+\delta_1\mathsf{1}_{x_2=B}+\delta_2\mathsf{1}_{x_2=C}+\delta_3x_1+\delta_4x_1\mathsf{1}_{x_2=B}+\delta_5x_1\mathsf{1}_{x_2=C}.
$$ 
    
    On obtient ainsi par identification : 
$$
\begin{cases}
\alpha_0=\delta_0\\
\alpha_1=\delta_3\\
\beta_0=\delta_0+\delta_1\\
\beta_1=\delta_3+\delta_4\\
\gamma_0=\delta_0+\delta_2\\
\gamma_1=\delta_3+\delta_5
\end{cases}
$$ 

    On déduit les valeurs des estimateurs que l'on peut retrouver avec la commande :


    ```{python}
    model2 = smf.logit('etat ~ -1 + marque + marque:age', data=panne).fit()
    print(model2.summary())
    ```

:::

::: {#exr-12-6 name="Interprétation"}

```{python}
df = pd.read_csv("../donnees/logit_exo6.csv")

# Ajuster le modèle de régression logistique avec toutes les variables explicatives
mod = smf.logit('Y ~ X1+X2', data=df).fit()

# Ajuster le modèle de régression logistique avec seulement la variable X1
mod1 = smf.logit('Y ~ X1', data=df).fit()

# Afficher le résumé des modèles
print(mod.summary())
print(mod1.summary())
```

On remarque que la nullité du paramètre associé à **X1** est accepté dans le modèle avec uniquement **X1** alors qu'elle est refusée lorsqu'on considère **X1** et **X2** dans le modèle.

:::

::: {#exr-12-7 name="Tests à la main"}

1.  Le modèle s'écrit
$$log\left(\frac{p_\beta(x)}{1-p_\beta(x)}\right)=\beta_0+\beta_1x.$$

2.  La log vraisemblance s'obtient avec

    ```{python}
    p = np.array([0.76, 0.4, 0.6, 0.89, 0.35])
    Y = np.array([1, 0, 0, 1, 1])
    L1 = np.log(np.prod(p**Y * (1-p)**(1-Y)))
    print(L1)
    ```

3. 
  
    a)  On calcule les écart-type des estimateurs

        ```{python}
        p = np.array([0.76, 0.4, 0.6, 0.89, 0.35])
        X1 = np.array([0.47, -0.55, -0.01, 1.07, -0.71])
        X = np.column_stack((np.ones(5), X1))
        W = np.diag(p * (1 - p))
        SIG = np.linalg.inv(X.T @ W @ X)
        sig = np.sqrt(np.diag(SIG))
        print(sig)
        ```

        On en déduit les statistiques de test :

        ```{python}
        beta = np.array([0.4383, 1.5063])
        result = beta / sig
        print(result)
        ```

    b)  On peut faire le test de Wald et du rapport de vraisemblance. 
  
    c)  La statistique de test vaut 0.8632411, on obtient donc la probabilité critique

        ```{python}
        2 * (1 - norm.cdf(0.8632411))
        ```

        On peut également effectuer un test du rapport de vraisemblance. Le modèle null sans X1 a pour log-vraisemblance


        ```{python}
        p0 = 3 / 5
        Y = np.array([1, 0, 0, 1, 1])
        L0 = np.log(np.prod(p0**Y * (1-p0)**(1-Y)))
        print(L0)
        ```

        La statistique de test vaut donc

        ```{python}
        2*(L1-L0)
        ```

        et la probabilité critique est égale à

        ```{python}
        1 - chi2.cdf(2*(L1-L0), df=1)
        ```


        On peut retrouver (aux arrondis près) les résultats de l’exercice avec

        ```{python}
        X = [0.47, -0.55, -0.01, 1.07, -0.71]
        Y = [1, 0, 0, 1, 1]
        df = pd.DataFrame({'X': X, 'Y': Y})
        model = smf.logit('Y ~ X', data=df).fit()
        log_likelihood = model.llf
        print(log_likelihood)
        ```

        ```{python}
        wald_test = model.wald_test_terms()
        print(wald_test)
        ```

        ```{python}
        lr_test_model = model.llr
        print(f"LR stat: {lr_test_model:.4f}, p-value: {model.llr_pvalue:.4f}")
        ```

:::


::: {#exr-12-8 name="Vraisemblance du modèle saturé"}
1.  Les variables $(y_t,t=1,\dots,y_T)$ étant indépendantes et de loi binomiales $B(n_t,p_t)$, la log-vraisemblance est donnée par
\begin{align*}
\mathcal L_{\text{sat}}(Y,p)= & \log\left(\prod_{t=1}^T
\begin{pmatrix}
n_t\\
\tilde y_t
\end{pmatrix}
p_t^{\tilde y_t}(1-p_t)^{n_t-\tilde y_t}\right) \\
= &
\sum_{t=1}^T\left(\log
\begin{pmatrix}
n_t\\
\tilde y_t
\end{pmatrix}
+\tilde y_t\log(p_t)+(n_t-\tilde y_t)\log(1-p_t)\right)
\end{align*}

2.  La dérivée de la log-vraisemblance par rapport à $p_t$ s'écrit
$$\frac{\tilde y_t}{p_t}-\frac{n_t-\tilde y_t}{1-p_t}.$$
Cette dérivée s'annule pour
$$\widehat p_t=\frac{\tilde y_t}{n_t}.$$

3.  On note $\widehat \beta$ l'EMV du modèle logistique et $p_{\widehat\beta}$ le vecteur qui contient les valeurs ajustées $p_{\widehat\beta}(x_t),t=1,\dots,T$. On a pour tout $\beta\in\mathbb R^p$ :
$$\mathcal L(Y,\beta)\leq\mathcal L(Y,\widehat\beta)=\mathcal L_{\text{sat}}(Y,p_{\widehat\beta})\leq L_{\text{sat}}(Y,\widehat p_t).$$

:::


::: {#exr-12-9 name="Résidus partiels"}

1.  
    ```{python}
    artere = pd.read_csv('../donnees/artere.txt', header=0, index_col=0, sep=' ')
    modele = smf.glm('chd~age', data=artere, family=sm.families.Binomial()).fit()
    B0 = modele.params
    OriginalDeviance = modele.deviance
    ```

2.  
    ```{python}
    alpha=0.05
    ```

3.
    ```{python}
    stderr = modele.cov_params().iloc[1,1]**0.5
    from scipy import stats
    delta = stats.chi2.ppf(1-alpha/4, df=1)**0.5*stderr/5
    grille = B0[1] + np.arange(-10,11)*delta
    ```

4.  On a
  \begin{align*}
\mathcal D_1&=-2(\mathcal L(Y,\hat\beta)-\mathcal L_{sat})
  \end{align*}
Pour celle avec l'offset $K_i=x_i\beta_2^*$ elle vaut 
  \begin{align*}
\mathcal D_o&=-2(\mathcal L(Y,K,\hat\beta_1)-\mathcal L_{sat})
  \end{align*}
où $\hat \beta_1$ maximise $\mathcal L(Y,K,\hat\beta_1)$ c'est à dire $\mathcal L(Y,K,\hat\beta_1)=l(\beta_2^*)$  et nous avons donc
  \begin{align*}
\mathcal D_o - \mathcal D_1= 2(\mathcal L(Y,\hat\beta)-\mathcal L(Y,K,\beta_1)= 2(\mathcal L(Y,\hat\beta)-l(\beta_2^*))=P(\beta_2^*).
  \end{align*}

5.  
    ```{python}
    profil2 = []
    for valgrille in grille:
        modeleo = smf.glm('chd~1', data=artere, offset=artere.age*valgrille, family=sm.families.Binomial()).fit()
        if (modeleo.deviance -  OriginalDeviance)<0:
            profil2.append(0)
        else:
            profil2.append(modeleo.deviance -  OriginalDeviance)    
    ```

6.  
    ```{python}
    profil = np.sign(np.arange(-10,11))*np.sqrt(profil2)
    ```

7.
    ```{python}
    f = interpolate.interp1d(profil, grille)
    xnew = [-np.sqrt(stats.chi2.ppf(1-alpha, df=1)),
            np.sqrt(stats.chi2.ppf(1-alpha, df=1)) ]
    print(f(xnew))
    ```

8.  
    ```{python}
    print(modele.conf_int().iloc[1,:])
    ```

:::


