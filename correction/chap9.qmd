---
title: "Régularisation des moindres carrés : ridge, lasso, elastic-net"
toc: true
---

::: {.content-hidden}
{{< include ../macros.tex >}}
:::

```{python}
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import statsmodels.api as sm
from sklearn.linear_model import Lasso, LassoCV
from sklearn.linear_model import Lasso, lasso_path
```

::: {#exr-9-1 name="Questions de cours"}
A, B, B, B, A (pour un bon choix de $\lambda$) et B, A, C et D.
:::

::: {#exr-9-2 name="Projection et régression ridge"}

:::

::: {#exr-9-3 name="Variance des valeurs ajustées avec une régression ridge"}

:::

::: {#exr-9-4 name="Nombre effectif de paramètres de la régression ridge"}
1.  Rappelons que pour une valeur $\kappa$ donnée, le vecteur de coefficients de la régression ridge s'écrit
$$
\hat \beta_{\mathrm{ridge}}(\kappa) = (X'X + \kappa I)^{-1}X'Y.
$$
et donc l'ajustement par la régression ridge est
$$
\hat Y_{\mathrm{ridge}}(\kappa)=X(X'X + \kappa I)^{-1}X'Y=H^*(\kappa)Y
$$

2.  Soit $U_i$ le vecteur propre de $A$ associé à la valeur propre $d^2_i$. Nous avons donc par définition que
$$
\begin{eqnarray*}
AU_i&=&d^2_iU_i\\
AU_i+\lambda U_i&=&d^2_iU_i+\lambda U_i=(d^2_i+\lambda) U_i\\
(A+\lambda I_p)U_i&=&(d^2_i+\lambda) U_i,
\end{eqnarray*}
$$
c'est-à-dire que $U_i$ est aussi vecteur propre de $A+\lambda I_p$ associé à la valeur propre $\lambda+d^2_i$.

3.  Nous savons que $X=QD P'$ avec $Q$ et $P$ matrices orthogonales et $D=\diag(d_1,\dotsc,d_p)$. Puisque $Q$ est orthogonale, nous avons, par définition, $Q'Q=I$. Nous avons donc que $X'X=(QD P')'QD P'=PDQ'QDP'=PD^2P'$. Puisque $P$ est orthogonale $P'P=I_p$ et $P^{-1}=P$.
$$
\begin{eqnarray*}
\tr(X(X'X+\lambda I_p)^{-1}X')&=&\tr((X'X+\lambda I_p)^{-1}X'X)\\
&=&\tr((PD^2P'+\lambda PP')^{-1}PD^2P')\\
&=&\tr((P(D+\lambda I_p )P')^{-1}PD^2P').
\end{eqnarray*}
$$
Ainsi
$$
\begin{eqnarray*}
\tr(X(X'X+\lambda I_p)^{-1}X')&=&\tr( (P')^{-1}(D+\lambda I_p )^{-1} P^{-1} PD^2P')\\
&=&\tr( (P')^{-1}(D+\lambda I_p )^{-1} D^2P')\\
&=&\tr( (D+\lambda I_p )^{-1} D^2).
\end{eqnarray*}
$$
Selon la définition de $H^*(\kappa)$, nous savons que sa trace vaut donc
$$
\begin{eqnarray*}
\tr( (D+\kappa I_p )^{-1} D^2).
\end{eqnarray*}
$$
Comme $D$ et $I_p$ sont des matrices diagonales, leur somme et produit sont simplement leur somme et produit terme à terme des éléments de la
diagonale, et donc cette trace (somme des éléments de la diagonale) vaut
$$
\sum_{i=1}^{p}{\frac{d_j^2}{d_j^2+\kappa}}.
$$
:::

::: {#exr-9-5 name="Estimateurs à rétrecissement - shrinkage"}
1.  Soit le modèle de régression
$$
Y=X\beta+\varepsilon.
$$
En le pré-multipliant par $P$, nous avons
$$
Z=PY=PX\beta+P\varepsilon=DQ\beta+\eta=D\gamma+\eta.
$$
Puisque $\varepsilon\sim\mathcal{N}(0,\sigma^2 I_n)$ et $P$ fixé, nous avons que $\eta=P\varepsilon$ suit une loi normale de moyenne $\E(\eta)=P\E(\varepsilon)=0$ et de variance $\V(\eta)=P\V(\varepsilon)P'=\sigma^2PP'=\sigma^2I_n$.
    
    Par définition, $Z$ vaut $PY$ et nous savons que $Y\sim\mathcal{N}(X\beta,\sigma^2 I_n)$, donc $Z\sim\mathcal{N}(PX\beta,\sigma^2 PP')$, c'est-à-dire $Z\sim\mathcal{N}(DQ\beta,\sigma^2 I_n)$ ou encore $Z\sim\mathcal{N}(D\gamma,\sigma^2 I_n)$. En utilisant la valeur de $D$ nous avons
$$    
\begin{eqnarray*}
D\gamma&=&
\begin{pmatrix}
  \Delta \gamma\\
0
\end{pmatrix}.
\end{eqnarray*}
$$
Donc $Z_{1:p}\sim\mathcal{N}(\Delta\gamma,\sigma^2I_p)$.

2.  Soit un estimateur de $\beta$ linéaire en $Y$~: $\hat \beta=AY$. Soit l'estimateur de $\gamma=Q\beta$ linéaire en $Y$~: $\hat\gamma=Q AY$. Pour calculer leur matrice de l'EQM, nous devons calculer leur biais et leur variance. Le biais de $\hat \beta$ est
$$
B(\hat \beta)=\E(\hat \beta)-\beta=\E(AY)-\beta=A\E(Y)-\beta=AX\beta-\beta.
$$
Le biais de $\hat\gamma$ s'écrit
$$
B(\hat\gamma)=\E(\hat \gamma)-\gamma=\E(Q\hat \beta)-\gamma=Q\E(\hat \beta)-\gamma=QAX\beta-\gamma.
$$
Comme $\gamma=Q\beta$ et $Q'Q=I_p$ nous avons
$$
B(\hat\gamma)=QAXQ'\gamma-\gamma.
$$
La variance de $\hat \beta$ s'écrit
$$
\V(\hat \beta)=\V(AY)=A\V(Y)A'=\sigma^2 AA',
$$
et celle de $\hat \gamma$ est
$$
\V(\hat\gamma)=\V(Q\hat \beta)=Q\V(\hat \beta)Q'=\sigma^2 QAA'Q'.
$$
Nous en déduisons que les matrices des EQM sont respectivement
$$
\begin{eqnarray*}
\EQM(\hat \beta)&=&(AX\beta-\beta)(AX\beta-\beta)'+\sigma^2 AA',\\
\EQM(\hat \gamma)&=&(QAXQ'\gamma-\gamma)(QAXQ'\gamma-\gamma)' + \sigma^2 QAA'Q',
\end{eqnarray*}
$$
et enfin les traces de ces matrices s'écrivent
$$
\begin{eqnarray*}
\tr(\EQM(\hat \beta))&=&(AX\beta-\beta)'(AX\beta-\beta)+\sigma^2\tr(AA'),\\
\tr(\EQM(\hat \gamma))&=&(QAXQ'\gamma-\gamma)'(QAXQ'\gamma-\gamma)+ \sigma^2\tr(AA').\\
\end{eqnarray*}
$$
Rappelons que $\gamma=Q\beta$ et que $Q'Q=I_p$, nous avons donc
$$
\begin{eqnarray*}
\tr(\EQM(\hat \gamma))&=&\gamma'(QAXQ'-I_p)'(QAXQ'-I_p)\gamma+ \sigma^2\tr(AA')\\
&=&\beta'(QAX - Q)'(QAX - Q)\beta+ \sigma^2\tr(AA')\\
&=&\beta'(AX-I_p)Q'Q(AX-I_p)\beta+ \sigma^2\tr(AA')\\
&=&\beta'(AX-I_p)(AX-I_p)\beta+ \sigma^2\tr(AA')=\tr(\EQM(\hat \beta)).
\end{eqnarray*}
$$
En conclusion, que l'on s'intéresse à un estimateur linéaire de $\beta$ ou à un estimateur linéaire de $\gamma$, dès que l'on passe de
l'un à l'autre en multipliant par $Q$ ou $Q'$, matrice orthogonale, la trace de l'EQM est identique, c'est-à-dire que les performances globales des 2 estimateurs sont identiques.

3.  Nous avons le modèle de régression suivant~:
$$
Z_{1:p}=\Delta\gamma+\eta_{1:p},
$$
et donc, par définition de l'estimateur des MC, nous avons
$$
\hat \gamma_{\mathrm{MC}}=(\Delta'\Delta)^{-1}\Delta'Z_{1:p}.
$$
Comme $\Delta$ est une matrice diagonale, nous avons
$$
\hat \gamma_{\mathrm{MC}}=\Delta^{-2}\Delta'Z_{1:p}=\Delta^{-1}Z_{1:p}.
$$
Cet estimateur est d'expression très simple et il est toujours défini de manière unique, ce qui n'est pas forcément le cas de $\hat \beta_{\mathrm{MC}}$.


    Comme $Z_{1:p}\sim\mathcal{N}(\Delta\gamma,\sigma^2 I_p)$ nous avons que $\hat \gamma_{\mathrm{MC}}=\Delta^{-1}Z_{1:p}$ suit une loi normale d'espérance $\E(\Delta^{-1}Z_{1:p})=\Delta^{-1}\E(Z_{1:p})=\gamma$ et de variance $\V(\hat \gamma_{\mathrm{MC}})=\sigma^2\Delta^{-2}$. Puisque $\hat \gamma_{\mathrm{MC}}$ est un estimateur des MC, il est sans biais, ce qui est habituel.

4.  L'EQM de $\hat \gamma_{\mathrm{MC}}$, estimateur sans biais, est simplement sa variance. Pour la $i^e$ coordonnée de
$\hat \gamma_{\mathrm{MC}}$, l'EQM est égal à l'élément $i,i$ de la matrice de variance $\V(\hat \gamma_{\mathrm{MC}})$, c'est-à-dire
$\sigma^2/\delta_i^2$. La trace de l'EQM est alors simplement la somme, sur toutes les coordonnées $i$, de cet EQM obtenu.

5.  Par définition $\hat \gamma(c)=\diag(c_i)Z_{1:p}$ et nous savons que $Z_{1:p}\sim\mathcal{N}(\Delta\gamma,\sigma^2 I_p).$ Nous obtenons que $\hat \gamma(c)$ suit une loi normale d'espérance $\E(\diag(c_i)Z_{1:p})=\diag(c_i)\Delta\gamma$ et de variance
$$
\V(\hat \gamma(c))= \diag(c_i)\V(Z_{1:p})\diag(c_i)'= \sigma^2\diag(c_i^2).
$$
La loi de $\hat \gamma(c)$ étant une loi normale de matrice de variance diagonale, nous en déduisons que les coordonnées
  de $\hat \gamma(c)$ sont indépendantes entre elles.
  
6.  Calculons l'EQM de la $i^e$ coordonnée de $\hat \gamma(c)$
$$
\EQM(\hat \gamma(c)_i)=\E(\hat \gamma(c)_i -\gamma)^2=\E(\hat \gamma(c)_i^2)+
\E(\gamma_i^2)-2\E(\hat \gamma(c)_i \gamma_i).
$$
Comme $\gamma_i$ et que $\E(\hat \gamma(c)_i^2)=\V(\hat \gamma(c)_i^2)+\{\E(\hat \gamma(c)_i^2)\}^2$, nous avons
$$
\begin{align*}
\EQM(\hat \gamma(c)_i)&=\sigma^2 c_i^2+(c_i\delta_i\gamma_i)^2+\gamma_i^2-2\gamma_i\E(\hat \gamma(c)_i)\\
&=\sigma^2 c_i^2+(c_i\delta_i\gamma_i)^2+\gamma_i^2-2\sigma^2 c_i\delta_i\gamma_i= \sigma^2c_i^2+\gamma_i^2(c_i\delta_i -1)^2.
\end{align*}
$$
7.  De manière évidente si $\gamma_i^2$ diminue, alors l'EQM de $\hat \gamma(c)_i$ diminue aussi. Calculons la valeur de l'EQM quand
$\gamma_i^2=\frac{\sigma^2}{\delta_i^2}\frac{(1/\delta_i)+c_i}{(1/\delta_i)-c_i}$. Nous avons, grâce à la question précédente,
$$
\begin{eqnarray*}
\EQM(\hat \gamma(c)_i)&=&\sigma^2 c_i^2+(c_i\delta_i -1)^2\frac{\sigma^2}{\delta_i^2}\frac{(1/\delta_i)+c_i}{(1/\delta_i)-c_i}\\
&=&\sigma^2 c_i^2+\frac{\sigma^2}{\delta_i^2}(1 - c_i\delta_i)^2\frac{1+\delta_ic_i}{1-\delta_ic_i}\\
&=&\sigma^2 c_i^2+\frac{\sigma^2}{\delta_i^2}(1 - c_i\delta_i)(1+\delta_ic_i)\\
&=&\sigma^2 c_i^2+\frac{\sigma^2}{\delta_i^2}(1-\delta_i^2c_i^2)\\
&=&\sigma^2 c_i^2+\frac{\sigma^2}{\delta_i^2}-\sigma^2c_i^2=\frac{\sigma^2}{\delta_i^2}\\
&=&\EQM(\hat \gamma_{\mathrm{MC}}),
\end{eqnarray*}
$$
d'où la conclusion demandée.

8.  Par définition de $\hat \gamma(c)$, nous avons
$$
\begin{eqnarray*}
\hat \gamma(c)&=&\diag(c_i)Z_{1:p}=\diag(\frac{\delta_i}{\delta_i^2+\kappa})Z_{1:p}\\
&=&(\Delta'\Delta + \kappa I_p)^{-1}\Delta'Z_{1:p},
\end{eqnarray*}
$$
puisque $\Delta$ est diagonale. De plus nous avons
$$
D =
\bigl( \begin{smallmatrix}
  \Delta\\
0
\end{smallmatrix}\bigr),
$$
ce qui entraîne que $D'D=\Delta'\Delta$ et $D'Z=\Delta' Z_{1:p}$.
Nous obtenons donc
$$
\hat \gamma(c)=(D'D+\kappa I_p)^{-1}D'Z.
$$
Rappelons que $D=PXQ'$ avec $P$ et $Q$ matrices orthogonales, nous avons
alors
$$
\begin{eqnarray*}
\hat \gamma(c)&=&(QX'P'PXQ' + \kappa I_p)^{-1} D'Z=(QX'XQ' + \kappa QQ')^{-1}D'Z\\
&=&(Q(X'X  + \kappa I_p)Q')^{-1}D'Z=(Q')^{-1}(X'X  + \kappa I_p)^{-1}(Q)^{-1}D'Z\\
&=&Q(X'X  + \kappa I_p)^{-1}Q'D'Z.
\end{eqnarray*}
$$
Comme $Z=PY$ et $D=PXQ'$, nous avons
$$
\hat \gamma(c)=Q(X'X  + \kappa I_p)^{-1}Q' QX'P' PY=Q(X'X  + \kappa I_p)^{-1}XY.
$$
Enfin, nous savons que $Q\hat\gamma=\hat \beta$, nous en déduisons que $\hat\gamma=Q'\hat \beta$ et donc que dans le cas particulier où $c_i=\frac{\delta_i}{\delta_i^2+\kappa}$ nous obtenons
$$
\hat \beta=Q\hat \gamma(c)=(X'X  + \kappa I_p)^{-1}XY,
$$
c'est-à-dire l'estimateur de la régression ridge.
:::

::: {#exr-9-6 name="Coefficient constant et régression sous contraintes"}

:::

::: {#exr-9-7 name="Unicité pour la régression lasso, Giraud (2014)"}

:::

::: {#exr-9-8 name="Traitement d'un signal"}
1.  

    ```{python}
    signal = pd.read_csv("../donnees/courbe_lasso.csv")
    donnees = pd.read_csv("../donnees/echan_lasso.csv")

    plt.figure(figsize=(10, 6))
    plt.plot(signal['x'], signal['y'], label='Signal')
    plt.scatter(donnees['X'], donnees['Y'], color='red', label='Données')

    plt.xlabel('x')
    plt.ylabel('y')
    plt.legend()
    plt.title('Signal et Données')

    plt.show()
    ```

2.  Nous cherchons à reconstruire le signal à partir de l'échantillon. Bien entendu, vu la forme du signal, un modèle linéaire de la forme 
$$
y_i=\beta_0+\beta_1x_i+\varepsilon_i
$$ 
n'est pas approprié. De nombreuses approches en **traitement du signal** proposent d'utiliser une `base` ou `dictionnaire` représentée par une collection de fonctions $\{\psi_j(x)\}_{j=1,\dots,K}$ et de décomposer le signal dans cette base :
$$
m(x)\approx \sum_{j=1}^K \beta_j\psi_j(x).
$$
Pour un dictionnaire donné, on peut alors considérer un **modèle linéaire**
$$
  y_i=\sum_{j=1}^K \beta_j\psi_j(x_i)+\varepsilon_i.
$$ {#eq-mod-lin-signal}
Le problème est toujours d'estimer les paramètres $\beta_j$ mais les variables sont maintenant définies par les éléments du dictionnaire. Il existe différents types de dictionnaire, dans cet exercice nous proposons de considérer la base de Fourier définie par
$$
\psi_0(x)=1,\quad \psi_{2j-1}(x)=\cos(2j\pi x)\quad\text{et}\quad \psi_{2j}(x)=\sin(2j\pi x),\quad j=1,\dots,K.
$$

3.  

    ```{python}
    def mat_dict(K, x):
        # Initialiser une matrice de zéros avec la taille appropriée
        res = np.zeros((len(x), 2 * K))
        
        # Remplir la matrice avec les valeurs cos et sin
        for j in range(1, K + 1):
            res[:, 2 * j - 2] = np.cos(2 * j * np.pi * x)
            res[:, 2 * j - 1] = np.sin(2 * j * np.pi * x)
        
        # Convertir la matrice en DataFrame pour un usage similaire à tibble
        res_df = pd.DataFrame(res)
        
        return res_df
    ```

4.  Il suffit d'ajuster le modèle linéaire où les variables explicatives sont données par le dictionnaire :

    ```{python}
    # Créer le dictionnaire pour les données
    D25 = mat_dict(25, donnees['X'])
    D25['Y'] = donnees['Y']

    # Ajuster le modèle linéaire
    X = sm.add_constant(D25.drop(columns='Y'))
    y = D25['Y']
    mod_lin = sm.OLS(y, X).fit()

    # Créer le dictionnaire pour le signal
    S25 = mat_dict(25, signal['x'])

    # Faire des prédictions
    S25 = sm.add_constant(S25)
    prev_MCO = mod_lin.predict(S25)

    # Préparer les données pour le tracé
    signal1 = signal.copy()
    signal1['MCO'] = prev_MCO
    signal1 = signal1.rename(columns={'y': 'signal'})
    signal2 = signal1.melt(id_vars=['x'], value_vars=['signal', 'MCO'], var_name='meth', value_name='y')

    # Tracer les résultats
    plt.figure(figsize=(10, 6))
    for key, grp in signal2.groupby('meth'):
        plt.plot(grp['x'], grp['y'], label=key)
    plt.scatter(donnees['X'], donnees['Y'], color='red', label='Données')
    plt.ylim(-2, 2)
    plt.xlabel('x')
    plt.ylabel('y')
    plt.legend()
    plt.title('Signal et Données')
    plt.show()
    ```

    Le signal estimé a tendance à surajuster les données. Cela vient du fait qu'on estime 51 paramètres avec seulement 60 observations.

5.  On regarde tout d'abord le `chemin de régularisation` des estimateurs **lasso**

    ```{python}
    X_25 = D25.drop(columns='Y').values
    y_25 = D25['Y'].values

    # Ajuster le modèle Lasso et obtenir le chemin de régularisation
    alphas, coefs, _ = lasso_path(X_25, y_25, alphas=np.logspace(-4, 0, 100))

    # Tracer le chemin de régularisation
    plt.figure(figsize=(10, 6))
    # Tracer les coefficients pour chaque alpha
    for coef in coefs:
        plt.plot(-np.log10(alphas), coef)

    plt.xlabel('-log(alpha)')
    plt.ylabel('Coefficients')
    plt.title('Lasso Paths')
    plt.axis('tight')
    plt.show()
    ```
    
    Il semble que quelques coefficients quittent la valeur 0 bien avant les autres. On effectue maintenant la validation croisée pour sélectionner le paramètre $\lambda$.

    ```{python}
    lasso_cv = LassoCV(cv=10, random_state=1234).fit(X_25, y_25)

    # Tracer les résultats de la validation croisée
    m_log_alphas = -np.log10(lasso_cv.alphas_)

    plt.figure(figsize=(10, 6))
    plt.plot(m_log_alphas, lasso_cv.mse_path_, ':')
    plt.plot(m_log_alphas, lasso_cv.mse_path_.mean(axis=-1), 'k', label='Average across the folds', linewidth=2)
    plt.axvline(-np.log10(lasso_cv.alpha_), linestyle='--', color='k', label='alpha: CV estimate')

    plt.xlabel('-log(alpha)')
    plt.ylabel('Mean square error')
    plt.title('Lasso CV Paths')
    plt.legend()
    plt.axis('tight')
    plt.show()
    ```

    ```{python}
    S25 = mat_dict(25, signal['x'])
    prev_lasso = lasso_cv.predict(S25)
    signal1['lasso'] = prev_lasso
    signal1 = signal1.rename(columns={'y': 'signal'})
    signal2 = signal1.melt(id_vars=['x'], value_vars=['signal','MCO' ,'lasso'], var_name='meth', value_name='y')
    # Tracer les résultats
    plt.figure(figsize=(10, 6))
    for key, grp in signal2.groupby('meth'):
        plt.plot(grp['x'], grp['y'], label=key)
    plt.scatter(donnees['X'], donnees['Y'], color='red', label='Données')
    plt.ylim(-2, 2)  # Fixer les limites de l'axe des ordonnées
    plt.xlabel('x')
    plt.ylabel('y')
    plt.legend()
    plt.title('Signal et Données avec Lasso')
    plt.show()

    ```


```{python}
#| echo: false
#| eval: false 

# Obtenir les coefficients du modèle Lasso ajusté
coefficients = lasso_cv.coef_

# Trouver les indices des coefficients non nuls
v_sel = np.where(coefficients != 0)[0]

# Afficher les indices des coefficients non nuls
print(v_sel)
```


:::

::: {#exr-9-9 name="GridSearchCV"}

:::

::: {#exr-9-10 name="Variablilité de la validation croisée $K$ blocs"}

1.  Il suffit d'écrire 
$$
\bar{\mathcal{R}}=\frac{1}{n}\sum_{k=1}^K \sum_{i\in\mathcal I_k}(y_i-\hat m_k(x_i))^2=\frac{1}{K}\sum_{k=1}^K\frac{n}{K}\sum_{i\in\mathcal I_k}(y_i-\hat m_k(x_i))^2.
$$

2.  Les données étant i.i.d. et les blocs étant de taille égale, on a $\V(\mathcal R_1)=\V(\mathcal R_2)=\dots=\V(\mathcal R_K)$. On obtient donc le résutat demandé en négligeant les covariances.

3.  On estime la variance d'un bloc $\V(\mathcal R_1)$ par la variance empirique des erreurs sur chaque bloc :
$$
\frac{1}{K-1}\sum_{k=1}^K(\mathcal{R}_k - \bar{\mathcal{R}})^{2}.
$$
En utilisant la question précédente, on estime la variance de $\bar{\mathcal{R}}$ par
$$
\V(\bar{\mathcal{R}}) = \frac{1}{K(K-1)}\sum_{k=1}^K(\mathcal{R}_k - \bar{\mathcal{R}})^{2}.
$$
:::

