{
  "hash": "e5b0695749992e3ac766b0c688ed778c",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Régression logistique\"\ntoc: true\n---\n\n\n\n\n::: {.content-hidden}\n\n\\DeclareMathOperator{\\C}{Cov}\n\\DeclareMathOperator{\\V}{V}\n\\DeclareMathOperator{\\leR}{R^2}\n\\newcommand{\\M}{\\mathcal M}\n\\DeclareMathOperator{\\MCO}{MCO}\n\\DeclareMathOperator{\\SC}{SC}\n\\DeclareMathOperator{\\SCR}{SCR}\n\\DeclareMathOperator{\\SCT}{SCT}\n\\DeclareMathOperator{\\SCE}{SCE}\n\\DeclareMathOperator{\\EQM}{EQM}\n\\DeclareMathOperator{\\ro}{R^2_0}\n\\DeclareMathOperator{\\CMR}{CMR}\n\\DeclareMathOperator{\\CME}{CME}\n\\DeclareMathOperator{\\radeux}{R^2_a}\n\\DeclareMathOperator{\\Cp}{C_p}\n\n\n\\newcommand{\\1}{\\mathbf{1}}\n\\newcommand{\\un}{\\mathbf{1}}\n\\newcommand{\\D}{\\displaystyle}\n\n\n\\newcommand{\\prob}{\\mathbf P}\n\\newcommand{\\argmin}{\\mathop{\\mathrm{argmin}}}\n\\newcommand{\\ind}{\\mathbf 1}\n\\newcommand{\\R}{\\mathbb R}\n\\newcommand{\\E}{\\mathbf E}\n\\newcommand{\\var}{\\mathbf V}\n\\newcommand{\\ps}[2]{\\langle #1,#2\\rangle}\n\\newcommand{\\card}[1]{|{#1}|}\n\\newcommand{\\cov}{\\mathbf{cov}}\n\\newcommand{\\corr}{\\text{corr}}\n\\newcommand{\\AUC}{\\text{AUC}}\n\\newcommand{\\logit}{\\text{logit}}\n\\newcommand{\\diag}{\\text{diag}}\n\\newcommand{\\tr}{\\text{tr}}\n\\newcommand{\\li}[2]{#1_{#2}}\n\\newcommand{\\ssli}[2]{#1_{(#2)}}\n\\newcommand{\\HH}{{\\mathcal{H}}}\n\\newcommand{\\Hz}{{\\mathrm{H_0}}}\n\\newcommand{\\sfrac}[2]{{#1}/{#2}}\n\n\n\n:::\n\n::: {#822b9bd8 .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom statsmodels.stats.anova import anova_lm\nimport matplotlib.pyplot as plt\nfrom scipy.stats import chi2,norm\nfrom scipy import interpolate\n```\n:::\n\n\n::: {#exr-12-1 name=\"Questions de cours\"}\n1.  A\n2.  A\n3.  B\n4.  A\n5.  A\n6.  A\n7.  B\n8.  A\n:::\n\n::: {#exr-12-2 name=\"Interprétation des coefficients\"}\n1.  On génère l'échantillon.\n\n\n    ::: {#b9830709 .cell execution_count=2}\n    ``` {.python .cell-code}\n    n = 100\n    np.random.seed(48967365)\n    X = np.random.choice(['A', 'B', 'C'], size=n, replace=True)\n    Y = np.zeros(n, dtype=int)\n    np.random.seed(487365)\n    Y[X == 'A'] = np.random.binomial(1, 0.95, size=np.sum(X == 'A'))\n    np.random.seed(4878365)\n    Y[X == 'B'] = np.random.binomial(1, 0.95, size=np.sum(X == 'B'))\n    np.random.seed(4653965)\n    Y[X == 'C'] = np.random.binomial(1, 0.05, size=np.sum(X == 'C'))\n    donnees = pd.DataFrame({'Y': Y, 'X': X})\n    print(donnees.head())\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n       Y  X\n    0  1  A\n    1  0  C\n    2  1  A\n    3  1  B\n    4  0  A\n    ```\n    :::\n    :::\n    \n    \n2.  On ajuste le modèle avec les contraintes par défaut.\n\n\n    ::: {#295305d1 .cell execution_count=3}\n    ``` {.python .cell-code}\n    model1 = smf.logit('Y ~ X', data=donnees).fit()\n    print(model1.summary())\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    Optimization terminated successfully.\n             Current function value: 0.297451\n             Iterations 7\n                               Logit Regression Results                           \n    ==============================================================================\n    Dep. Variable:                      Y   No. Observations:                  100\n    Model:                          Logit   Df Residuals:                       97\n    Method:                           MLE   Df Model:                            2\n    Date:                Tue, 04 Feb 2025   Pseudo R-squ.:                  0.5521\n    Time:                        12:22:32   Log-Likelihood:                -29.745\n    converged:                       True   LL-Null:                       -66.406\n    Covariance Type:            nonrobust   LLR p-value:                 1.197e-16\n    ==============================================================================\n                     coef    std err          z      P>|z|      [0.025      0.975]\n    ------------------------------------------------------------------------------\n    Intercept      1.9459      0.478      4.070      0.000       1.009       2.883\n    X[T.B]         0.5798      0.877      0.661      0.508      -1.138       2.298\n    X[T.C]        -4.6868      0.872     -5.373      0.000      -6.396      -2.977\n    ==============================================================================\n    ```\n    :::\n    :::\n    \n    \n    On obtient les résultats du **test de Wald** sur la nullité des paramètres $\\beta_0,\\beta_2$ et $\\beta_3$.\n\n3.  On change la modalité de référence.\n\n\n    ::: {#decd4891 .cell execution_count=4}\n    ``` {.python .cell-code}\n    model2 = smf.logit('Y ~ C(X, Treatment(reference=\"C\"))', data=donnees).fit()\n    print(model2.summary())\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    Optimization terminated successfully.\n             Current function value: 0.297451\n             Iterations 7\n                               Logit Regression Results                           \n    ==============================================================================\n    Dep. Variable:                      Y   No. Observations:                  100\n    Model:                          Logit   Df Residuals:                       97\n    Method:                           MLE   Df Model:                            2\n    Date:                Tue, 04 Feb 2025   Pseudo R-squ.:                  0.5521\n    Time:                        12:22:32   Log-Likelihood:                -29.745\n    converged:                       True   LL-Null:                       -66.406\n    Covariance Type:            nonrobust   LLR p-value:                 1.197e-16\n    =======================================================================================================\n                                              coef    std err          z      P>|z|      [0.025      0.975]\n    -------------------------------------------------------------------------------------------------------\n    Intercept                              -2.7408      0.730     -3.757      0.000      -4.171      -1.311\n    C(X, Treatment(reference=\"C\"))[T.A]     4.6868      0.872      5.373      0.000       2.977       6.396\n    C(X, Treatment(reference=\"C\"))[T.B]     5.2666      1.035      5.086      0.000       3.237       7.296\n    =======================================================================================================\n    ```\n    :::\n    :::\n    \n    \n    On obtient les résultats du **test de Wald** sur la nullité des paramètres $\\beta_0,\\beta_1$ et $\\beta_2$.\n\n4.  On remarque que dans **model1** on accepte la nullité de $\\beta_2$ alors qu'on la rejette dans **model2**. Ceci est logique dans la mesure où ces tests dépendent de la contrainte identifiante choisie. Dans **model1** le test de nullité de $\\beta_2$ permet de vérifier si $B$ à un effet similaire à $A$ sur $Y$. Dans **model2**, on compare l'effet de $B$ à celui de $C$. On peut donc conclure $A$ et $B$ ont des effets proches sur $Y$ alors que $B$ et $C$ ont un impact différent. Ceci est logique vu la façon dont les données ont été générées.\n\n5.  Tester l'effet global de $X$ sur $Y$ revient à tester si les coefficients $\\beta_1,\\beta_2$ et $\\beta_3$ sont égaux, ce qui, compte tenu des contraintes revient à considérer les hypothèses nulles :\n\n    -   $\\beta_2=\\beta_3=0$ dans **model1** ;\n    -   $\\beta_1=\\beta_2=0$ dans **model2**.\n\n    On peut effectuer les tests de **Wald** ou du **rapport de vraisemblance**. On obtient les résultats du **rapport de vraisemblance** avec :\n\n\n    ::: {#62143931 .cell execution_count=5}\n    ``` {.python .cell-code}\n    lr_test_model1 = model1.llr\n    lr_test_model2 = model2.llr\n    \n    print(\"Test de rapport de vraisemblance pour model1:\")\n    print(f\"LR stat: {lr_test_model1:.4f}, p-value: {model1.llr_pvalue:.4f}\")\n    \n    print(\"\\nTest de rapport de vraisemblance pour model2:\")\n    print(f\"LR stat: {lr_test_model2:.4f}, p-value: {model2.llr_pvalue:.4f}\")\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    Test de rapport de vraisemblance pour model1:\n    LR stat: 73.3227, p-value: 0.0000\n    \n    Test de rapport de vraisemblance pour model2:\n    LR stat: 73.3227, p-value: 0.0000\n    ```\n    :::\n    :::\n    \n    \n    On remarque ici que ces deux tests sont identiques : ils ne dépendent pas de la contrainte identifiante choisie.\n:::\n\n::: {#exr-12-3 name=\"Séparabilité\"}\n1.  On génère l'échantillon demandé.\n\n\n    ::: {#0bc3e4a5 .cell execution_count=6}\n    ``` {.python .cell-code}\n    np.random.seed(1234)\n    X = np.concatenate([np.random.uniform(-1, 0, 50), np.random.uniform(0, 1, 50)])\n    Y = np.concatenate([np.zeros(50), np.ones(50)])\n    df = pd.DataFrame({'X': X, 'Y': Y})\n    print(df.head())\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n              X    Y\n    0 -0.808481  0.0\n    1 -0.377891  0.0\n    2 -0.562272  0.0\n    3 -0.214641  0.0\n    4 -0.220024  0.0\n    ```\n    :::\n    :::\n    \n    \n2.  Le graphe s'obtient avec :\n\n\n    ::: {#f600f279 .cell execution_count=7}\n    ``` {.python .cell-code}\n    beta = np.arange(0, 100, 0.01)\n    def log_vrais(X, Y, beta):\n        LV = np.zeros(len(beta))\n        for i in range(len(beta)):\n            Pbeta = np.exp(beta[i] * X) / (1 + np.exp(beta[i] * X))\n            LV[i] = np.sum(Y * X * beta[i] - np.log(1 + np.exp(X * beta[i])))\n        return LV\n    LL = log_vrais(df['X'], df['Y'], beta)\n    ```\n    :::\n    \n    \n\n    ::: {#49d3a164 .cell execution_count=8}\n    ``` {.python .cell-code}\n    plt.plot(beta,LL)\n    plt.xlabel('beta')\n    plt.ylabel('Log-vraisemblance')\n    plt.title('Log-vraisemblance en fonction de beta')\n    plt.show()\n    ```\n    \n    ::: {.cell-output .cell-output-display}\n    ![](chap12_files/figure-html/cell-9-output-1.png){width=596 height=449}\n    :::\n    :::\n    \n    \n3.  On obtient un avertissement qui nous dit que l'algorithme d'optimisation n'a pas convergé.\n\n\n    ::: {#2ffe54b3 .cell message='true' execution_count=9}\n    ``` {.python .cell-code}\n    model = smf.logit('Y ~ X - 1', data=df).fit()\n    print(model.params)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    Warning: Maximum number of iterations has been exceeded.\n             Current function value: 0.000000\n             Iterations: 35\n    X    1801.824972\n    dtype: float64\n    ```\n    :::\n    \n    ::: {.cell-output .cell-output-stderr}\n    ```\n    /opt/miniconda3/lib/python3.12/site-packages/statsmodels/discrete/discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n      return 1/(1+np.exp(-X))\n    /opt/miniconda3/lib/python3.12/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n      warnings.warn(\"Maximum Likelihood optimization failed to \"\n    ```\n    :::\n    :::\n    \n    \n4.  Le changement proposé supprime la séparabilité des données. On obtient bien un maximum fini pour cette nouvelle vraisemblance.\n\n\n    ::: {#57913488 .cell execution_count=10}\n    ``` {.python .cell-code}\n    Y1 = Y.copy()\n    Y1[0] = 1\n    LL1 = log_vrais(X, Y1, beta)\n    plt.plot(beta, LL1)\n    plt.xlabel('beta')\n    plt.ylabel('Log-vraisemblance')\n    plt.title('Log-vraisemblance en fonction de beta pour Y1')\n    plt.show()\n    ```\n    \n    ::: {.cell-output .cell-output-display}\n    ![](chap12_files/figure-html/cell-11-output-1.png){width=596 height=449}\n    :::\n    :::\n    \n    \n:::\n\n\n::: {#exr-12-4 name=\"Matrice hessienne\"}\nLe gradient de la log-vraisemblance en $\\beta$ est donné par $\\nabla \\mathcal L(Y,\\beta)=X'(Y-P_\\beta)$. Sa $j$ème composante vaut $$\\frac{\\partial\\mathcal L}{\\partial\\beta_j}(\\beta)=\\sum_{i=1}^nx_{ij}(y_i-p_\\beta(x_i)).$$\n\nOn peut donc calculer la drivée par rapport à $\\beta_\\ell$ : \\begin{align*}\n\\frac{\\partial\\mathcal L}{\\partial\\beta_j\\partial\\beta_\\ell}(\\beta)= & \\frac{\\partial}{\\partial\\beta_\\ell}\\left[\n\\sum_{i=1}^nx_{ij}\\left(y_i-\\frac{\\exp(x_i'\\beta)}{1+\\exp(x_i'\\beta)}\\right)\\right] \\\\\n=& -\\sum_{i=1}^nx_{ij}x_{i\\ell}\\frac{\\exp(x_i'\\beta)}{[1+\\exp(x_i'\\beta)]^2} \\\\\n=& -\\sum_{i=1}^nx_{ij}x_{i\\ell}p_\\beta(x_i)(1-p_\\beta(x_i)).\n\\end{align*} Matriciellement on déduit donc que la hessienne vaut $$\\nabla^2\\mathcal L(Y,\\beta)=-X'W_\\beta X,$$ où $W_\\beta$ est la matrice $n\\times n$ diagonale dont le $i$ème terme de la diagonale vaut $p_\\beta(x_i)(1-p_\\beta(x_i))$. Par ailleurs, comme pour tout $i=1,\\dots,n$, on a $p_\\beta(x_i)(1-p_\\beta(x_i))>0$ et que $X$ est de plein rang, on déduit que $X'W_\\beta X$ est définie positive et par conséquent que la hessienne est définie négative.\n:::\n\n::: {#exr-12-5 name=\"Modèles avec R\"}\nOn importe les données :\n\n::: {#5a4447af .cell execution_count=11}\n``` {.python .cell-code}\npanne = pd.read_csv(\"../donnees/panne.txt\", sep=\" \")\nprint(panne.head())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   etat  age marque\n0     0    4      A\n1     0    2      C\n2     0    3      C\n3     0    9      B\n4     0    7      B\n```\n:::\n:::\n\n\n1.  La commande\n\n\n    ::: {#e588e1e8 .cell execution_count=12}\n    ``` {.python .cell-code}\n    model = smf.logit('etat ~ age+marque', data=panne).fit()\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    Optimization terminated successfully.\n             Current function value: 0.659124\n             Iterations 5\n    ```\n    :::\n    :::\n    \n    \n    ajuste le modèle $$\\log\\left(\\frac{p_\\beta(x)}{1-p_\\beta(x)}\\right)=\\beta_0+\\beta_1x_1+\\beta_2\\mathsf{1}_{x_2=B}+\\beta_3\\mathsf{1}_{x_2=C}$$ où $x_1$ et $x_2$ désigne respectivement les variables **age** et **marque**. On obtient les estimateurs avec\n\n\n    ::: {#d4625094 .cell execution_count=13}\n    ``` {.python .cell-code}\n    print(model.summary())\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n                               Logit Regression Results                           \n    ==============================================================================\n    Dep. Variable:                   etat   No. Observations:                   33\n    Model:                          Logit   Df Residuals:                       29\n    Method:                           MLE   Df Model:                            3\n    Date:                Tue, 04 Feb 2025   Pseudo R-squ.:                 0.04845\n    Time:                        12:22:41   Log-Likelihood:                -21.751\n    converged:                       True   LL-Null:                       -22.859\n    Covariance Type:            nonrobust   LLR p-value:                    0.5290\n    ===============================================================================\n                      coef    std err          z      P>|z|      [0.025      0.975]\n    -------------------------------------------------------------------------------\n    Intercept       0.4781      0.833      0.574      0.566      -1.155       2.111\n    marque[T.B]    -0.4194      0.814     -0.515      0.607      -2.015       1.177\n    marque[T.C]    -1.4561      1.054     -1.382      0.167      -3.521       0.609\n    age             0.0139      0.094      0.148      0.883      -0.170       0.198\n    ===============================================================================\n    ```\n    :::\n    :::\n    \n    \n2.  Il s'agit des tests de Wald pour tester l'effet des variables dans le modèle. Pour l'effet de marque, on va par exemple tester $$H_0:\\beta_2=\\beta_3=0\\quad\\text{contre}\\quad H_1:\\beta_2\\neq 0\\text{ ou }\\beta_3\\neq 0.$$ Sous $H_0$ la statistique de Wald suit une loi du $\\chi^2$ à 4-2=2 degrés de liberté. Pour le test de la variable **age** le nombre de degrés de liberté manquant est 1. On retrouve cela dans la sortie\n\n\n    ::: {#9bdac9a6 .cell execution_count=14}\n    ``` {.python .cell-code}\n    wald_tests = model.wald_test_terms()\n    print(\"Tests de Wald pour chaque coefficient:\")\n    print(wald_tests)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    Tests de Wald pour chaque coefficient:\n                                  chi2              P>chi2  df constraint\n    Intercept   [[0.3293833811483461]]  0.5660224013616977              1\n    marque      [[1.9306493813691281]]  0.3808595181236274              2\n    age        [[0.02182485519152971]]  0.8825539788968159              1\n    ```\n    :::\n    :::\n    \n    \n<!--\n3.  Il s'agit cette fois du test du rapport de vraisemblance. Les degrés de liberté manquants sont identiques.\n\n\n    ::: {#224d016f .cell execution_count=15}\n    ``` {.python .cell-code}\n    def lr_test(full_model, reduced_model):\n        lr_stat = 2 * (full_model.llf - reduced_model.llf)\n        p_value = chi2.sf(lr_stat, df=full_model.df_model - reduced_model.df_model)\n        return lr_stat, p_value\n    \n    # Liste des variables explicatives\n    variables = panne.columns.drop('etat')\n    \n    # Effectuer les tests de rapport de vraisemblance pour chaque variable\n    lr_results = {}\n    for var in variables:\n        formula_reduced = 'etat ~ ' + ' + '.join(variables.drop(var))\n        reduced_model = smf.logit(formula_reduced, data=panne).fit(disp=0)\n        lr_stat, p_value = lr_test(model, reduced_model)\n        lr_results[var] = {'LR stat': lr_stat, 'p-value': p_value}\n    \n    # Afficher les résultats des tests de rapport de vraisemblance\n    print(\"Tests de rapport de vraisemblance pour chaque variable:\")\n    for var, result in lr_results.items():\n        print(f\"{var}: LR stat = {result['LR stat']:.4f}, p-value = {result['p-value']:.4f}\")\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    Tests de rapport de vraisemblance pour chaque variable:\n    age: LR stat = 0.0219, p-value = 0.8824\n    marque: LR stat = 2.0956, p-value = 0.3507\n    ```\n    :::\n    :::\n    \n    \n-->\n\n3.  \n\n    a)  Le modèle s'écrit $$\\log\\left(\\frac{p_\\beta(x)}{1-p_\\beta(x)}\\right)=\\beta_0+\\beta_1\\mathsf{1}_{x_2=A}+\\beta_2\\mathsf{1}_{x_2=B}.$$\n\n    b)  Le modèle ajusté ici est \n        $$\n        \\log\\left(\\frac{p_\\beta(x)}{1-p_\\beta(x)}\\right)=\\gamma_0+\\gamma_1\\mathsf{1}_{x_2=B}+\\gamma_2\\mathsf{1}_{x_2=C}.\n        $$ \n        Par identification on a \n        $$\n        \\begin{cases}\n            \\beta_0+\\beta_1=\\gamma_0 \\\\\n            \\beta_0+\\beta_2=\\gamma_0+\\gamma_1 \\\\\n            \\beta_0=\\gamma_0+\\gamma_2 \\\\\n            \\end{cases}\n            \\Longleftrightarrow\n            \\begin{cases}\n            \\beta_0=\\gamma_0+\\gamma_2 \\\\\n            \\beta_1=-\\gamma_2 \\\\\n            \\beta_2=\\gamma_1-\\gamma_2 \\\\\n            \\end{cases}\n            \\Longrightarrow\n            \\begin{cases}\n            \\widehat\\beta_0=-0.92 \\\\\n            \\widehat\\beta_1=1.48 \\\\\n            \\widehat\\beta_2=1.05 \\\\\n            \\end{cases}\n        $$\n        On peut retrouver ces résultats avec\n\n\n        ::: {#c1ee341c .cell execution_count=16}\n        ``` {.python .cell-code}\n        model1 = smf.logit('etat ~ C(marque, Treatment(reference=\"C\"))', data=panne).fit()\n        model1.summary()\n        ```\n        \n        ::: {.cell-output .cell-output-stdout}\n        ```\n        Optimization terminated successfully.\n                 Current function value: 0.659456\n                 Iterations 5\n        ```\n        :::\n        \n        ::: {.cell-output .cell-output-display execution_count=16}\n        ```{=html}\n        <table class=\"simpletable\">\n        <caption>Logit Regression Results</caption>\n        <tr>\n          <th>Dep. Variable:</th>         <td>etat</td>       <th>  No. Observations:  </th>  <td>    33</td> \n        </tr>\n        <tr>\n          <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>    30</td> \n        </tr>\n        <tr>\n          <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     2</td> \n        </tr>\n        <tr>\n          <th>Date:</th>            <td>Tue, 04 Feb 2025</td> <th>  Pseudo R-squ.:     </th>  <td>0.04798</td>\n        </tr>\n        <tr>\n          <th>Time:</th>                <td>12:22:41</td>     <th>  Log-Likelihood:    </th> <td> -21.762</td>\n        </tr>\n        <tr>\n          <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -22.859</td>\n        </tr>\n        <tr>\n          <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>  <td>0.3340</td> \n        </tr>\n        </table>\n        <table class=\"simpletable\">\n        <tr>\n                              <td></td>                        <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n        </tr>\n        <tr>\n          <th>Intercept</th>                                <td>   -0.9163</td> <td>    0.837</td> <td>   -1.095</td> <td> 0.273</td> <td>   -2.556</td> <td>    0.724</td>\n        </tr>\n        <tr>\n          <th>C(marque, Treatment(reference=\"C\"))[T.A]</th> <td>    1.4759</td> <td>    1.045</td> <td>    1.412</td> <td> 0.158</td> <td>   -0.573</td> <td>    3.525</td>\n        </tr>\n        <tr>\n          <th>C(marque, Treatment(reference=\"C\"))[T.B]</th> <td>    1.0498</td> <td>    0.984</td> <td>    1.067</td> <td> 0.286</td> <td>   -0.878</td> <td>    2.978</td>\n        </tr>\n        </table>\n        ```\n        :::\n        :::\n        \n        \n4.  Il y a interaction si l'age agit différemment sur la panne en fonction de la marque.\n\n5.  Le modèle ajusté sur est \n$$\n\\log\\left(\\frac{p_\\beta(x)}{1-p_\\beta(x)}\\right)=\\delta_0+\\delta_1\\mathsf{1}_{x_2=B}+\\delta_2\\mathsf{1}_{x_2=C}+\\delta_3x_1+\\delta_4x_1\\mathsf{1}_{x_2=B}+\\delta_5x_1\\mathsf{1}_{x_2=C}.\n$$ \n    \n    On obtient ainsi par identification : \n$$\n\\begin{cases}\n\\alpha_0=\\delta_0\\\\\n\\alpha_1=\\delta_3\\\\\n\\beta_0=\\delta_0+\\delta_1\\\\\n\\beta_1=\\delta_3+\\delta_4\\\\\n\\gamma_0=\\delta_0+\\delta_2\\\\\n\\gamma_1=\\delta_3+\\delta_5\n\\end{cases}\n$$ \n\n    On déduit les valeurs des estimateurs que l'on peut retrouver avec la commande :\n\n\n    ::: {#afd6301e .cell execution_count=17}\n    ``` {.python .cell-code}\n    model2 = smf.logit('etat ~ -1 + marque + marque:age', data=panne).fit()\n    print(model2.summary())\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    Optimization terminated successfully.\n             Current function value: 0.645772\n             Iterations 5\n                               Logit Regression Results                           \n    ==============================================================================\n    Dep. Variable:                   etat   No. Observations:                   33\n    Model:                          Logit   Df Residuals:                       27\n    Method:                           MLE   Df Model:                            5\n    Date:                Tue, 04 Feb 2025   Pseudo R-squ.:                 0.06773\n    Time:                        12:22:41   Log-Likelihood:                -21.310\n    converged:                       True   LL-Null:                       -22.859\n    Covariance Type:            nonrobust   LLR p-value:                    0.6851\n    =================================================================================\n                        coef    std err          z      P>|z|      [0.025      0.975]\n    ---------------------------------------------------------------------------------\n    marque[A]         0.2351      1.047      0.225      0.822      -1.817       2.288\n    marque[B]         0.4337      0.889      0.488      0.626      -1.308       2.176\n    marque[C]        -2.1963      2.008     -1.094      0.274      -6.131       1.739\n    marque[A]:age     0.0564      0.150      0.377      0.706      -0.237       0.350\n    marque[B]:age    -0.0555      0.133     -0.416      0.677      -0.317       0.206\n    marque[C]:age     0.2723      0.367      0.742      0.458      -0.447       0.992\n    =================================================================================\n    ```\n    :::\n    :::\n    \n    \n:::\n\n::: {#exr-12-6 name=\"Interprétation\"}\n\n::: {#18d653b5 .cell execution_count=18}\n``` {.python .cell-code}\ndf = pd.read_csv(\"../donnees/logit_exo6.csv\")\n\n# Ajuster le modèle de régression logistique avec toutes les variables explicatives\nmod = smf.logit('Y ~ X1+X2', data=df).fit()\n\n# Ajuster le modèle de régression logistique avec seulement la variable X1\nmod1 = smf.logit('Y ~ X1', data=df).fit()\n\n# Afficher le résumé des modèles\nprint(mod.summary())\nprint(mod1.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOptimization terminated successfully.\n         Current function value: 0.618257\n         Iterations 6\nOptimization terminated successfully.\n         Current function value: 0.692991\n         Iterations 3\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                      Y   No. Observations:                  500\nModel:                          Logit   Df Residuals:                      497\nMethod:                           MLE   Df Model:                            2\nDate:                Tue, 04 Feb 2025   Pseudo R-squ.:                  0.1080\nTime:                        12:22:41   Log-Likelihood:                -309.13\nconverged:                       True   LL-Null:                       -346.57\nCovariance Type:            nonrobust   LLR p-value:                 5.469e-17\n==============================================================================\n                 coef    std err          z      P>|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -0.5787      0.119     -4.852      0.000      -0.812      -0.345\nX1            -0.1947      0.066     -2.970      0.003      -0.323      -0.066\nX2             0.3190      0.044      7.244      0.000       0.233       0.405\n==============================================================================\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                      Y   No. Observations:                  500\nModel:                          Logit   Df Residuals:                      498\nMethod:                           MLE   Df Model:                            1\nDate:                Tue, 04 Feb 2025   Pseudo R-squ.:               0.0002259\nTime:                        12:22:41   Log-Likelihood:                -346.50\nconverged:                       True   LL-Null:                       -346.57\nCovariance Type:            nonrobust   LLR p-value:                    0.6923\n==============================================================================\n                 coef    std err          z      P>|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.0011      0.089      0.012      0.990      -0.174       0.177\nX1            -0.0205      0.052     -0.396      0.692      -0.122       0.081\n==============================================================================\n```\n:::\n:::\n\n\nOn remarque que la nullité du paramètre associé à **X1** est accepté dans le modèle avec uniquement **X1** alors qu'elle est refusée lorsqu'on considère **X1** et **X2** dans le modèle.\n\n:::\n\n::: {#exr-12-7 name=\"Tests à la main\"}\n\n1.  Le modèle s'écrit\n$$log\\left(\\frac{p_\\beta(x)}{1-p_\\beta(x)}\\right)=\\beta_0+\\beta_1x.$$\n\n2.  La log vraisemblance s'obtient avec\n\n\n    ::: {#d6ef065e .cell execution_count=19}\n    ``` {.python .cell-code}\n    p = np.array([0.76, 0.4, 0.6, 0.89, 0.35])\n    Y = np.array([1, 0, 0, 1, 1])\n    L1 = np.log(np.prod(p**Y * (1-p)**(1-Y)))\n    print(L1)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    -2.867909142096535\n    ```\n    :::\n    :::\n    \n    \n3. \n  \n    a)  On calcule les écart-type des estimateurs\n\n\n        ::: {#1ebb7fb1 .cell execution_count=20}\n        ``` {.python .cell-code}\n        p = np.array([0.76, 0.4, 0.6, 0.89, 0.35])\n        X1 = np.array([0.47, -0.55, -0.01, 1.07, -0.71])\n        X = np.column_stack((np.ones(5), X1))\n        W = np.diag(p * (1 - p))\n        SIG = np.linalg.inv(X.T @ W @ X)\n        sig = np.sqrt(np.diag(SIG))\n        print(sig)\n        ```\n        \n        ::: {.cell-output .cell-output-stdout}\n        ```\n        [1.0232524  1.74493535]\n        ```\n        :::\n        :::\n        \n        \n        On en déduit les statistiques de test :\n\n\n        ::: {#c8a8b1ca .cell execution_count=21}\n        ``` {.python .cell-code}\n        beta = np.array([0.4383, 1.5063])\n        result = beta / sig\n        print(result)\n        ```\n        \n        ::: {.cell-output .cell-output-stdout}\n        ```\n        [0.42834006 0.86324115]\n        ```\n        :::\n        :::\n        \n        \n    b)  On peut faire le test de Wald et du rapport de vraisemblance. \n  \n    c)  La statistique de test vaut 0.8632411, on obtient donc la probabilité critique\n\n\n        ::: {#c2a48e80 .cell execution_count=22}\n        ``` {.python .cell-code}\n        2 * (1 - norm.cdf(0.8632411))\n        ```\n        \n        ::: {.cell-output .cell-output-display execution_count=22}\n        ```\n        0.3880049206652636\n        ```\n        :::\n        :::\n        \n        \n        On peut également effectuer un test du rapport de vraisemblance. Le modèle null sans X1 a pour log-vraisemblance\n\n\n        ::: {#82054446 .cell execution_count=23}\n        ``` {.python .cell-code}\n        p0 = 3 / 5\n        Y = np.array([1, 0, 0, 1, 1])\n        L0 = np.log(np.prod(p0**Y * (1-p0)**(1-Y)))\n        print(L0)\n        ```\n        \n        ::: {.cell-output .cell-output-stdout}\n        ```\n        -3.365058335046282\n        ```\n        :::\n        :::\n        \n        \n        La statistique de test vaut donc\n\n\n        ::: {#1d468d24 .cell execution_count=24}\n        ``` {.python .cell-code}\n        2*(L1-L0)\n        ```\n        \n        ::: {.cell-output .cell-output-display execution_count=24}\n        ```\n        0.9942983858994943\n        ```\n        :::\n        :::\n        \n        \n        et la probabilité critique est égale à\n\n\n        ::: {#e2f90c18 .cell execution_count=25}\n        ``` {.python .cell-code}\n        1 - chi2.cdf(2*(L1-L0), df=1)\n        ```\n        \n        ::: {.cell-output .cell-output-display execution_count=25}\n        ```\n        0.31869407584847365\n        ```\n        :::\n        :::\n        \n        \n        On peut retrouver (aux arrondis près) les résultats de l’exercice avec\n\n\n        ::: {#ca21d325 .cell execution_count=26}\n        ``` {.python .cell-code}\n        X = [0.47, -0.55, -0.01, 1.07, -0.71]\n        Y = [1, 0, 0, 1, 1]\n        df = pd.DataFrame({'X': X, 'Y': Y})\n        model = smf.logit('Y ~ X', data=df).fit()\n        log_likelihood = model.llf\n        print(log_likelihood)\n        ```\n        \n        ::: {.cell-output .cell-output-stdout}\n        ```\n        Optimization terminated successfully.\n                 Current function value: 0.579753\n                 Iterations 6\n        -2.898765148750008\n        ```\n        :::\n        :::\n        \n        \n\n        ::: {#5168505b .cell execution_count=27}\n        ``` {.python .cell-code}\n        wald_test = model.wald_test_terms()\n        print(wald_test)\n        ```\n        \n        ::: {.cell-output .cell-output-stdout}\n        ```\n                                     chi2               P>chi2  df constraint\n        Intercept  [[0.1845505636560877]]   0.6674913759958387              1\n        X           [[0.755071501627431]]  0.38487529817766963              1\n        ```\n        :::\n        :::\n        \n        \n\n        ::: {#f3d9f796 .cell execution_count=28}\n        ``` {.python .cell-code}\n        lr_test_model = model.llr\n        print(f\"LR stat: {lr_test_model:.4f}, p-value: {model.llr_pvalue:.4f}\")\n        ```\n        \n        ::: {.cell-output .cell-output-stdout}\n        ```\n        LR stat: 0.9326, p-value: 0.3342\n        ```\n        :::\n        :::\n        \n        \n:::\n\n\n::: {#exr-12-8 name=\"Vraisemblance du modèle saturé\"}\n1.  Les variables $(y_t,t=1,\\dots,y_T)$ étant indépendantes et de loi binomiales $B(n_t,p_t)$, la log-vraisemblance est donnée par\n\\begin{align*}\n\\mathcal L_{\\text{sat}}(Y,p)= & \\log\\left(\\prod_{t=1}^T\n\\begin{pmatrix}\nn_t\\\\\n\\tilde y_t\n\\end{pmatrix}\np_t^{\\tilde y_t}(1-p_t)^{n_t-\\tilde y_t}\\right) \\\\\n= &\n\\sum_{t=1}^T\\left(\\log\n\\begin{pmatrix}\nn_t\\\\\n\\tilde y_t\n\\end{pmatrix}\n+\\tilde y_t\\log(p_t)+(n_t-\\tilde y_t)\\log(1-p_t)\\right)\n\\end{align*}\n\n2.  La dérivée de la log-vraisemblance par rapport à $p_t$ s'écrit\n$$\\frac{\\tilde y_t}{p_t}-\\frac{n_t-\\tilde y_t}{1-p_t}.$$\nCette dérivée s'annule pour\n$$\\widehat p_t=\\frac{\\tilde y_t}{n_t}.$$\n\n3.  On note $\\widehat \\beta$ l'EMV du modèle logistique et $p_{\\widehat\\beta}$ le vecteur qui contient les valeurs ajustées $p_{\\widehat\\beta}(x_t),t=1,\\dots,T$. On a pour tout $\\beta\\in\\mathbb R^p$ :\n$$\\mathcal L(Y,\\beta)\\leq\\mathcal L(Y,\\widehat\\beta)=\\mathcal L_{\\text{sat}}(Y,p_{\\widehat\\beta})\\leq L_{\\text{sat}}(Y,\\widehat p_t).$$\n\n:::\n\n\n::: {#exr-12-9 name=\"Résidus partiels\"}\n\n1.  \n\n\n    ::: {#dc2125f8 .cell execution_count=29}\n    ``` {.python .cell-code}\n    artere = pd.read_csv('../donnees/artere.txt', header=0, index_col=0, sep=' ')\n    modele = smf.glm('chd~age', data=artere, family=sm.families.Binomial()).fit()\n    B0 = modele.params\n    OriginalDeviance = modele.deviance\n    ```\n    :::\n    \n    \n2.  \n\n\n    ::: {#8ff64acd .cell execution_count=30}\n    ``` {.python .cell-code}\n    alpha=0.05\n    ```\n    :::\n    \n    \n3.\n\n\n    ::: {#496f8de4 .cell execution_count=31}\n    ``` {.python .cell-code}\n    stderr = modele.cov_params().iloc[1,1]**0.5\n    from scipy import stats\n    delta = stats.chi2.ppf(1-alpha/4, df=1)**0.5*stderr/5\n    grille = B0[1] + np.arange(-10,11)*delta\n    ```\n    :::\n    \n    \n4.  On a\n  \\begin{align*}\n\\mathcal D_1&=-2(\\mathcal L(Y,\\hat\\beta)-\\mathcal L_{sat})\n  \\end{align*}\nPour celle avec l'offset $K_i=x_i\\beta_2^*$ elle vaut \n  \\begin{align*}\n\\mathcal D_o&=-2(\\mathcal L(Y,K,\\hat\\beta_1)-\\mathcal L_{sat})\n  \\end{align*}\noù $\\hat \\beta_1$ maximise $\\mathcal L(Y,K,\\hat\\beta_1)$ c'est à dire $\\mathcal L(Y,K,\\hat\\beta_1)=l(\\beta_2^*)$  et nous avons donc\n  \\begin{align*}\n\\mathcal D_o - \\mathcal D_1= 2(\\mathcal L(Y,\\hat\\beta)-\\mathcal L(Y,K,\\beta_1)= 2(\\mathcal L(Y,\\hat\\beta)-l(\\beta_2^*))=P(\\beta_2^*).\n  \\end{align*}\n\n5.  \n\n\n    ::: {#2262b8b0 .cell execution_count=32}\n    ``` {.python .cell-code}\n    profil2 = []\n    for valgrille in grille:\n        modeleo = smf.glm('chd~1', data=artere, offset=artere.age*valgrille, family=sm.families.Binomial()).fit()\n        if (modeleo.deviance -  OriginalDeviance)<0:\n            profil2.append(0)\n        else:\n            profil2.append(modeleo.deviance -  OriginalDeviance)    \n    ```\n    :::\n    \n    \n6.  \n\n\n    ::: {#724f0d63 .cell execution_count=33}\n    ``` {.python .cell-code}\n    profil = np.sign(np.arange(-10,11))*np.sqrt(profil2)\n    ```\n    :::\n    \n    \n7.\n\n\n    ::: {#79a2dfd6 .cell execution_count=34}\n    ``` {.python .cell-code}\n    f = interpolate.interp1d(profil, grille)\n    xnew = [-np.sqrt(stats.chi2.ppf(1-alpha, df=1)),\n            np.sqrt(stats.chi2.ppf(1-alpha, df=1)) ]\n    print(f(xnew))\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    [0.06697424 0.16204287]\n    ```\n    :::\n    :::\n    \n    \n8.  \n\n\n    ::: {#bd5e7cfc .cell execution_count=35}\n    ``` {.python .cell-code}\n    print(modele.conf_int().iloc[1,:])\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    0    0.063765\n    1    0.158078\n    Name: age, dtype: float64\n    ```\n    :::\n    :::\n    \n    \n:::\n\n",
    "supporting": [
      "chap12_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}