{
  "hash": "565c8541a1b4a280ec73ae8dba72dc19",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"3 Validation du modèle\"\ntoc: true\n---\n\n\n\n\n::: {.content-hidden}\n\n\\DeclareMathOperator{\\C}{Cov}\n\\DeclareMathOperator{\\V}{V}\n\\DeclareMathOperator{\\leR}{R^2}\n\\newcommand{\\M}{\\mathcal M}\n\\DeclareMathOperator{\\MCO}{MCO}\n\\DeclareMathOperator{\\SCR}{SCR}\n\\DeclareMathOperator{\\SCT}{SCT}\n\\DeclareMathOperator{\\SCE}{SCE}\n\\DeclareMathOperator{\\EQM}{EQM}\n\\newcommand{\\1}{\\mathbf{1}}\n\\newcommand{\\un}{\\mathbf{1}}\n\\newcommand{\\D}{\\displaystyle}\n\n\n\\newcommand{\\prob}{\\mathbf P}\n\\newcommand{\\argmin}{\\mathop{\\mathrm{argmin}}}\n\\newcommand{\\ind}{\\mathbf 1}\n\\newcommand{\\R}{\\mathbb R}\n\\newcommand{\\E}{\\mathbf E}\n\\newcommand{\\var}{\\mathbf V}\n\\newcommand{\\ps}[2]{\\langle #1,#2\\rangle}\n\\newcommand{\\card}[1]{|{#1}|}\n\\newcommand{\\cov}{\\mathbf{cov}}\n\\newcommand{\\corr}{\\text{corr}}\n\\newcommand{\\AUC}{\\text{AUC}}\n\\newcommand{\\logit}{\\text{logit}}\n\\newcommand{\\diag}{\\text{diag}}\n\\newcommand{\\tr}{\\text{tr}}\n\\newcommand{\\li}[2]{#1_{#2}}\n\\newcommand{\\ssli}[2]{#1_{(#2)}}\n\\newcommand{\\HH}{{\\mathcal{H}}}\n\n\n:::\n\n::: {#exr-3-1 name=\"Questions de cours\"}\nC si $\\1$ fait partie des variables ou si $\\1 \\in \\Im(X)$, A, C, C, A.\n:::\n\n::: {#exr-3-2 name=\"Propriétés d'une matrice de projection\"}\nLa trace d'un projecteur vaut la dimension de l'espace sur lequel s'effectue \nla projection, donc $\\tr(P_X)=p$. Le second point découle de la propriété \n$P^2=P$.\n\nLes matrices $P_X$ et $P_XP_X$ sont égales, nous avons que \n$(P_X)_{ii}$ vaut $(P_XP_X)_{ii}$. Cela s'écrit \n\\begin{eqnarray*}\nh_{ii} &=&  \\sum_{k=1}^n h_{ik} h_{ki}\\\\\n&=& h_{ii}^2 + \\sum_{k=1, k \\neq i}^n h_{ik}^2\\\\\nh_{ii}(1-h_{ii}) &=& \\sum_{k=1, k \\neq i}^n h_{ik}^2.\n\\end{eqnarray*}\nLa dernière quantité de droite de l'égalité est positive et \ndonc le troisième point est démontré. En nous servant de cet\nécriture les deux derniers points sont aussi démontrés.\n\nNous pouvons écrire \n\\begin{eqnarray*}\nh_{ii}(1-h_{ii}) &=& h_{ij}^2 + \\sum_{k=1, k \\neq i ,j }^n h_{ik}^2.\n\\end{eqnarray*}\nLa quantité de gauche est maximum lorsque $h_{ii}=0.5$ et vaut \nalors $0.25$. Le quatrième point est démontré.\n:::\n\n::: {#exr-3-3 name=\"Lemme d'invertion matricielle\"}\nCommençons par effectuer les calculs en notant que la quantité \n$u'M^{-1}v$ est un scalaire que nous noterons $k$. Nous avons\n\\begin{eqnarray*}\n\\left(M+uv'\\right)\\left(M^{-1}-\\frac{M^{-1}uv'M^{-1}}{1+u'M^{-1}v}\\right)\n&=&MM^{-1}-\\frac{MM^{-1}uv'M^{-1}}{1+k}+uv'M^{-1}\n-\\frac{uv'M^{-1}uv'M^{-1}}{1+k}\\\\\n&=&I+\\frac{-uv'M^{-1}+uv'M^{-1}+kuv'M^{-1}-ukv'M^{-1}}{1+k}.\n\\end{eqnarray*}\nLe résultat est démontré.\n:::\n\n::: {#exr-3-4 name=\"Résidus studentisés\"}\n\n2.  Il suffit d'utiliser la définition du produit matriciel et de la somme matricielle et d'identifier les 2 membres des égalités.\n\n3.  En utilisant maintenant l'égalité de l'exercice précédent sur les \ninverses, avec $u=-x_i$ et $v=x_i'$, nous avons \n\\begin{eqnarray*}\n(\\ssli{X'}{i}\\ssli{X}{i})^{-1}=(X'X-\\li{x}{i}\\li{x}{i}')^{-1}=(X'X)^{-1}+\n\\frac{(X'X)^{-1}\\li{x}{i}\\li{x}{i}'(X'X)^{-1}}\n{1-\\li{x}{i}'(X'X)^{-1}\\li{x}{i}}.%\\label{eq:hiieta}\n\\end{eqnarray*}\n    La définition de $h_{ii}=\\li{x}{i}'(X'X)^{-1}\\li{x}{i}$\ndonne le résultat.\n\n4.  Calculons la prévision où $\\hat \\beta_{(i)}$ est l'estimateur de \n$\\beta$ obtenu sans la $i^e$ observation\n\n\\begin{eqnarray*}\n\\hat y_i^p \n= \\li{x}{i}'\\hat \\beta_{(i)} \n&=& \\li{x}{i}' (\\ssli{X'}{i}\\ssli{X}{i})^{-1}\\ssli{X'}{i}Y_{(i)}\\\\\n&=& \\li{x}{i}'\\left[(X'X)^{-1} \n+ \\frac{(X'X)^{-1}\\li{x}{i}\\li{x}{i}'(X'X)^{-1}}{1-h_{ii}}\n\\right]\\left(X'Y-\\li{x}{i}'y_i\\right)\\\\\n&=& \\li{x}{i}' \\hat \\beta + \\frac{h_{ii}}{1-h_{ii}}\\li{x}{i}' \\hat \\beta \n- h_{ii}y_i -\\frac{h_{ii}^2}{1-h_{ii}}y_i\\\\\n&=& \\frac{1}{1-h_{ii}}\\hat y_i - \\frac{h_{ii}}{1-h_{ii}}y_i.\n\\end{eqnarray*}\n\n5.  Ce dernier résultat donne\n\\begin{eqnarray*}\n\\hat \\varepsilon_i = (1-h_{ii})(y_i-\\hat y^p_i).\n\\end{eqnarray*}\nNous avons alors\n\\begin{eqnarray*}\nt^*_i &=& \\frac{\\hat \\varepsilon_i}{\\hat \\sigma_{(i)}\\sqrt{1-h_{ii}}}\\\\\n&=&\\frac{\\sqrt{(1-h_{ii})}(y_i - \\hat y_i^p)}{\\hat \\sigma_{(i)}}.\n\\end{eqnarray*}\nPour terminer, remarquons qu'en multipliant l'égalité de la question 3 \nà gauche par $\\li{x}{i}'$ et à droite par $\\li{x}{i}$ \n\\begin{eqnarray*}\n\\li{x}{i}'(\\ssli{X'}{i}\\ssli{X}{i})^{-1}\\li{x}{i} \n&=& h_{ii}+ \\frac{h_{ii}^2}{1-h_{ii}}.\\\\\n1+\\li{x}{i}'(\\ssli{X'}{i}\\ssli{X}{i})^{-1}\\li{x}{i} \n&=& 1 +\\frac{h_{ii}}{1-h_{ii}}=\\frac{h_{ii}}{1-h_{ii}}.\n\\end{eqnarray*}\n\n6.   Utilisons l'expression\n\\begin{eqnarray*}\nt^*_i=\\frac{y_i-\\hat y_i^p }\n{\\hat \\sigma_{(i)}\\sqrt{1+\\li{x}{i}'(\\ssli{X'}{i}\\ssli{X}{i})^{-1}\\li{x}{i}}}.\n\\end{eqnarray*}\nNous pouvons alors appliquer la preuve de la proposition 5.4\npage 97, en constatant que la $i^e$ observation \nest une nouvelle observation. Nous avons donc $n-1$ observations \npour estimer les paramètres, cela donne donc un Student à $n-1-p$ \nparamètres.\n:::\n\n::: {#exr-3-5 name=\"Distance de Cook\"}\n1.  Nous reprenons une partie des calculs de l'exercice précédent :\n\\begin{eqnarray*}\n\\hat \\beta_{(i)} &=& (\\ssli{X'}{i}\\ssli{X}{i})^{-1}\\ssli{X'}{i}\\ssli{Y}{i}\\\\\n&=& (X'X)^{-1}[X'Y-\\li{x}{i}y_i]+\\frac{1}{1-h_{ii}}\n(X'X)^{-1}\\li{x}{i}\\li{x}{i}'(X'X)^{-1}[X'Y-\\li{x}{i}y_i]\\\\\n&=& \\hat \\beta - (X'X)^{-1}\\li{x}{i}y_i + \\frac{1}{1-h_{ii}}\n(X'X)^{-1}\\li{x}{i}\\li{x}{i}'\\hat \\beta - \\frac{h_{ii}}{1-h_{ii}}\n(X'X)^{-1}\\li{x}{i}y_i,\n\\end{eqnarray*}\n    d'où le résultat. \n    \n2.  Pour obtenir la seconde écriture de la distance de Cook, nous écrivons d'abord que \n\\begin{eqnarray*}\n\\hat \\beta_{(i)} - \\hat \\beta = \\frac{- \\hat \\varepsilon_i}{1-h_{ii}}\n(X'X)^{-1}\\li{x}{i}.\n\\end{eqnarray*}\n    Puis nous développons\n\\begin{eqnarray*}\nC_i &=& \\frac{1}{p \\hat \\sigma^2}(\\hat \\beta_{[i]}-\\hat \\beta)'\nX'X(\\hat \\beta_{(i)}-\\hat \\beta)\\\\\n&=& \\frac{1}{p \\hat \\sigma^2} \\left(\\frac{- \\hat \\varepsilon_i}{1-h_{ii}}\\right)^2 \\li{x}{i}' (X'X)^{-1}(X'X)(X'X)^{-1}\\li{x}{i}.\n\\end{eqnarray*}\n    Le résultat est démontré.\n:::\n\n::: {#exr-3-6 name=\"Régression partielle\"}\nNous avons le modèle suivant :\n\\begin{eqnarray*}\nP_{X_{\\bar{j}}^\\perp} Y&=&\\beta_jP_{X_{\\bar{j}}^\\perp} X_j + \\eta.\n\\end{eqnarray*}\nL'estimateur des moindres carrés $\\tilde\\beta_j$ issu de ce \nmodèle vaut\n\\begin{eqnarray*}\n\\tilde \\beta_j = (X'_j P_{X_{\\bar{j}}^\\perp} X_j)^{-1}\nX'_j P_{X_{\\bar{j}}^\\perp} Y.\n\\end{eqnarray*}\nLa projection de $Y$ sur $\\Im(X_{\\bar{j}})$ (i.e. la prévision par \nle modèle sans la variable $X_j$) peut s'écrire comme la projection $Y$ \nsur $\\Im(X)$ qui est ensuite projetée sur $\\Im(X_{\\bar{j}})$, \npuisque $\\Im(X_{\\bar{j}})\\subset \\Im(X)$. Ceci s'écrit\n\\begin{eqnarray*}\nP_{X_{\\bar{j}}}Y&=&P_{X_{\\bar{j}}}P_{X}Y=P_{X_{\\bar{j}}}X\\hat{\\beta}\n=P_{X_{\\bar{j}}}(X_{\\bar{j}}\\hat\\beta_{\\bar{j}}+\\hat\\beta_jX_j)\n=X_{\\bar{j}}\\hat\\beta_{\\bar{j}}+\\hat\\beta_jP_{X_{\\bar{j}}}X_j,\n\\end{eqnarray*}\net donc\n\\begin{eqnarray*}\nX_{\\bar{j}}\\hat\\beta_{\\bar{j}} = P_{X_{\\bar{j}}} Y - \n\\hat\\beta_jP_{X_{\\bar{j}}}X_j.\n\\end{eqnarray*}\nRécrivons les résidus \n\\begin{eqnarray*}\n\\hat{\\varepsilon}&=&P_{X^\\perp} Y=Y-X\\hat\\beta\n=Y-X_{\\bar{j}}\\hat\\beta_{\\bar{j}}-\\hat\\beta_jX_j\\nonumber\\\\\n&=&Y-P_{X_{\\bar{j}}}Y + \\hat\\beta_jP_{X_{\\bar{j}}}X_j  -\\hat\\beta_j X_j\\nonumber\\\\\n&=&(I-P_{X_{\\bar{j}}})Y - \\hat\\beta_j(I-P_{X_{\\bar{j}}})X_j\\nonumber\\\\\n&=&P_{X_{\\bar{j}}^\\perp} Y-\\hat\\beta_jP_{X_{\\bar{j}}^\\perp} X_j.%\\label{eq:origine:residpartiel}\n\\end{eqnarray*}\nEn réordonnant cette dernière égalité, nous pouvons écrire\n\\begin{eqnarray}\nP_{X_{\\bar{j}}^\\perp} Y&=&\\hat\\beta_jP_{X_{\\bar{j}}^\\perp} X_j+\\hat{\\varepsilon}.\\nonumber\n\\end{eqnarray}\nNous avons alors\n\\begin{eqnarray*}\n\\tilde\\beta_j &=& (X'_j P_{X_{\\bar{j}}^\\perp} X_j)^{-1}\nX'_j P_{X_{\\bar{j}}^\\perp} Y\\\\\n&=& (X'_j P_{X_{\\bar{j}}^\\perp} X_j)^{-1}\nX'_j(\\hat\\beta_jP_{X_{\\bar{j}}^\\perp} X_j+\\hat{\\varepsilon})\\\\\n&=& \\hat\\beta_j +(X'_j P_{X_{\\bar{j}}^\\perp} X_j)^{-1} X'_j\\hat{\\varepsilon}).\n\\end{eqnarray*}\nLe produit scalaire $X'_j\\hat{\\varepsilon} = \\langle X_j,\\hat{\\varepsilon} \\rangle$ \nest nul car les deux vecteurs appartiennent à des sous-espaces orthogonaux, d'où le \nrésultat.\n\n:::\n\n::: {#exr-3-7 name=\"TP : Résidus partiels\"}\n\n::: {#f6613090 .cell execution_count=2}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom patsy import dmatrices\n```\n:::\n\n\n1.  Importation\n\n\n    ::: {#238d123a .cell execution_count=3}\n    ``` {.python .cell-code}\n    don = pd.read_csv(\"../donnees/tprespartiel.dta\", sep=\";\")\n    ```\n    :::\n    \n    \n2.  Estimation du modèle\n\n\n    ::: {#e7672970 .cell execution_count=4}\n    ``` {.python .cell-code}\n    X = don[['X1', 'X2', 'X3', 'X4']]\n    X = sm.add_constant(X)\n    Y = don['Y']\n    mod = sm.OLS(Y, X).fit()\n    ```\n    :::\n    \n    \n3.  Analyse des résidus partiels. Commençons par visualiser les résidus partiels :\n\n\n    ::: {#ecf47f0d .cell execution_count=5}\n    ``` {.python .cell-code}\n    fig = sm.graphics.plot_ccpr_grid(mod)\n    plt.show()\n    ```\n    \n    ::: {.cell-output .cell-output-display}\n    ![](chap3_files/figure-html/cell-5-output-1.png){width=657 height=348}\n    :::\n    :::\n    \n    \n    Les 3 premières variables montrent des tendances linéaires (ou aucune pour la troisième) alors que la troisième semble montrer plutôt une tendance quadratique.\n\n4.  Refaisons le modèle avec `X5` :\n\n\n    ::: {#efdb4160 .cell execution_count=6}\n    ``` {.python .cell-code}\n    don['X5'] = don['X4'] ** 2\n    X2 = don[['X1', 'X2', 'X3', 'X5']]\n    X2 = sm.add_constant(X2)\n    mod2 = sm.OLS(Y, X2).fit()\n    fig2 = sm.graphics.plot_ccpr_grid(mod2)\n    plt.show()\n    ```\n    \n    ::: {.cell-output .cell-output-display}\n    ![](chap3_files/figure-html/cell-6-output-1.png){width=662 height=349}\n    :::\n    :::\n    \n    \n    et nous constatons que les résidus partiels sont tous à tendance linéaire. Les 2 modèles ayant le même nombre de variables nous pouvons les comparer via leur $\\leR$ qui valent\n\n\n    ::: {#94eb04e2 .cell execution_count=7}\n    ``` {.python .cell-code}\n    print(mod.rsquared)\n    print(mod2.rsquared)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    0.9860422309885765\n    0.9966109930897685\n    ```\n    :::\n    :::\n    \n    \n    La seconde modélisation est la meilleure tant pour la qualité globale que pour l'analyse des résidus.\n\n\n5.  Avec le second jeu de données\n\n\n    ::: {#6f373172 .cell execution_count=8}\n    ``` {.python .cell-code}\n    donbis = pd.read_csv(\"../donnees/tpbisrespartiel.dta\", sep=\";\")\n    X = donbis[['X1', 'X2', 'X3', 'X4']]\n    X = sm.add_constant(X)\n    Y = donbis['Y']\n    mod3 = sm.OLS(Y, X).fit()\n    fig = sm.graphics.plot_ccpr_grid(mod3)\n    plt.show()\n    ```\n    \n    ::: {.cell-output .cell-output-display}\n    ![](chap3_files/figure-html/cell-8-output-1.png){width=657 height=347}\n    :::\n    :::\n    \n    \n    Nous voyons clairement une sinusoïde de type $\\sin(-2\\pi X_4)$ sur le dernier graphique. Changeons `X4`\n\n\n    ::: {#44acbe76 .cell execution_count=9}\n    ``` {.python .cell-code}\n    donbis['X5'] = np.sin(-2*np.pi*donbis['X4'])\n    X2 = donbis[['X1', 'X2', 'X3', 'X5']]\n    X2 = sm.add_constant(X2)\n    mod4 = sm.OLS(Y, X2).fit()\n    fig2 = sm.graphics.plot_ccpr_grid(mod4)\n    plt.show()\n    ```\n    \n    ::: {.cell-output .cell-output-display}\n    ![](chap3_files/figure-html/cell-9-output-1.png){width=656 height=348}\n    :::\n    :::\n    \n    \n\n    ::: {#6f01d16b .cell execution_count=10}\n    ``` {.python .cell-code}\n    print(mod3.rsquared)\n    print(mod4.rsquared)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    0.8106256841560806\n    0.9984664330652744\n    ```\n    :::\n    :::\n    \n    \n:::\n\n",
    "supporting": [
      "chap3_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}