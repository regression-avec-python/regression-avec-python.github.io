{
  "hash": "c02dab9c147410fbcd825ad1407fb50d",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Régularisation des moindres carrés : ridge, lasso, elastic-net\"\ntoc: true\n---\n\n\n\n\n::: {.content-hidden}\n\n\\DeclareMathOperator{\\C}{Cov}\n\\DeclareMathOperator{\\V}{V}\n\\DeclareMathOperator{\\leR}{R^2}\n\\newcommand{\\M}{\\mathcal M}\n\\DeclareMathOperator{\\MCO}{MCO}\n\\DeclareMathOperator{\\SCR}{SCR}\n\\DeclareMathOperator{\\SCT}{SCT}\n\\DeclareMathOperator{\\SCE}{SCE}\n\\DeclareMathOperator{\\EQM}{EQM}\n\\newcommand{\\1}{\\mathbf{1}}\n\\newcommand{\\un}{\\mathbf{1}}\n\\newcommand{\\D}{\\displaystyle}\n\n\n\\newcommand{\\prob}{\\mathbf P}\n\\newcommand{\\argmin}{\\mathop{\\mathrm{argmin}}}\n\\newcommand{\\ind}{\\mathbf 1}\n\\newcommand{\\R}{\\mathbb R}\n\\newcommand{\\E}{\\mathbf E}\n\\newcommand{\\var}{\\mathbf V}\n\\newcommand{\\ps}[2]{\\langle #1,#2\\rangle}\n\\newcommand{\\card}[1]{|{#1}|}\n\\newcommand{\\cov}{\\mathbf{cov}}\n\\newcommand{\\corr}{\\text{corr}}\n\\newcommand{\\AUC}{\\text{AUC}}\n\\newcommand{\\logit}{\\text{logit}}\n\\newcommand{\\diag}{\\text{diag}}\n\\newcommand{\\tr}{\\text{tr}}\n\\newcommand{\\li}[2]{#1_{#2}}\n\\newcommand{\\ssli}[2]{#1_{(#2)}}\n\n\n:::\n\n::: {#b1bab4a9 .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport statsmodels.api as sm\nfrom sklearn.linear_model import Lasso, LassoCV\nfrom sklearn.linear_model import Lasso, lasso_path\n```\n:::\n\n\n::: {#exr-9-1 name=\"Questions de cours\"}\nA, B, B, B, A (pour un bon choix de $\\lambda$) et B, A, C et D.\n:::\n\n::: {#exr-9-2 name=\"Projection et régression ridge\"}\n\n:::\n\n::: {#exr-9-3 name=\"Variance des valeurs ajustées avec une régression ridge\"}\n\n:::\n\n::: {#exr-9-4 name=\"Nombre effectif de paramètres de la régression ridge\"}\n1.  Rappelons que pour une valeur $\\kappa$ donnée, le vecteur de coefficients de la régression ridge s'écrit\n$$\n\\hat \\beta_{\\mathrm{ridge}}(\\kappa) = (X'X + \\kappa I)^{-1}X'Y.\n$$\net donc l'ajustement par la régression ridge est\n$$\n\\hat Y_{\\mathrm{ridge}}(\\kappa)=X(X'X + \\kappa I)^{-1}X'Y=H^*(\\kappa)Y\n$$\n\n2.  Soit $U_i$ le vecteur propre de $A$ associé à la valeur propre $d^2_i$. Nous avons donc par définition que\n$$\n\\begin{eqnarray*}\nAU_i&=&d^2_iU_i\\\\\nAU_i+\\lambda U_i&=&d^2_iU_i+\\lambda U_i=(d^2_i+\\lambda) U_i\\\\\n(A+\\lambda I_p)U_i&=&(d^2_i+\\lambda) U_i,\n\\end{eqnarray*}\n$$\nc'est-à-dire que $U_i$ est aussi vecteur propre de $A+\\lambda I_p$ associé à la valeur propre $\\lambda+d^2_i$.\n\n3.  Nous savons que $X=QD P'$ avec $Q$ et $P$ matrices orthogonales et $D=\\diag(d_1,\\dotsc,d_p)$. Puisque $Q$ est orthogonale, nous avons, par définition, $Q'Q=I$. Nous avons donc que $X'X=(QD P')'QD P'=PDQ'QDP'=PD^2P'$. Puisque $P$ est orthogonale $P'P=I_p$ et $P^{-1}=P$.\n$$\n\\begin{eqnarray*}\n\\tr(X(X'X+\\lambda I_p)^{-1}X')&=&\\tr((X'X+\\lambda I_p)^{-1}X'X)\\\\\n&=&\\tr((PD^2P'+\\lambda PP')^{-1}PD^2P')\\\\\n&=&\\tr((P(D+\\lambda I_p )P')^{-1}PD^2P').\n\\end{eqnarray*}\n$$\nAinsi\n$$\n\\begin{eqnarray*}\n\\tr(X(X'X+\\lambda I_p)^{-1}X')&=&\\tr( (P')^{-1}(D+\\lambda I_p )^{-1} P^{-1} PD^2P')\\\\\n&=&\\tr( (P')^{-1}(D+\\lambda I_p )^{-1} D^2P')\\\\\n&=&\\tr( (D+\\lambda I_p )^{-1} D^2).\n\\end{eqnarray*}\n$$\nSelon la définition de $H^*(\\kappa)$, nous savons que sa trace vaut donc\n$$\n\\begin{eqnarray*}\n\\tr( (D+\\kappa I_p )^{-1} D^2).\n\\end{eqnarray*}\n$$\nComme $D$ et $I_p$ sont des matrices diagonales, leur somme et produit sont simplement leur somme et produit terme à terme des éléments de la\ndiagonale, et donc cette trace (somme des éléments de la diagonale) vaut\n$$\n\\sum_{i=1}^{p}{\\frac{d_j^2}{d_j^2+\\kappa}}.\n$$\n:::\n\n::: {#exr-9-5 name=\"Estimateurs à rétrecissement - shrinkage\"}\n1.  Soit le modèle de régression\n$$\nY=X\\beta+\\varepsilon.\n$$\nEn le pré-multipliant par $P$, nous avons\n$$\nZ=PY=PX\\beta+P\\varepsilon=DQ\\beta+\\eta=D\\gamma+\\eta.\n$$\nPuisque $\\varepsilon\\sim\\mathcal{N}(0,\\sigma^2 I_n)$ et $P$ fixé, nous avons que $\\eta=P\\varepsilon$ suit une loi normale de moyenne $\\E(\\eta)=P\\E(\\varepsilon)=0$ et de variance $\\V(\\eta)=P\\V(\\varepsilon)P'=\\sigma^2PP'=\\sigma^2I_n$.\n    \n    Par définition, $Z$ vaut $PY$ et nous savons que $Y\\sim\\mathcal{N}(X\\beta,\\sigma^2 I_n)$, donc $Z\\sim\\mathcal{N}(PX\\beta,\\sigma^2 PP')$, c'est-à-dire $Z\\sim\\mathcal{N}(DQ\\beta,\\sigma^2 I_n)$ ou encore $Z\\sim\\mathcal{N}(D\\gamma,\\sigma^2 I_n)$. En utilisant la valeur de $D$ nous avons\n$$    \n\\begin{eqnarray*}\nD\\gamma&=&\n\\begin{pmatrix}\n  \\Delta \\gamma\\\\\n0\n\\end{pmatrix}.\n\\end{eqnarray*}\n$$\nDonc $Z_{1:p}\\sim\\mathcal{N}(\\Delta\\gamma,\\sigma^2I_p)$.\n\n2.  Soit un estimateur de $\\beta$ linéaire en $Y$~: $\\hat \\beta=AY$. Soit l'estimateur de $\\gamma=Q\\beta$ linéaire en $Y$~: $\\hat\\gamma=Q AY$. Pour calculer leur matrice de l'EQM, nous devons calculer leur biais et leur variance. Le biais de $\\hat \\beta$ est\n$$\nB(\\hat \\beta)=\\E(\\hat \\beta)-\\beta=\\E(AY)-\\beta=A\\E(Y)-\\beta=AX\\beta-\\beta.\n$$\nLe biais de $\\hat\\gamma$ s'écrit\n$$\nB(\\hat\\gamma)=\\E(\\hat \\gamma)-\\gamma=\\E(Q\\hat \\beta)-\\gamma=Q\\E(\\hat \\beta)-\\gamma=QAX\\beta-\\gamma.\n$$\nComme $\\gamma=Q\\beta$ et $Q'Q=I_p$ nous avons\n$$\nB(\\hat\\gamma)=QAXQ'\\gamma-\\gamma.\n$$\nLa variance de $\\hat \\beta$ s'écrit\n$$\n\\V(\\hat \\beta)=\\V(AY)=A\\V(Y)A'=\\sigma^2 AA',\n$$\net celle de $\\hat \\gamma$ est\n$$\n\\V(\\hat\\gamma)=\\V(Q\\hat \\beta)=Q\\V(\\hat \\beta)Q'=\\sigma^2 QAA'Q'.\n$$\nNous en déduisons que les matrices des EQM sont respectivement\n$$\n\\begin{eqnarray*}\n\\EQM(\\hat \\beta)&=&(AX\\beta-\\beta)(AX\\beta-\\beta)'+\\sigma^2 AA',\\\\\n\\EQM(\\hat \\gamma)&=&(QAXQ'\\gamma-\\gamma)(QAXQ'\\gamma-\\gamma)' + \\sigma^2 QAA'Q',\n\\end{eqnarray*}\n$$\net enfin les traces de ces matrices s'écrivent\n$$\n\\begin{eqnarray*}\n\\tr(\\EQM(\\hat \\beta))&=&(AX\\beta-\\beta)'(AX\\beta-\\beta)+\\sigma^2\\tr(AA'),\\\\\n\\tr(\\EQM(\\hat \\gamma))&=&(QAXQ'\\gamma-\\gamma)'(QAXQ'\\gamma-\\gamma)+ \\sigma^2\\tr(AA').\\\\\n\\end{eqnarray*}\n$$\nRappelons que $\\gamma=Q\\beta$ et que $Q'Q=I_p$, nous avons donc\n$$\n\\begin{eqnarray*}\n\\tr(\\EQM(\\hat \\gamma))&=&\\gamma'(QAXQ'-I_p)'(QAXQ'-I_p)\\gamma+ \\sigma^2\\tr(AA')\\\\\n&=&\\beta'(QAX - Q)'(QAX - Q)\\beta+ \\sigma^2\\tr(AA')\\\\\n&=&\\beta'(AX-I_p)Q'Q(AX-I_p)\\beta+ \\sigma^2\\tr(AA')\\\\\n&=&\\beta'(AX-I_p)(AX-I_p)\\beta+ \\sigma^2\\tr(AA')=\\tr(\\EQM(\\hat \\beta)).\n\\end{eqnarray*}\n$$\nEn conclusion, que l'on s'intéresse à un estimateur linéaire de $\\beta$ ou à un estimateur linéaire de $\\gamma$, dès que l'on passe de\nl'un à l'autre en multipliant par $Q$ ou $Q'$, matrice orthogonale, la trace de l'EQM est identique, c'est-à-dire que les performances globales des 2 estimateurs sont identiques.\n\n3.  Nous avons le modèle de régression suivant~:\n$$\nZ_{1:p}=\\Delta\\gamma+\\eta_{1:p},\n$$\net donc, par définition de l'estimateur des MC, nous avons\n$$\n\\hat \\gamma_{\\mathrm{MC}}=(\\Delta'\\Delta)^{-1}\\Delta'Z_{1:p}.\n$$\nComme $\\Delta$ est une matrice diagonale, nous avons\n$$\n\\hat \\gamma_{\\mathrm{MC}}=\\Delta^{-2}\\Delta'Z_{1:p}=\\Delta^{-1}Z_{1:p}.\n$$\nCet estimateur est d'expression très simple et il est toujours défini de manière unique, ce qui n'est pas forcément le cas de $\\hat \\beta_{\\mathrm{MC}}$.\n\n\n    Comme $Z_{1:p}\\sim\\mathcal{N}(\\Delta\\gamma,\\sigma^2 I_p)$ nous avons que $\\hat \\gamma_{\\mathrm{MC}}=\\Delta^{-1}Z_{1:p}$ suit une loi normale d'espérance $\\E(\\Delta^{-1}Z_{1:p})=\\Delta^{-1}\\E(Z_{1:p})=\\gamma$ et de variance $\\V(\\hat \\gamma_{\\mathrm{MC}})=\\sigma^2\\Delta^{-2}$. Puisque $\\hat \\gamma_{\\mathrm{MC}}$ est un estimateur des MC, il est sans biais, ce qui est habituel.\n\n4.  L'EQM de $\\hat \\gamma_{\\mathrm{MC}}$, estimateur sans biais, est simplement sa variance. Pour la $i^e$ coordonnée de\n$\\hat \\gamma_{\\mathrm{MC}}$, l'EQM est égal à l'élément $i,i$ de la matrice de variance $\\V(\\hat \\gamma_{\\mathrm{MC}})$, c'est-à-dire\n$\\sigma^2/\\delta_i^2$. La trace de l'EQM est alors simplement la somme, sur toutes les coordonnées $i$, de cet EQM obtenu.\n\n5.  Par définition $\\hat \\gamma(c)=\\diag(c_i)Z_{1:p}$ et nous savons que $Z_{1:p}\\sim\\mathcal{N}(\\Delta\\gamma,\\sigma^2 I_p).$ Nous obtenons que $\\hat \\gamma(c)$ suit une loi normale d'espérance $\\E(\\diag(c_i)Z_{1:p})=\\diag(c_i)\\Delta\\gamma$ et de variance\n$$\n\\V(\\hat \\gamma(c))= \\diag(c_i)\\V(Z_{1:p})\\diag(c_i)'= \\sigma^2\\diag(c_i^2).\n$$\nLa loi de $\\hat \\gamma(c)$ étant une loi normale de matrice de variance diagonale, nous en déduisons que les coordonnées\n  de $\\hat \\gamma(c)$ sont indépendantes entre elles.\n  \n6.  Calculons l'EQM de la $i^e$ coordonnée de $\\hat \\gamma(c)$\n$$\n\\EQM(\\hat \\gamma(c)_i)=\\E(\\hat \\gamma(c)_i -\\gamma)^2=\\E(\\hat \\gamma(c)_i^2)+\n\\E(\\gamma_i^2)-2\\E(\\hat \\gamma(c)_i \\gamma_i).\n$$\nComme $\\gamma_i$ et que $\\E(\\hat \\gamma(c)_i^2)=\\V(\\hat \\gamma(c)_i^2)+\\{\\E(\\hat \\gamma(c)_i^2)\\}^2$, nous avons\n$$\n\\begin{align*}\n\\EQM(\\hat \\gamma(c)_i)&=\\sigma^2 c_i^2+(c_i\\delta_i\\gamma_i)^2+\\gamma_i^2-2\\gamma_i\\E(\\hat \\gamma(c)_i)\\\\\n&=\\sigma^2 c_i^2+(c_i\\delta_i\\gamma_i)^2+\\gamma_i^2-2\\sigma^2 c_i\\delta_i\\gamma_i= \\sigma^2c_i^2+\\gamma_i^2(c_i\\delta_i -1)^2.\n\\end{align*}\n$$\n7.  De manière évidente si $\\gamma_i^2$ diminue, alors l'EQM de $\\hat \\gamma(c)_i$ diminue aussi. Calculons la valeur de l'EQM quand\n$\\gamma_i^2=\\frac{\\sigma^2}{\\delta_i^2}\\frac{(1/\\delta_i)+c_i}{(1/\\delta_i)-c_i}$. Nous avons, grâce à la question précédente,\n$$\n\\begin{eqnarray*}\n\\EQM(\\hat \\gamma(c)_i)&=&\\sigma^2 c_i^2+(c_i\\delta_i -1)^2\\frac{\\sigma^2}{\\delta_i^2}\\frac{(1/\\delta_i)+c_i}{(1/\\delta_i)-c_i}\\\\\n&=&\\sigma^2 c_i^2+\\frac{\\sigma^2}{\\delta_i^2}(1 - c_i\\delta_i)^2\\frac{1+\\delta_ic_i}{1-\\delta_ic_i}\\\\\n&=&\\sigma^2 c_i^2+\\frac{\\sigma^2}{\\delta_i^2}(1 - c_i\\delta_i)(1+\\delta_ic_i)\\\\\n&=&\\sigma^2 c_i^2+\\frac{\\sigma^2}{\\delta_i^2}(1-\\delta_i^2c_i^2)\\\\\n&=&\\sigma^2 c_i^2+\\frac{\\sigma^2}{\\delta_i^2}-\\sigma^2c_i^2=\\frac{\\sigma^2}{\\delta_i^2}\\\\\n&=&\\EQM(\\hat \\gamma_{\\mathrm{MC}}),\n\\end{eqnarray*}\n$$\nd'où la conclusion demandée.\n\n8.  Par définition de $\\hat \\gamma(c)$, nous avons\n$$\n\\begin{eqnarray*}\n\\hat \\gamma(c)&=&\\diag(c_i)Z_{1:p}=\\diag(\\frac{\\delta_i}{\\delta_i^2+\\kappa})Z_{1:p}\\\\\n&=&(\\Delta'\\Delta + \\kappa I_p)^{-1}\\Delta'Z_{1:p},\n\\end{eqnarray*}\n$$\npuisque $\\Delta$ est diagonale. De plus nous avons\n$$\nD =\n\\bigl( \\begin{smallmatrix}\n  \\Delta\\\\\n0\n\\end{smallmatrix}\\bigr),\n$$\nce qui entraîne que $D'D=\\Delta'\\Delta$ et $D'Z=\\Delta' Z_{1:p}$.\nNous obtenons donc\n$$\n\\hat \\gamma(c)=(D'D+\\kappa I_p)^{-1}D'Z.\n$$\nRappelons que $D=PXQ'$ avec $P$ et $Q$ matrices orthogonales, nous avons\nalors\n$$\n\\begin{eqnarray*}\n\\hat \\gamma(c)&=&(QX'P'PXQ' + \\kappa I_p)^{-1} D'Z=(QX'XQ' + \\kappa QQ')^{-1}D'Z\\\\\n&=&(Q(X'X  + \\kappa I_p)Q')^{-1}D'Z=(Q')^{-1}(X'X  + \\kappa I_p)^{-1}(Q)^{-1}D'Z\\\\\n&=&Q(X'X  + \\kappa I_p)^{-1}Q'D'Z.\n\\end{eqnarray*}\n$$\nComme $Z=PY$ et $D=PXQ'$, nous avons\n$$\n\\hat \\gamma(c)=Q(X'X  + \\kappa I_p)^{-1}Q' QX'P' PY=Q(X'X  + \\kappa I_p)^{-1}XY.\n$$\nEnfin, nous savons que $Q\\hat\\gamma=\\hat \\beta$, nous en déduisons que $\\hat\\gamma=Q'\\hat \\beta$ et donc que dans le cas particulier où $c_i=\\frac{\\delta_i}{\\delta_i^2+\\kappa}$ nous obtenons\n$$\n\\hat \\beta=Q\\hat \\gamma(c)=(X'X  + \\kappa I_p)^{-1}XY,\n$$\nc'est-à-dire l'estimateur de la régression ridge.\n:::\n\n::: {#exr-9-6 name=\"Coefficient constant et régression sous contraintes\"}\n\n:::\n\n::: {#exr-9-7 name=\"Unicité pour la régression lasso, Giraud (2014)\"}\n\n:::\n\n::: {#exr-9-8 name=\"Traitement d'un signal\"}\n1.  \n\n\n    ::: {#4fb3a631 .cell execution_count=2}\n    ``` {.python .cell-code}\n    signal = pd.read_csv(\"../donnees/courbe_lasso.csv\")\n    donnees = pd.read_csv(\"../donnees/echan_lasso.csv\")\n    \n    plt.figure(figsize=(10, 6))\n    plt.plot(signal['x'], signal['y'], label='Signal')\n    plt.scatter(donnees['X'], donnees['Y'], color='red', label='Données')\n    \n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.legend()\n    plt.title('Signal et Données')\n    \n    plt.show()\n    ```\n    \n    ::: {.cell-output .cell-output-display}\n    ![](chap9_files/figure-html/cell-3-output-1.png){width=823 height=524}\n    :::\n    :::\n    \n    \n2.  Nous cherchons à reconstruire le signal à partir de l'échantillon. Bien entendu, vu la forme du signal, un modèle linéaire de la forme \n$$\ny_i=\\beta_0+\\beta_1x_i+\\varepsilon_i\n$$ \nn'est pas approprié. De nombreuses approches en **traitement du signal** proposent d'utiliser une `base` ou `dictionnaire` représentée par une collection de fonctions $\\{\\psi_j(x)\\}_{j=1,\\dots,K}$ et de décomposer le signal dans cette base :\n$$\nm(x)\\approx \\sum_{j=1}^K \\beta_j\\psi_j(x).\n$$\nPour un dictionnaire donné, on peut alors considérer un **modèle linéaire**\n$$\n  y_i=\\sum_{j=1}^K \\beta_j\\psi_j(x_i)+\\varepsilon_i.\n$$ {#eq-mod-lin-signal}\nLe problème est toujours d'estimer les paramètres $\\beta_j$ mais les variables sont maintenant définies par les éléments du dictionnaire. Il existe différents types de dictionnaire, dans cet exercice nous proposons de considérer la base de Fourier définie par\n$$\n\\psi_0(x)=1,\\quad \\psi_{2j-1}(x)=\\cos(2j\\pi x)\\quad\\text{et}\\quad \\psi_{2j}(x)=\\sin(2j\\pi x),\\quad j=1,\\dots,K.\n$$\n\n3.  \n\n\n    ::: {#1e9888f9 .cell execution_count=3}\n    ``` {.python .cell-code}\n    def mat_dict(K, x):\n        # Initialiser une matrice de zéros avec la taille appropriée\n        res = np.zeros((len(x), 2 * K))\n        \n        # Remplir la matrice avec les valeurs cos et sin\n        for j in range(1, K + 1):\n            res[:, 2 * j - 2] = np.cos(2 * j * np.pi * x)\n            res[:, 2 * j - 1] = np.sin(2 * j * np.pi * x)\n        \n        # Convertir la matrice en DataFrame pour un usage similaire à tibble\n        res_df = pd.DataFrame(res)\n        \n        return res_df\n    ```\n    :::\n    \n    \n4.  Il suffit d'ajuster le modèle linéaire où les variables explicatives sont données par le dictionnaire :\n\n\n    ::: {#4eb18776 .cell execution_count=4}\n    ``` {.python .cell-code}\n    # Créer le dictionnaire pour les données\n    D25 = mat_dict(25, donnees['X'])\n    D25['Y'] = donnees['Y']\n    \n    # Ajuster le modèle linéaire\n    X = sm.add_constant(D25.drop(columns='Y'))\n    y = D25['Y']\n    mod_lin = sm.OLS(y, X).fit()\n    \n    # Créer le dictionnaire pour le signal\n    S25 = mat_dict(25, signal['x'])\n    \n    # Faire des prédictions\n    S25 = sm.add_constant(S25)\n    prev_MCO = mod_lin.predict(S25)\n    \n    # Préparer les données pour le tracé\n    signal1 = signal.copy()\n    signal1['MCO'] = prev_MCO\n    signal1 = signal1.rename(columns={'y': 'signal'})\n    signal2 = signal1.melt(id_vars=['x'], value_vars=['signal', 'MCO'], var_name='meth', value_name='y')\n    \n    # Tracer les résultats\n    plt.figure(figsize=(10, 6))\n    for key, grp in signal2.groupby('meth'):\n        plt.plot(grp['x'], grp['y'], label=key)\n    plt.scatter(donnees['X'], donnees['Y'], color='red', label='Données')\n    plt.ylim(-2, 2)\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.legend()\n    plt.title('Signal et Données')\n    plt.show()\n    ```\n    \n    ::: {.cell-output .cell-output-display}\n    ![](chap9_files/figure-html/cell-5-output-1.png){width=823 height=524}\n    :::\n    :::\n    \n    \n    Le signal estimé a tendance à surajuster les données. Cela vient du fait qu'on estime 51 paramètres avec seulement 60 observations.\n\n5.  On regarde tout d'abord le `chemin de régularisation` des estimateurs **lasso**\n\n\n    ::: {#512fc2f5 .cell execution_count=5}\n    ``` {.python .cell-code}\n    X_25 = D25.drop(columns='Y').values\n    y_25 = D25['Y'].values\n    \n    # Ajuster le modèle Lasso et obtenir le chemin de régularisation\n    alphas, coefs, _ = lasso_path(X_25, y_25, alphas=np.logspace(-4, 0, 100))\n    \n    # Tracer le chemin de régularisation\n    plt.figure(figsize=(10, 6))\n    # Tracer les coefficients pour chaque alpha\n    for coef in coefs:\n        plt.plot(-np.log10(alphas), coef)\n    \n    plt.xlabel('-log(alpha)')\n    plt.ylabel('Coefficients')\n    plt.title('Lasso Paths')\n    plt.axis('tight')\n    plt.show()\n    ```\n    \n    ::: {.cell-output .cell-output-display}\n    ![](chap9_files/figure-html/cell-6-output-1.png){width=823 height=523}\n    :::\n    :::\n    \n    \n    Il semble que quelques coefficients quittent la valeur 0 bien avant les autres. On effectue maintenant la validation croisée pour sélectionner le paramètre $\\lambda$.\n\n\n    ::: {#d81f11f8 .cell execution_count=6}\n    ``` {.python .cell-code}\n    lasso_cv = LassoCV(cv=10, random_state=1234).fit(X_25, y_25)\n    \n    # Tracer les résultats de la validation croisée\n    m_log_alphas = -np.log10(lasso_cv.alphas_)\n    \n    plt.figure(figsize=(10, 6))\n    plt.plot(m_log_alphas, lasso_cv.mse_path_, ':')\n    plt.plot(m_log_alphas, lasso_cv.mse_path_.mean(axis=-1), 'k', label='Average across the folds', linewidth=2)\n    plt.axvline(-np.log10(lasso_cv.alpha_), linestyle='--', color='k', label='alpha: CV estimate')\n    \n    plt.xlabel('-log(alpha)')\n    plt.ylabel('Mean square error')\n    plt.title('Lasso CV Paths')\n    plt.legend()\n    plt.axis('tight')\n    plt.show()\n    ```\n    \n    ::: {.cell-output .cell-output-display}\n    ![](chap9_files/figure-html/cell-7-output-1.png){width=812 height=523}\n    :::\n    :::\n    \n    \n\n    ::: {#9ae50fe5 .cell execution_count=7}\n    ``` {.python .cell-code}\n    S25 = mat_dict(25, signal['x'])\n    prev_lasso = lasso_cv.predict(S25)\n    signal1['lasso'] = prev_lasso\n    signal1 = signal1.rename(columns={'y': 'signal'})\n    signal2 = signal1.melt(id_vars=['x'], value_vars=['signal','MCO' ,'lasso'], var_name='meth', value_name='y')\n    # Tracer les résultats\n    plt.figure(figsize=(10, 6))\n    for key, grp in signal2.groupby('meth'):\n        plt.plot(grp['x'], grp['y'], label=key)\n    plt.scatter(donnees['X'], donnees['Y'], color='red', label='Données')\n    plt.ylim(-2, 2)  # Fixer les limites de l'axe des ordonnées\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.legend()\n    plt.title('Signal et Données avec Lasso')\n    plt.show()\n    ```\n    \n    ::: {.cell-output .cell-output-display}\n    ![](chap9_files/figure-html/cell-8-output-1.png){width=823 height=524}\n    :::\n    :::\n    \n    \n\n\n:::\n\n::: {#exr-9-9 name=\"GridSearchCV\"}\n\n:::\n\n::: {#exr-9-10 name=\"Variablilité de la validation croisée $K$ blocs\"}\n\n1.  Il suffit d'écrire \n$$\n\\bar{\\mathcal{R}}=\\frac{1}{n}\\sum_{k=1}^K \\sum_{i\\in\\mathcal I_k}(y_i-\\hat m_k(x_i))^2=\\frac{1}{K}\\sum_{k=1}^K\\frac{n}{K}\\sum_{i\\in\\mathcal I_k}(y_i-\\hat m_k(x_i))^2.\n$$\n\n2.  Les données étant i.i.d. et les blocs étant de taille égale, on a $\\V(\\mathcal R_1)=\\V(\\mathcal R_2)=\\dots=\\V(\\mathcal R_K)$. On obtient donc le résutat demandé en négligeant les covariances.\n\n3.  On estime la variance d'un bloc $\\V(\\mathcal R_1)$ par la variance empirique des erreurs sur chaque bloc :\n$$\n\\frac{1}{K-1}\\sum_{k=1}^K(\\mathcal{R}_k - \\bar{\\mathcal{R}})^{2}.\n$$\nEn utilisant la question précédente, on estime la variance de $\\bar{\\mathcal{R}}$ par\n$$\n\\V(\\bar{\\mathcal{R}}) = \\frac{1}{K(K-1)}\\sum_{k=1}^K(\\mathcal{R}_k - \\bar{\\mathcal{R}})^{2}.\n$$\n:::\n\n",
    "supporting": [
      "chap9_files"
    ],
    "filters": [],
    "includes": {}
  }
}