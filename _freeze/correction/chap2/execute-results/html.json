{
  "hash": "64586567e1a85650dcde7f83d963efd2",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"2 La régression linéaire multiple\"\ntoc: true\n---\n\n\n\n\n::: {.content-hidden}\n\n\\DeclareMathOperator{\\C}{Cov}\n\\DeclareMathOperator{\\V}{V}\n\\DeclareMathOperator{\\leR}{R^2}\n\\newcommand{\\M}{\\mathcal M}\n\\DeclareMathOperator{\\MCO}{MCO}\n\\DeclareMathOperator{\\SCR}{SCR}\n\\DeclareMathOperator{\\SCT}{SCT}\n\\DeclareMathOperator{\\SCE}{SCE}\n\\DeclareMathOperator{\\EQM}{EQM}\n\\newcommand{\\1}{\\mathbf{1}}\n\\newcommand{\\un}{\\mathbf{1}}\n\\newcommand{\\D}{\\displaystyle}\n\n\n\\newcommand{\\prob}{\\mathbf P}\n\\newcommand{\\argmin}{\\mathop{\\mathrm{argmin}}}\n\\newcommand{\\ind}{\\mathbf 1}\n\\newcommand{\\R}{\\mathbb R}\n\\newcommand{\\E}{\\mathbf E}\n\\newcommand{\\var}{\\mathbf V}\n\\newcommand{\\ps}[2]{\\langle #1,#2\\rangle}\n\\newcommand{\\card}[1]{|{#1}|}\n\\newcommand{\\cov}{\\mathbf{cov}}\n\\newcommand{\\corr}{\\text{corr}}\n\\newcommand{\\AUC}{\\text{AUC}}\n\\newcommand{\\logit}{\\text{logit}}\n\\newcommand{\\diag}{\\text{diag}}\n\\newcommand{\\tr}{\\text{tr}}\n\\newcommand{\\li}[2]{#1_{#2}}\n\\newcommand{\\ssli}[2]{#1_{(#2)}}\n\\newcommand{\\HH}{{\\mathcal{H}}}\n\n\n:::\n\n::: {#exr-2-1 name=\"Question de cours\"}\nA, A, B, B, B, C.\n:::\n\n::: {#exr-2-2 name=\"Covariance de $\\hat\\varepsilon$ et $\\hat Y$\"}\nNous allons montrer que, pour tout autre estimateur $\\tilde{\\beta}$ de \n$\\beta$ linéaire et sans biais, $\\V (\\tilde{\\beta})  \\geq \\V (\\hat \\beta)$.\nDécomposons la variance de $\\tilde{\\beta}$\n\\begin{eqnarray*}\n\\V (\\tilde{\\beta})  = \\V (\\tilde{\\beta} - \\hat \\beta+\\hat \\beta)\n=\\V (\\tilde{\\beta} - \\hat \\beta)+\\V (\\hat \\beta) -\n2 \\C(\\tilde{\\beta} - \\hat \\beta,\\hat \\beta).\n\\end{eqnarray*}\nLes variances étant définies positives, si nous montrons que \n$\\C(\\tilde{\\beta}- \\hat \\beta,\\hat \\beta)=0$, nous aurons \nfini la démonstration.\\\\\nPuisque $\\tilde{\\beta}$ est linéaire, $\\tilde{\\beta} = A Y$. \nDe plus, nous savons qu'il est sans biais, c'est-à-dire \n$\\E (\\tilde{\\beta}) = \\beta$ pour tout $\\beta$, donc $A X = I$. \nLa covariance devient :\n\\begin{eqnarray*}\n\\C(\\tilde{\\beta} - \\hat \\beta,\\hat \\beta) &=& \n\\C(A Y,(X'X)^{-1}X'Y) - \\V (\\hat \\beta)\\\\\n&=& \\sigma^2 A X (X'X)^{-1} - \\sigma^2 (X'X)^{-1}=0.\n\\end{eqnarray*}\n:::\n\n::: {#exr-2-3 name=\"Théorème de Gauss Markov\"}\nNous devons montrer que, parmi tous les estimateurs linéaires \nsans biais, l'estimateur de MC est celui qui a la plus petite \nvariance. La linéarité de $\\hat \\beta$ est évidente. Calculons sa variance :\n\\begin{eqnarray*}\n\\V (\\hat \\beta) = \\V ((X'X)^{-1}X'Y) = \n(X'X)^{-1}X'\\V(Y)X(X'X)^{-1}=\\sigma^2 (X'X)^{-1}.\n\\end{eqnarray*}\nNous allons montrer que, pour tout autre estimateur $\\tilde{\\beta}$ de \n$\\beta$ linéaire et sans biais, $\\V (\\tilde{\\beta})  \\geq \\V (\\hat \\beta)$.\nDécomposons la variance de $\\tilde{\\beta}$\n\\begin{eqnarray*}\n\\V (\\tilde{\\beta})  = \\V (\\tilde{\\beta} - \\hat \\beta+\\hat \\beta)\n=\\V (\\tilde{\\beta} - \\hat \\beta)+\\V (\\hat \\beta) -\n2 \\C(\\tilde{\\beta} - \\hat \\beta,\\hat \\beta).\n\\end{eqnarray*}\nLes variances étant définies positives, si nous montrons que \n$\\C(\\tilde{\\beta}- \\hat \\beta,\\hat \\beta)=0$, nous aurons \nfini la démonstration.\\\\\nPuisque $\\tilde{\\beta}$ est linéaire, $\\tilde{\\beta} = A Y$. \nDe plus, nous savons qu'il est sans biais, c'est-à-dire \n$\\E (\\tilde{\\beta}) = \\beta$ pour tout $\\beta$, donc $A X = I$. \nLa covariance devient :\n\\begin{eqnarray*}\n\\C(\\tilde{\\beta} - \\hat \\beta,\\hat \\beta) &=& \n\\C(A Y,(X'X)^{-1}X'Y) - \\V (\\hat \\beta)\\\\\n&=& \\sigma^2 A X (X'X)^{-1} - \\sigma^2 (X'X)^{-1}=0.\n\\end{eqnarray*}\n:::\n\n::: {#exr-2-4 name=\"Représentation des variables\"}\nNous représentons les données dans $\\R^2$ pour le premier \njeu et dans $\\R^3$ pour le second.\n\n![](FIGURES/exo2-4.png){width='75%' fig-align=\"center\"}\n\n\nDans le premier modèle, nous projetons $Y$ sur l'espace \nengendré par $X$, soit la droite de vecteur directeur $\\overrightarrow{OX}$.\nNous trouvons par le calcul $\\hat \\beta = 1.47$, résultat \nque nous aurions pu trouver graphiquement car $\\overrightarrow{O \\hat Y}=\n\\hat \\beta . \\overrightarrow{OX}$.\n\nConsidérons $\\R^3$ muni de la base orthonormée\n$(\\vec{i},\\vec{j},\\vec{k})$.  Les vecteurs $\\overrightarrow{OX}$ et\n$\\overrightarrow{OZ}$ engendrent le même plan que celui engendré par\n$(\\vec{i},\\vec{j})$. La projection de $Y$ sur ce plan donne\n$\\overrightarrow{O \\hat Y}$. Il est quasiment impossible de trouver\n$\\hat \\beta$ et $\\hat \\gamma$ graphiquement mais nous trouvons par le\ncalcul $\\hat \\beta = -3.33$ et $\\hat \\gamma =5$.\n\n:::\n\n::: {#exr-2-5 name=\"Modèles emboîtés\"}\nNous obtenons\n\\begin{eqnarray*}\n\\hat Y_p = X \\hat \\beta \\quad  \\hbox{et} \\quad \\hat Y_q= X_q \\hat \\gamma.\n\\end{eqnarray*}\nPar définition du $\\R2$, il faut comparer la norme au carré des\nvecteurs $\\hat Y_p$ et $\\hat Y_q$. Notons les espaces engendrés\npar les colonnes de $X_q$ et $X$, $\\M_{X_q}$ et $\\M_{X}$, nous avons \n$\\M_{X_q} \\subset  \\M_{X}$. Nous obtenons alors \n\\begin{eqnarray*}\n\\hat Y_p = P_{X_p}Y \n= (P_{X_q} + P_{X^{\\perp}_q})P_{X_p}Y &=& P_{X_q}P_{X_p}Y + P_{X^{\\perp}_q}P_{X_p}Y\\\\\n&=& P_{X_q}Y + P_{X^{\\perp}_q \\cap X_p} Y\\\\\n&=& \\hat Y_q + P_{X^{\\perp}_q \\cap X_p} Y.\n\\end{eqnarray*}\nEn utilisant le théorème de Pythagore, nous avons\n\\begin{eqnarray*}\n\\| \\hat Y_p \\|^2 &=& \\|\\hat Y_q \\|^2 + \\| P_{X^{\\perp}_q \\cap X_p} Y \\|^2 \n\\geq \\|\\hat Y_q \\|^2,\n\\end{eqnarray*}\nd'où\n\\begin{eqnarray*}\n\\R2(p)=\\frac{\\| \\hat Y_p \\|^2}{\\| Y \\|^2} \\geq\n\\frac{\\| \\hat Y_q \\|^2}{\\| Y \\|^2} =\\R2(q).\n\\end{eqnarray*}\n\nEn conclusion, lorsque les modèles sont emboîtés $\\M_{X_q} \\subset  \\M_{X}$, \nle $\\R2$ du modèle le plus grand (ayant le plus de variables) sera\ntoujours plus grand que le $\\R2$ du modèle le plus petit.\n:::\n\n::: {#exr-2-6}\nLa matrice $X'X$ est symétrique, $n$ vaut 30 et $\\bar x= \\bar z=0$.\nLe coefficient de corrélation\n\\begin{equation*}\n\\rho_{x,z} = \\frac{\\sum_{i=1}^{30} (x_i -\\bar x)(z_i - \\bar z)}\n{\\sqrt{\\sum_{i=1}^{30} (x_i -\\bar x)^2\\sum_{i=1}^{30} (z_i - \\bar z)^2}}\n=\\frac{\\sum_{i=1}^{30} x_i z_i}\n{\\sqrt{\\sum_{i=1}^{30} x_i^2 \\sum_{i=1}^{30} z_i^2}}\n=\\frac{7}{\\sqrt{150}}=0.57.\n\\end{equation*}\nNous avons \n\\begin{eqnarray*}\ny_i &=& -2 +x_i+z_i+\\hat \\varepsilon_i\n\\end{eqnarray*}\net la moyenne vaut alors\n\\begin{eqnarray*}\n\\bar y &=& -2 + \\bar x +\\bar z + \\frac{1}{n}\\sum_i \\hat \\varepsilon_i.\n\\end{eqnarray*}\nLa constante étant dans le modèle, la somme des résidus est\nnulle car le vecteur $\\hat \\varepsilon$ est orthogonal au \nvecteur $\\1$. Nous obtenons donc que la moyenne de $Y$ vaut 2 \ncar $\\bar x=0$ et $\\bar z=0$.\nNous obtenons en développant\n\\begin{eqnarray*}\n\\|\\hat Y \\|^2 &=& \\sum_{i=1}^{30}(-2+x_i+2z_i)^2\\\\\n&=& 4+10+60+14=88.\n\\end{eqnarray*}\nPar le théorème de Pythagore, nous concluons que \n\\begin{eqnarray*}\n\\SCT=\\SCE+\\SCR=88+12=100.\n\\end{eqnarray*}\n\n:::\n\n::: {#exr-2-7 name=\"Changement d'échelle des variables explicatives\"}\nNous avons l'estimation sur le modèle avec les variables originales qui minimise\n\\begin{align*}\n\\MCO(\\beta)&=\\|Y - \\sum_{j=1}^{p} X_j \\beta_j \\|^{2}\n\\end{align*}\nCette solution est notée $\\hat \\beta$.\n\nNous avons l'estimation sur le modèle avec les variables prémultipliées par $a_{j}$ (changement d'échelle) qui minimise\n\n$$   \n\\begin{align*}\n\\tilde{\\MCO}(\\beta)&=\\|Y - \\sum_{j=1}^{p} \\tilde X_j \\beta_j \\|^{2} = \\|Y - \\sum_{j=1}^{p} a_{j} X_j \\beta_j \\|^{2}\\\\\n&= \\|Y - \\sum_{j=1}^{p} X_j \\gamma_j \\|^{2}=\\MCO(\\gamma),\n\\end{align*}\n$$\n\nen posant en dernière ligne $\\gamma_{j}=a_{j} \\beta_j$. La solution\nde de $\\MCO(\\gamma)$ (ou encore $\\MCO(\\beta)$) est $\\hat \\beta$. La\nsolution de $\\tilde{\\MCO}(\\beta)$ est alors donnée par\n$\\hat \\beta_{j}=a_{j} \\tilde \\beta_j$.\n:::\n\n::: {#exr-2-8 name=\"Différence entre régression multiple et régressions simples\"}\n1.  Calculons l'estimateur des MCO noté traditionnellement $(X'X)^{-1}X'Y$ avec la matrice $X$ qui possède ici deux colonnes (notées ici $X$ et $Z$) et $n$ lignes. On a donc\n    \\begin{align*}\n      (X'X)&=\n             \\begin{pmatrix}\n               \\|X\\|^{2} & <X,Z>\\\\\n               <X,Z> & \\|Z\\|^{2} \\\\\n             \\end{pmatrix}\n    \\end{align*}\n    Son déterminant est $\\Delta= \\|X\\|^{2}\\|Z\\|^{2} - 2 <X,Z>$ et son inverse est\n    \\begin{align*}\n      \\frac{1}{\\Delta}\n      \\begin{pmatrix}\n               \\|Z\\|^{2} & -<X,Z>\\\\\n               -<X,Z> & \\|X\\|^{2} \\\\\n      \\end{pmatrix}\n    \\end{align*}\n    Ensuite $X'Y$ est simplement le vecteur colonne de coordonnées $<X,Y>$ et  $<Z,Y>$. En rassemblant le tout nous avons\n    \\begin{align*}\n      \\hat \\beta_{1}&=\\frac{1}{\\Delta}(\\|Z\\|^{2} <X,Y> - <X,Z><Z,Y>),\\\\\n      \\hat \\beta_{2}&=\\frac{1}{\\Delta}(\\|X\\|^{2} <Z,Y> - <X,Z><X,Y>).\n    \\end{align*}\n    Si $<X,Z>=0$ (les deux vecteurs sont orthogonaux) alors cette écriture se simplifie en\n    $$\n      \\hat \\beta_{1}=\\frac{<X,Y>}{\\|X\\|^{2}},\\quad\n      \\hat \\beta_{2}=\\frac{<Z,Y>}{\\|Z\\|^{2}}.\n    $$\n    \n2.  Calculons l'estimateur des MCO noté traditionnellement $(X'X)^{-1}X'Y$ avec la matrice $X$ qui possède ici une colonne (notée ici $X$) et $n$ lignes. On a donc\n    \\begin{align*}\n      \\hat \\beta_{X} = \\frac{<X,Y>}{\\|X\\|^{2}}.\n    \\end{align*}\n    Passons maintenant à la matrice qui possède ici une colonne (notée ici $Z$) et $n$ lignes. On a donc\n    \\begin{align*}\n      \\hat \\beta_{Z} = \\frac{<Z,Y>}{\\|Z\\|^{2}}.\n    \\end{align*}\n    \n3.  En général les coefficients des régressions simples ne sont pas ceux obtenus par régression multiple sauf si les variables sont orthogonales.\n\n4.  Nous avons ici les résidus de la première régression qui sont\n    \\begin{align}\n      \\hat \\varepsilon = Y - \\hat \\beta_{X} X.\n    \\end{align}\n    La deuxième régression (sur les résidus) donne le coefficient\n    \\begin{align*}\n      \\hat \\beta_{Z} = \\frac{<Z,\\hat \\varepsilon>}{\\|Z\\|^{2}} = \\hat \\beta_{Z} - \\hat \\beta_{X}\\frac{<Z,X>}{\\|Z\\|^{2}}\n    \\end{align*}\nLa régression séquentielle donne des coefficients différents des régressions univariées ou bivariées sauf si les variables sont orthogonales.\n  \\end{enumerate}\n\n:::\n\n::: {#exr-2-9 name=\"TP : différence entre régression multiple et régressions simples\"}\n:::\n\n::: {#exr-2-10 name=\"TP : régression multiple et code R\"}\n\n::: {#68bffe14 .cell execution_count=2}\n``` {.python .cell-code}\nimport pandas as pd\n#import numpy as np\n#import matplotlib\n#import matplotlib.pyplot as plt\n#from mpl_toolkits.mplot3d import Axes3D\n#from pandas import DataFrame\n#import statsmodels.api as sm\nimport statsmodels.formula.api as smf\n```\n:::\n\n\n1.  \n\n\n    ::: {#9f2ba1aa .cell execution_count=3}\n    ``` {.python .cell-code}\n    ozone = pd.read_csv(\"../donnees/ozone.txt\", header=0, sep=\";\")\n    nomvar = []\n    for var in ozone.columns[2:]:\n        if (var!=\"nebulosite\") & (var!=\"vent\"):\n            nomvar.append(var)\n    ```\n    :::\n    \n    \n2.  \n\n\n    ::: {#cfc56eeb .cell execution_count=4}\n    ``` {.python .cell-code}\n    fd = \"+\".join(nomvar[2:])\n    ```\n    :::\n    \n    \n3.  \n\n\n    ::: {#b3ab97e9 .cell execution_count=5}\n    ``` {.python .cell-code}\n    formule= \"O3 ~ 1+\" + fd\n    ```\n    :::\n    \n    \n4.  \n\n\n    ::: {#1a864322 .cell execution_count=6}\n    ``` {.python .cell-code}\n    regmult = smf.ols(formule, data = ozone).fit()\n    regmult.summary()\n    ```\n    \n    ::: {.cell-output .cell-output-display execution_count=5}\n    ```{=html}\n    <table class=\"simpletable\">\n    <caption>OLS Regression Results</caption>\n    <tr>\n      <th>Dep. Variable:</th>           <td>O3</td>        <th>  R-squared:         </th> <td>   0.715</td>\n    </tr>\n    <tr>\n      <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.668</td>\n    </tr>\n    <tr>\n      <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   15.06</td>\n    </tr>\n    <tr>\n      <th>Date:</th>             <td>Fri, 31 Jan 2025</td> <th>  Prob (F-statistic):</th> <td>1.19e-09</td>\n    </tr>\n    <tr>\n      <th>Time:</th>                 <td>15:38:38</td>     <th>  Log-Likelihood:    </th> <td> -197.75</td>\n    </tr>\n    <tr>\n      <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   411.5</td>\n    </tr>\n    <tr>\n      <th>Df Residuals:</th>          <td>    42</td>      <th>  BIC:               </th> <td>   426.8</td>\n    </tr>\n    <tr>\n      <th>Df Model:</th>              <td>     7</td>      <th>                     </th>     <td> </td>   \n    </tr>\n    <tr>\n      <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n    </tr>\n    </table>\n    <table class=\"simpletable\">\n    <tr>\n          <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n    </tr>\n    <tr>\n      <th>Intercept</th> <td>   85.0828</td> <td>   12.133</td> <td>    7.012</td> <td> 0.000</td> <td>   60.597</td> <td>  109.568</td>\n    </tr>\n    <tr>\n      <th>Ne12</th>      <td>   -5.2469</td> <td>    1.008</td> <td>   -5.204</td> <td> 0.000</td> <td>   -7.281</td> <td>   -3.212</td>\n    </tr>\n    <tr>\n      <th>N12</th>       <td>    0.2932</td> <td>    1.352</td> <td>    0.217</td> <td> 0.829</td> <td>   -2.435</td> <td>    3.022</td>\n    </tr>\n    <tr>\n      <th>S12</th>       <td>    2.6226</td> <td>    1.887</td> <td>    1.390</td> <td> 0.172</td> <td>   -1.186</td> <td>    6.431</td>\n    </tr>\n    <tr>\n      <th>E12</th>       <td>    0.4653</td> <td>    2.021</td> <td>    0.230</td> <td> 0.819</td> <td>   -3.614</td> <td>    4.545</td>\n    </tr>\n    <tr>\n      <th>W12</th>       <td>    1.4222</td> <td>    2.020</td> <td>    0.704</td> <td> 0.485</td> <td>   -2.653</td> <td>    5.498</td>\n    </tr>\n    <tr>\n      <th>Vx</th>        <td>    0.4364</td> <td>    0.497</td> <td>    0.878</td> <td> 0.385</td> <td>   -0.567</td> <td>    1.439</td>\n    </tr>\n    <tr>\n      <th>O3v</th>       <td>    0.2765</td> <td>    0.100</td> <td>    2.773</td> <td> 0.008</td> <td>    0.075</td> <td>    0.478</td>\n    </tr>\n    </table>\n    <table class=\"simpletable\">\n    <tr>\n      <th>Omnibus:</th>       <td> 0.296</td> <th>  Durbin-Watson:     </th> <td>   1.843</td>\n    </tr>\n    <tr>\n      <th>Prob(Omnibus):</th> <td> 0.863</td> <th>  Jarque-Bera (JB):  </th> <td>   0.383</td>\n    </tr>\n    <tr>\n      <th>Skew:</th>          <td> 0.169</td> <th>  Prob(JB):          </th> <td>   0.826</td>\n    </tr>\n    <tr>\n      <th>Kurtosis:</th>      <td> 2.735</td> <th>  Cond. No.          </th> <td>    549.</td>\n    </tr>\n    </table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n    ```\n    :::\n    :::\n    \n    \n:::\n\n::: {#exr-2-11 name=\"Régression orthogonale\"}\nLes vecteurs étant orthogonaux, nous avons \n$\\M_X = \\M_U \\stackrel{\\perp}{\\oplus} \\M_V$. Nous pouvons \nalors écrire\n\\begin{eqnarray*}\n\\hat Y_X = P_X Y &=& (P_U + P_{U^{\\perp}})P_X Y \\\\\n&=& P_U P_X Y + P_{U^{\\perp}}P_X Y =  P_U Y + P_{U^{\\perp}\\cap X} Y \\\\\n&=& \\hat Y_U + \\hat Y_V.\n\\end{eqnarray*}\nLa suite de l'exercice est identique. En conclusion, effectuer une régression \nmultiple sur des variables orthogonales revient à effectuer $p$ \nrégressions simples.\n:::\n\n::: {#exr-2-12 name=\"Centrage, centrage-réduction et coefficient constant\"}\n:::\n\n\n::: {#exr-2-13 name=\"Moindres carrés contraints\"}\n1.  L'estimateur des MC vaut\n\\begin{eqnarray*}\n\\hat \\beta = (X'X)^{-1}X'Y,\n\\end{eqnarray*}\n\n2.  Calculons maintenant l'estimateur contraint. Nous pouvons procéder de deux manières différentes.\n\n    La première consiste à écrire le lagrangien \n\\begin{eqnarray*}\n\\mathcal{L} = S(\\beta) - \\lambda'(R\\beta-r).\n\\end{eqnarray*}\nLes conditions de Lagrange permettent d'obtenir un minimum\n\\begin{eqnarray*}\n\\left\\{\n\\begin{array}{l}\n\\D \\frac{\\partial \\mathcal{L}}{\\partial \\beta} = -2X'Y+2X'X\\hat{\\beta}_c-\nR'\\hat{\\lambda}=0,\\\\\n\\D \\frac{\\partial \\mathcal{L}}{\\partial \\lambda} = R\\hat{\\beta}_c-r=0,\n\\end{array}\n\\right.\n\\end{eqnarray*}\nMultiplions à gauche la première égalité par $R(X'X)^{-1}$, nous obtenons\n\\begin{eqnarray*}\n-2 R(X'X)^{-1}X'Y+2R(X'X)^{-1}X'X \\hat{\\beta}_c-R(X'X)^{-1}R'\\hat{\\lambda}&=&0\\\\\n-2 R(X'X)^{-1}X'Y+2R\\hat{\\beta}_c-R(X'X)^{-1}R'\\hat{\\lambda}&=&0\\\\\n-2 R(X'X)^{-1}X'Y+2r-R(X'X)^{-1}R'\\hat{\\lambda}&=&0.\n\\end{eqnarray*}\nNous obtenons alors pour $\\hat \\lambda$\n\\begin{eqnarray*}\n\\hat \\lambda = 2 \\left[R(X'X)^{-1}R'\\right]^{-1}\\left[r-R(X'X)^{-1}X'Y\\right].\n\\end{eqnarray*}\nRemplaçons ensuite $\\hat \\lambda$ \n\\begin{eqnarray*}\n-2X'Y+2X'X\\hat{\\beta}_c-R'\\hat{\\lambda}&=&0\\\\\n-2X'Y+2X'X\\hat{\\beta}_c-2R'\\left[R(X'X)^{-1}R'\\right]^{-1}\\left[r-R(X'X)^{-1}X'Y\\right]&=& 0,\n\\end{eqnarray*}\nd'où nous calculons $\\hat \\beta_c$\n\\begin{eqnarray*}\n\\hat \\beta_c &=& (X'X)^{-1}X'Y+(X'X)^{-1}R'\\left[R(X'X)^{-1}R'\\right]^{-1}\n(r-R\\hat \\beta)\\\\\n&=& \\hat \\beta + (X'X)^{-1}R'\\left[R(X'X)^{-1}R'\\right]^{-1}(r-R\\hat \\beta).\n\\end{eqnarray*}\nLa fonction $S(\\beta)$ à minimiser est une fonction convexe sur un \nensemble convexe (contraintes linéaires), le minimum est donc unique.\n\n    Une autre façon de procéder consiste à utiliser les projecteurs.\nSupposons pour commencer que $r=0$, la contrainte vaut donc $R\\beta=0$.\nCalculons analytiquement le projecteur \northogonal sur $\\M_0$. Rappelons que $\\dim(\\M_0)=p-q$, nous avons \nde plus\n\\begin{eqnarray*}\nR \\beta &=& 0 \\quad \\quad \n\\Leftrightarrow \\quad \\beta \\in Ker(R)\\\\\nR (X'X)^{-1}X'X \\beta &=& 0\\\\\nU' X \\beta &=& 0\\quad \\quad \\hbox{où} \\quad \\quad U = X (X'X)^{-1}R'.\n\\end{eqnarray*}\nNous avons donc que $\\forall \\beta \\in \\ker(R)$, $U' X \\beta = 0$, c'est-à-dire \nque $\\M_U$, l'espace engendré par les colonnes de $U$, est orthogonal \nà l'espace engendré par $X\\beta$, $\\forall \\beta \\in \\ker(R)$.\nNous avons donc que $\\M_U \\perp \\M_0$. Comme $U=X[(X'X)^{-1}R']$, \n$\\M_U \\subset \\M_X$. En résumé, nous avons\n\\begin{eqnarray*}\n\\M_U \\subset \\M_X \\quad  \\hbox{et} \n\\quad  \\M_U \\perp \\M_0 \\quad \\hbox{donc} \n\\quad \\M_U \\subset  (\\M_X \\cap \\M_0^{\\perp}).\n\\end{eqnarray*}\nAfin de montrer que les colonnes de $U$ engendrent $\\M_X \\cap \\M_0^{\\perp}$, \nil faut démontrer que la dimension des deux sous-espaces est égale. Or le \nrang de $U$ vaut $q$ ($R'$ est de rang $q$, $(X'X)^{-1}$ est de rang $p$ \net $X$ est de rang $p$) donc la dimension de $\\M_U$ vaut $q$. De plus, \nnous avons vu que \n\\begin{eqnarray*}\n\\M_X = \\M_0 \\stackrel{\\perp}{\\oplus}\\left(\\M_0^{\\perp} \\cap \\M_X \\right)\n\\end{eqnarray*}\net donc, en passant aux dimensions des sous-espaces, nous en \ndéduisons que  $\\dim(\\M_0^{\\perp} \\cap \\M_X )=q$. Nous venons \nde démontrer que\n\\begin{eqnarray*}\n\\M_U = \\M_X \\cap \\M_0^{\\perp}.\n\\end{eqnarray*}\nLe projecteur orthogonal sur $\\M_U=\\M_X \\cap \\M_0^{\\perp}$ s'écrit\n\\begin{eqnarray*}\nP_{U} = U (U'U)^{-1} U'= X (X'X)^{-1} R' [R(X'X)^{-1}R']^{-1}R(X'X)^{-1}X'.\n\\end{eqnarray*}\nNous avons alors \n\\begin{eqnarray*}\n\\hat Y - \\hat Y_0 &=& P_U Y\\\\\nX \\hat \\beta - X \\hat \\beta_0 &=& X (X'X)^{-1} R' [R(X'X)^{-1}R']^{-1}R(X'X)^{-1}X'Y\\\\\n&=& X (X'X)^{-1} R [R(X'X)^{-1}R']^{-1}R \\hat \\beta.\n\\end{eqnarray*}\nCela donne\n\\begin{eqnarray*}\n\\hat \\beta_0 = \\hat \\beta - (X'X)^{-1} R [R(X'X)^{-1}R']^{-1}R \\hat \\beta.\n\\end{eqnarray*}\nSi maintenant $r\\neq 0$, nous avons alors un sous-espace affine défini par \n$\\{\\beta\\in \\R^p : R\\beta=r\\}$ dans lequel nous cherchons une solution qui \nminimise les moindres carrés. Un sous-espace affine peut être défini \nde manière équivalente par un point particulier $\\beta_p \\in \\R^p$ tel \nque $R\\beta_p=r$ et le sous-espace vectoriel associé \n$\\M_0^v=\\{\\beta\\in \\R^p : R\\beta=0\\}$. \nLes points du sous-espace affine sont alors \n$\\{\\beta_0 \\in \\R^p : \\beta_0=\\beta_p+\\beta_0^v, \\beta_0^v \\in \\M_0^v \n\\quad et \\quad \\beta_p : R\\beta_p=r\\}$.\nLa solution qui minimise les moindres carrés, notée $\\hat \\beta_0$, est \nélément de ce sous-espace affine et est définie par \n$\\hat \\beta_0=\\beta_p+\\hat \\beta_0^v$ où\n\\begin{eqnarray*}\n\\hat \\beta_0^v = \\hat \\beta - (X'X)^{-1}R'[R(X'X)^{-1}R']^{-1}R\\hat \\beta.\n\\end{eqnarray*}\nNous savons que $R\\beta_p=r$ donc\n\\begin{eqnarray*}\nR\\beta_p = [R(X'X)^{-1}R'][R(X'X)^{-1}R']^{-1}r\n\\end{eqnarray*}\ndonc une solution particulière est \n$\\beta_p = (X'X)^{-1}R'[R(X'X)^{-1}R']^{-1}r$.\nLa solution $\\hat \\beta_0$ qui minimise les moindres carrés sous la \ncontrainte $R\\beta=r$ est alors \n$$\n\\begin{align}\n\\hat \\beta_0 &= \\beta_p+\\hat \\beta_0^v\\\\\\nonumber\n&=(X'X)^{-1}R'[R(X'X)^{-1}R']^{-1}r +\n\\hat \\beta - (X'X)^{-1}R'[R(X'X)^{-1}R']^{-1}R\\hat \\beta\\\\\\nonumber\n&=\\hat \\beta + (X'X)^{-1}R'[R(X'X)^{-1}R']^{-1}(r-R\\hat \\beta).\n\\end{align}\n$${#eq-betacontraint}\n\n3.  Calculons l'EQM de $\\hat \\beta$ qui vaut selon la formule classique:\n        \\begin{align*}\n          \\EQM &= (\\E(\\hat \\beta) - \\beta) (\\E(\\hat \\beta) - \\beta)' + \\V(\\hat\\beta)\n        \\end{align*}\n    avec $\\E(\\hat \\beta)=\\beta$ (sous l'hypothèse que $Y$ est généré par le modèle de régression) et$\\V(\\hat\\beta) =\\sigma^{2} (X'X)^{-1}$.\n\n    Pour l'EQM de $\\hat \\beta_0$ qui vaut selon la formule classique:\n        \\begin{align*}\n          \\EQM &= (\\E(\\hat \\beta_0) - \\beta_0) (\\E(\\hat \\beta_0) - \\beta_0)' + \\V(\\hat\\beta_0)\n        \\end{align*}\n    calculons d'abord $\\E(\\hat \\beta_0)$ en utilisant l'équation (@eq-betacontraint):\n        \\begin{align*}\n          \\E(\\hat \\beta_0)&=\\E(\\hat \\beta) + \\E[(X'X)^{-1}R'[R(X'X)^{-1}R']^{-1}(r-R\\hat \\beta)]\\\\\n          &=\\beta + (X'X)^{-1}R'[R(X'X)^{-1}R']^{-1}(r-R\\beta)\n        \\end{align*}\n    puisque  $\\hat \\beta$ est sans biais. Si nous supposons que le modèle satisfait la contrainte ($R\\beta=r$) alors là encore le biais est nul.\n\n    Calculons maintemant la variance:\n        \\begin{align*}\n          \\V(\\hat\\beta_0) &=  \\V[\\hat \\beta + (X'X)^{-1}R'[R(X'X)^{-1}R']^{-1}(r-R\\hat \\beta)]\\\\\n                          &=\\V(\\hat\\beta) + \\V[(X'X)^{-1}R'[R(X'X)^{-1}R']^{-1}(r-R\\hat \\beta)] +\\\\\n                          &\\quad \\quad 2\\C[\\hat\\beta ; (X'X)^{-1}R'[R(X'X)^{-1}R']^{-1}(r-R\\hat \\beta)]\\\\\n          &=   \\sigma^{2}V_{X}^{-1} + (\\mathrm{II}) +  (\\mathrm{III})\n        \\end{align*}\n    Intéressons nous à la seconde partie $(\\mathrm{II})$. Comme $X$ est déterministe ainsi que $R$ et $r$ on a en posant pour alléger $V_{X}=(X'X)$ (matrice symétrique)\n        \\begin{align*}\n          (\\mathrm{II}) &= V_{X}^{-1}R'[RV_{X}^{-1}R']^{-1} \\V(r-R\\hat \\beta)\n                            [RV_{X}^{-1}R']^{-1} R V_{X}^{-1}\\\\\n                        &= V_{X}^{-1}R'[RV_{X}^{-1}R']^{-1} \\V(R\\hat \\beta)[RV_{X}^{-1}R']^{-1} R V_{X}^{-1}\\\\\n                        &= V_{X}^{-1}R'[RV_{X}^{-1}R']^{-1} R\\sigma^{2}V_{X}^{-1}R'[RV_{X}^{-1}R']^{-1} R V_{X}^{-1}\\\\\n           &= \\sigma^{2}V_{X}^{-1}R'[RV_{X}^{-1}R']^{-1} R V_{X}^{-1}\n        \\end{align*}\n    La troisième partie est\n        \\begin{align*}\n          (\\mathrm{III}) &= 2V_{X}^{-1}R'[RV_{X}^{-1}R']^{-1}\\C[\\hat\\beta ; (r-R\\hat \\beta)]\\\\\n                         &=-2V_{X}^{-1}R'[RV_{X}^{-1}R']^{-1}R\\C[\\hat\\beta ;\\hat\\beta]\\\\\n          &= -2V_{X}^{-1}R'[RV_{X}^{-1}R']^{-1}R\\sigma^{2}V_{X}^{-1}\n        \\end{align*}\n    Ce qui donne au final\n        \\begin{align*}\n          \\V(\\hat\\beta_0) &=\\sigma^{2}(X'X)^{-1} - \\sigma^{2}(X'X)^{-1}R'[R(X'X)^{-1}R']^{-1}R(X'X)^{-1}\n        \\end{align*}\n    L'écart entre les 2 variances est donc de $\\sigma^{2}(X'X)^{-1}R'[R(X'X)^{-1}R']^{-1}R(X'X)^{-1}$ qui est une matrice de la forme $A'A$ donc semi-définie positive.\n\n:::\n\n",
    "supporting": [
      "chap2_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}