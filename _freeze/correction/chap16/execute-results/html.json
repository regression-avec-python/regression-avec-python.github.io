{
  "hash": "fcbed413afa89b4cae755d0a17e7b047",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"16 Données déséquilibrées\"\ntoc: true\n---\n\n\n\n\n::: {.content-hidden}\n\n\\DeclareMathOperator{\\C}{Cov}\n\\DeclareMathOperator{\\V}{V}\n\\DeclareMathOperator{\\leR}{R^2}\n\\newcommand{\\M}{\\mathcal M}\n\\DeclareMathOperator{\\MCO}{MCO}\n\\DeclareMathOperator{\\SCR}{SCR}\n\\DeclareMathOperator{\\SCT}{SCT}\n\\DeclareMathOperator{\\SCE}{SCE}\n\\DeclareMathOperator{\\EQM}{EQM}\n\\newcommand{\\1}{\\mathbf{1}}\n\\newcommand{\\un}{\\mathbf{1}}\n\\newcommand{\\D}{\\displaystyle}\n\n\n\\newcommand{\\prob}{\\mathbf P}\n\\newcommand{\\argmin}{\\mathop{\\mathrm{argmin}}}\n\\newcommand{\\ind}{\\mathbf 1}\n\\newcommand{\\R}{\\mathbb R}\n\\newcommand{\\E}{\\mathbf E}\n\\newcommand{\\var}{\\mathbf V}\n\\newcommand{\\ps}[2]{\\langle #1,#2\\rangle}\n\\newcommand{\\card}[1]{|{#1}|}\n\\newcommand{\\cov}{\\mathbf{cov}}\n\\newcommand{\\corr}{\\text{corr}}\n\\newcommand{\\AUC}{\\text{AUC}}\n\\newcommand{\\logit}{\\text{logit}}\n\\newcommand{\\diag}{\\text{diag}}\n\\newcommand{\\tr}{\\text{tr}}\n\\newcommand{\\li}[2]{#1_{#2}}\n\\newcommand{\\ssli}[2]{#1_{(#2)}}\n\n\n:::\n\n::: {#38b711ab .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nrng = np.random.default_rng(seed=1234)\n```\n:::\n\n\n::: {#exr-15-1 name=\"Critères pour un exemple de données déséquilibrées\"}\n\n1.  \n\n\n    ::: {#de27070d .cell execution_count=2}\n    ``` {.python .cell-code}\n    n = 500\n    p = 0.05\n    Y = rng.binomial(1, p=p, size=n)\n    ```\n    :::\n    \n    \n2.  \n\n\n    ::: {#7d05ec6b .cell execution_count=3}\n    ``` {.python .cell-code}\n    rng = np.random.default_rng(seed=123)\n    P1 = rng.binomial(1, p=0.005, size=n)\n    ```\n    :::\n    \n    \n3.  \n\n\n    ::: {#33584f20 .cell execution_count=4}\n    ``` {.python .cell-code}\n    P2 = np.zeros_like(P1)\n    for yy in range(n):\n        if Y[yy]==0:\n            P2[yy] = rng.binomial(1, p=0.10, size=1)[0]\n        else:\n            P2[yy] = rng.binomial(1, p=0.85, size=1)[0]\n    ```\n    :::\n    \n    \n4.  \n\n\n    ::: {#78494cf7 .cell execution_count=5}\n    ``` {.python .cell-code}\n    from sklearn.metrics import confusion_matrix\n    print(confusion_matrix(Y, P1))\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    [[478   0]\n     [ 22   0]]\n    ```\n    :::\n    :::\n    \n    \n\n    ::: {#e0cb4311 .cell execution_count=6}\n    ``` {.python .cell-code}\n    print(confusion_matrix(Y, P2))\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    [[432  46]\n     [  4  18]]\n    ```\n    :::\n    :::\n    \n    \n5.  \n\n\n    ::: {#59af29f9 .cell execution_count=7}\n    ``` {.python .cell-code}\n    cm = confusion_matrix(Y, P2)\n    acc = cm.diagonal().sum()/cm.sum()\n    rec = cm[1,1]/cm[1,:].sum()\n    prec = cm[1,1]/cm[:,1].sum()\n    print(acc)\n    print(rec)\n    print(prec)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    0.9\n    0.8181818181818182\n    0.28125\n    ```\n    :::\n    :::\n    \n    \n6.  \n\n\n    ::: {#6db13b3a .cell execution_count=8}\n    ``` {.python .cell-code}\n    F1 = 2*(rec*prec)/(rec+prec)\n    print(F1)\n    rand = cm[:,0].sum()/n*cm[0,:].sum()/n + cm[:,1].sum()/n*cm[1,:].sum()/n\n    kappa = (acc-rand)/(1-rand)\n    print(kappa)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    0.41860465116279066\n    0.37786183555644093\n    ```\n    :::\n    :::\n    \n    \n7.  \n\n\n    ::: {#da2ed9a1 .cell execution_count=9}\n    ``` {.python .cell-code}\n    from sklearn.metrics import accuracy_score, recall_score, precision_score\n    from sklearn.metrics import f1_score, cohen_kappa_score\n    print(accuracy_score(Y, P2), \"**\", accuracy_score(Y, P1))\n    print(recall_score(Y, P2), \"**\", recall_score(Y, P1))\n    print(precision_score(Y, P2), \"**\", precision_score(Y, P1))\n    print(f1_score(Y, P2), \"**\", f1_score(Y, P1))\n    print(cohen_kappa_score(Y, P2), \"**\", cohen_kappa_score(Y, P1))\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    0.9 ** 0.956\n    0.8181818181818182 ** 0.0\n    0.28125 ** 0.0\n    0.4186046511627907 ** 0.0\n    0.3778618355564404 ** 0.0\n    ```\n    :::\n    :::\n    \n    \n:::\n\n\n::: {#exr-15-2 name=\"Échantillonnage rétrospectif\"}\nOn remarque d'abord que $\\prob(\\tilde y_i=1)=\\prob(y_i=1|s_i=1)$. De plus\n$$\n\\logit\\, p_\\beta(x_i)=\\log\\frac{\\prob(y_i=1)}{\\prob(y_i=0)}\\quad\\text{et}\\quad \\logit\\, p_\\gamma(x_i)=\\log\\frac{\\prob(y_i=1|s_i=1)}{\\prob(y_i=0|s_i=1)}.\n$$\nOr\n$$\n\\prob(y_i=1|s_i=1)=\\frac{\\prob(y_i=1,s_i=1)}{\\prob(s_i=1)}=\\frac{\\prob(s_i=1|y_i=1)\\prob(y_i=1)}{\\prob(s_i=1)}\n$$\net\n$$\n\\prob(y_i=0|s_i=1)=\\frac{\\prob(y_i=0,s_i=1)}{\\prob(s_i=1)}=\\frac{\\prob(s_i=1|y_i=0)\\prob(y_i=0)}{\\prob(s_i=1)}.\n$$\nDonc\n$$\n\\logit\\, p_\\gamma(x_i)=\\log\\frac{\\prob(y_i=1)}{\\prob(y_i=0)}+\\log\\frac{\\prob(s_i=1|y_i=1)}{\\prob(s_i=1|y_i=0)}=\\logit\\,p_\\beta(x_i)+\\log\\left(\\frac{\\tau_{1i}}{\\tau_{0i}}\\right).\n$$\n\n:::\n\n::: {#exr-15-3 name=\"Rééquilibrage\"}\n\n1.  \n\n\n    ::: {#56fccb15 .cell execution_count=10}\n    ``` {.python .cell-code}\n    df1 = pd.read_csv(\"../donnees/dd_exo3_1.csv\", header=0, sep=',')\n    df2 = pd.read_csv(\"../donnees/dd_exo3_2.csv\", header=0, sep=',')\n    df3 = pd.read_csv(\"../donnees/dd_exo3_3.csv\", header=0, sep=',')\n    ```\n    :::\n    \n    \n\n    ::: {#571829c5 .cell execution_count=11}\n    ``` {.python .cell-code}\n    print(df1.describe())\n    print(df2.describe())\n    print(df3.describe())\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n                    X1           X2            Y\n    count  1000.000000  1000.000000  1000.000000\n    mean      0.514433     0.492924     0.441000\n    std       0.281509     0.291467     0.496755\n    min       0.000516     0.000613     0.000000\n    25%       0.284947     0.238695     0.000000\n    50%       0.518250     0.494121     0.000000\n    75%       0.753628     0.739679     1.000000\n    max       0.999567     0.999829     1.000000\n                    X1           X2            Y\n    count  1000.000000  1000.000000  1000.000000\n    mean      0.520809     0.472473     0.308000\n    std       0.280013     0.283496     0.461898\n    min       0.002732     0.000890     0.000000\n    25%       0.296167     0.225272     0.000000\n    50%       0.521226     0.468858     0.000000\n    75%       0.764060     0.693746     1.000000\n    max       0.996044     0.999183     1.000000\n                    X1           X2            Y\n    count  1000.000000  1000.000000  1000.000000\n    mean      0.538032     0.454919     0.158000\n    std       0.273863     0.271638     0.364924\n    min       0.004914     0.000613     0.000000\n    25%       0.322489     0.221447     0.000000\n    50%       0.545587     0.450438     0.000000\n    75%       0.781116     0.663637     0.000000\n    max       0.996044     0.999829     1.000000\n    ```\n    :::\n    :::\n    \n    \n\n    ::: {#3a96a574 .cell execution_count=12}\n    ``` {.python .cell-code}\n    colo = [\"C1\", \"C2\"]\n    mark = [\"o\", \"d\"]\n    for yy in [0, 1]:\n        plt.scatter(df1.loc[df1.Y==yy, \"X1\"], df1.loc[df1.Y==yy, \"X2\"], color=colo[yy], marker=mark[yy])\n    ```\n    \n    ::: {.cell-output .cell-output-display}\n    ![](chap16_files/figure-html/cell-13-output-1.png){width=571 height=411}\n    :::\n    :::\n    \n    \n\n    ::: {#7166543e .cell execution_count=13}\n    ``` {.python .cell-code}\n    for yy in [0, 1]:\n        plt.scatter(df2.loc[df2.Y==yy, \"X1\"], df2.loc[df2.Y==yy, \"X2\"], color=colo[yy], marker=mark[yy])\n    ```\n    \n    ::: {.cell-output .cell-output-display}\n    ![](chap16_files/figure-html/cell-14-output-1.png){width=571 height=411}\n    :::\n    :::\n    \n    \n\n    ::: {#5377b48b .cell execution_count=14}\n    ``` {.python .cell-code}\n    for yy in [0, 1]:\n        plt.scatter(df3.loc[df3.Y==yy, \"X1\"], df3.loc[df3.Y==yy, \"X2\"], color=colo[yy], marker=mark[yy])\n    ```\n    \n    ::: {.cell-output .cell-output-display}\n    ![](chap16_files/figure-html/cell-15-output-1.png){width=571 height=411}\n    :::\n    :::\n    \n    \n2.  \n\n\n    ::: {#2716851a .cell execution_count=15}\n    ``` {.python .cell-code}\n    from sklearn.model_selection import train_test_split\n    ## separation en matrice X, Y (et creation du produit=interaction)\n    T1 = df1.drop(columns=\"Y\")\n    X1 = T1.assign(inter= T1.X1 * T1.X2).to_numpy()\n    y1 = df1.Y.to_numpy()\n    T2 = df2.drop(columns=\"Y\")\n    X2 = T2.assign(inter= T2.X1 * T2.X2).to_numpy()\n    y2 = df2.Y.to_numpy()\n    T3 = df3.drop(columns=\"Y\")\n    X3 = T3.assign(inter= T3.X1 * T3.X2).to_numpy()\n    y3 = df3.Y.to_numpy()\n    ## separation apprentissage/validation\n    X1_app, X1_valid, y1_app, y1_valid = train_test_split(\n        X1, y1, test_size=0.33, random_state=1234)\n    X2_app, X2_valid, y2_app, y2_valid = train_test_split(\n        X2, y2, test_size=0.33, random_state=1234)\n    X3_app, X3_valid, y3_app, y3_valid = train_test_split(\n        X3, y3, test_size=0.33, random_state=1234)\n    ```\n    :::\n    \n    \n3.  \n\n\n    ::: {#885c1d76 .cell execution_count=16}\n    ``` {.python .cell-code}\n    from sklearn.linear_model import LogisticRegression\n    mod1 = LogisticRegression(penalty=None, solver=\"newton-cholesky\").fit(X1_app, y1_app)\n    mod2 = LogisticRegression(penalty=None, solver=\"newton-cholesky\").fit(X2_app, y2_app)\n    mod3 = LogisticRegression(penalty=None, solver=\"newton-cholesky\").fit(X3_app, y3_app)\n    ```\n    :::\n    \n    \n\n    ::: {#f1cb3552 .cell execution_count=17}\n    ``` {.python .cell-code}\n    from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, cohen_kappa_score\n    P1 = mod1.predict(X1_valid)\n    P2 = mod1.predict(X2_valid)\n    P3 = mod1.predict(X3_valid)\n    s1 = pd.DataFrame({\"crit\": [\"acc\", \"bal_acc\", \"F1\", \"Kappa\"]})\n    s2 = pd.DataFrame({\"crit\": [\"acc\", \"bal_acc\", \"F1\", \"Kappa\"]})\n    s3 = pd.DataFrame({\"crit\": [\"acc\", \"bal_acc\", \"F1\", \"Kappa\"]})\n    print(\"--- donnees 1 ---\")\n    s1 = s1.assign(brut=0.0)\n    s1.iloc[0,1] = accuracy_score(y1_valid, P1)\n    s1.iloc[1,1] = balanced_accuracy_score(y1_valid, P1)\n    s1.iloc[2,1] = f1_score(y1_valid, P1)\n    s1.iloc[3,1] = cohen_kappa_score(y1_valid, P1)\n    print(s1)\n    print(\"--- donnees 2 ---\")\n    s2 = s2.assign(brut=0.0)\n    s2.iloc[0,1] = accuracy_score(y2_valid, P2)\n    s2.iloc[1,1] = balanced_accuracy_score(y2_valid, P2)\n    s2.iloc[2,1] = f1_score(y2_valid, P2)\n    s2.iloc[3,1] = cohen_kappa_score(y2_valid, P2)\n    print(s2)\n    print(\"--- donnees 3 ---\")\n    s3 = s3.assign(brut=0.0)\n    s3.iloc[0,1] = accuracy_score(y3_valid, P3)\n    s3.iloc[1,1] = balanced_accuracy_score(y3_valid, P3)\n    s3.iloc[2,1] = f1_score(y3_valid, P3)\n    s3.iloc[3,1] = cohen_kappa_score(y3_valid, P3)\n    print(s3)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    --- donnees 1 ---\n          crit      brut\n    0      acc  0.657576\n    1  bal_acc  0.655825\n    2       F1  0.622074\n    3    Kappa  0.309572\n    --- donnees 2 ---\n          crit      brut\n    0      acc  0.724242\n    1  bal_acc  0.740841\n    2       F1  0.637450\n    3    Kappa  0.427280\n    --- donnees 3 ---\n          crit      brut\n    0      acc  0.693939\n    1  bal_acc  0.729286\n    2       F1  0.435754\n    3    Kappa  0.278103\n    ```\n    :::\n    :::\n    \n    \n4.  \n\n\n    ::: {#9453a93e .cell execution_count=18}\n    ``` {.python .cell-code}\n    from imblearn.under_sampling import RandomUnderSampler\n    from imblearn.under_sampling import TomekLinks\n    from imblearn.over_sampling import RandomOverSampler\n    from imblearn.over_sampling import SMOTE\n    ## RandomOverSampler\n    ros3 = RandomOverSampler(random_state=123)\n    X3_app_reech, y3_app_reech = ros3.fit_resample(X3_app, y3_app)\n    mod3_ros = LogisticRegression(penalty=None, solver=\"newton-cholesky\").fit(X3_app_reech, y3_app_reech)\n    ## Smote\n    sm = RandomOverSampler(random_state=123)\n    X3_app_reech, y3_app_reech = sm.fit_resample(X3_app, y3_app)\n    mod3_sm = LogisticRegression(penalty=None, solver=\"newton-cholesky\").fit(X3_app_reech, y3_app_reech)\n    ## RandomUnderSampler\n    rus3 = RandomUnderSampler(random_state=123)\n    X3_app_reech, y3_app_reech = rus3.fit_resample(X3_app, y3_app)\n    mod3_rus = LogisticRegression(penalty=None, solver=\"newton-cholesky\").fit(X3_app_reech, y3_app_reech)\n    ## Tomek\n    tl = TomekLinks(sampling_strategy='all')\n    X3_app_reech, y3_app_reech = tl.fit_resample(X3_app, y3_app)\n    mod3_tl = LogisticRegression(penalty=None, solver=\"newton-cholesky\").fit(X3_app_reech, y3_app_reech)\n    ```\n    :::\n    \n    \n\n    ::: {#bf9daa34 .cell execution_count=19}\n    ``` {.python .cell-code}\n    P3_ros = mod3_ros.predict(X3_valid)\n    P3_sm = mod3_sm.predict(X3_valid)\n    P3_rus = mod3_rus.predict(X3_valid)\n    P3_tl = mod3_tl.predict(X3_valid)\n    ```\n    :::\n    \n    \n\n    ::: {#4ab3718e .cell execution_count=20}\n    ``` {.python .cell-code}\n    s3 = s3.assign(ros=[accuracy_score(y3_valid, P3_ros),\n    balanced_accuracy_score(y3_valid, P3_ros),\n    f1_score(y3_valid, P3_ros),\n    cohen_kappa_score(y3_valid, P3_ros)])\n    s3 = s3.assign(sm=[accuracy_score(y3_valid, P3_sm),\n    balanced_accuracy_score(y3_valid, P3_sm),\n    f1_score(y3_valid, P3_sm),\n    cohen_kappa_score(y3_valid, P3_sm)])\n    s3 = s3.assign(rus=[accuracy_score(y3_valid, P3_rus),\n    balanced_accuracy_score(y3_valid, P3_rus),\n    f1_score(y3_valid, P3_rus),\n    cohen_kappa_score(y3_valid, P3_rus)])\n    s3 = s3.assign(tl=[accuracy_score(y3_valid, P3_tl),\n    balanced_accuracy_score(y3_valid, P3_tl),\n    f1_score(y3_valid, P3_tl),\n    cohen_kappa_score(y3_valid, P3_tl)])\n    print(s3)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n          crit      brut       ros        sm       rus        tl\n    0      acc  0.693939  0.603030  0.603030  0.612121  0.854545\n    1  bal_acc  0.729286  0.683929  0.683929  0.689286  0.520000\n    2       F1  0.435754  0.379147  0.379147  0.384615  0.076923\n    3    Kappa  0.278103  0.192415  0.192415  0.200606  0.066038\n    ```\n    :::\n    :::\n    \n    \n:::\n\n",
    "supporting": [
      "chap16_files"
    ],
    "filters": [],
    "includes": {}
  }
}