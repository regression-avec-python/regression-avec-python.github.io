{
  "hash": "979d62009989156728acd62950838790",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"8 Choix de variables\"\ntoc: true\n---\n\n\n\n\n::: {.content-hidden}\n\n\\DeclareMathOperator{\\C}{Cov}\n\\DeclareMathOperator{\\V}{V}\n\\DeclareMathOperator{\\leR}{R^2}\n\\newcommand{\\M}{\\mathcal M}\n\\DeclareMathOperator{\\MCO}{MCO}\n\\DeclareMathOperator{\\SC}{SC}\n\\DeclareMathOperator{\\SCR}{SCR}\n\\DeclareMathOperator{\\SCT}{SCT}\n\\DeclareMathOperator{\\SCE}{SCE}\n\\DeclareMathOperator{\\EQM}{EQM}\n\\DeclareMathOperator{\\ro}{R^2_0}\n\\DeclareMathOperator{\\CMR}{CMR}\n\\DeclareMathOperator{\\CME}{CME}\n\\DeclareMathOperator{\\radeux}{R^2_a}\n\\DeclareMathOperator{\\Cp}{C_p}\n\n\n\\newcommand{\\1}{\\mathbf{1}}\n\\newcommand{\\un}{\\mathbf{1}}\n\\newcommand{\\D}{\\displaystyle}\n\n\n\\newcommand{\\prob}{\\mathbf P}\n\\newcommand{\\argmin}{\\mathop{\\mathrm{argmin}}}\n\\newcommand{\\ind}{\\mathbf 1}\n\\newcommand{\\R}{\\mathbb R}\n\\newcommand{\\E}{\\mathbf E}\n\\newcommand{\\var}{\\mathbf V}\n\\newcommand{\\ps}[2]{\\langle #1,#2\\rangle}\n\\newcommand{\\card}[1]{|{#1}|}\n\\newcommand{\\cov}{\\mathbf{cov}}\n\\newcommand{\\corr}{\\text{corr}}\n\\newcommand{\\AUC}{\\text{AUC}}\n\\newcommand{\\logit}{\\text{logit}}\n\\newcommand{\\diag}{\\text{diag}}\n\\newcommand{\\tr}{\\text{tr}}\n\\newcommand{\\li}[2]{#1_{#2}}\n\\newcommand{\\ssli}[2]{#1_{(#2)}}\n\\newcommand{\\HH}{{\\mathcal{H}}}\n\\newcommand{\\Hz}{{\\mathrm{H_0}}}\n\\newcommand{\\sfrac}[2]{{#1}/{#2}}\n\n\n\n:::\n\n::: {#exr-8-1 name=\"Questions de cours\"}\nA, C, B en général. Un cas particulier de la dernière question est le suivant : si les variables sélectionnées $\\xi$ engendrent un sous-espace orthogonal au sous-espace engendré par les variables non sélectionnées $\\bar\\xi$, alors C est la bonne réponse.\n:::\n\n::: {#exr-8-2 name=\"Analyse du biais\"}\nLa preuve des deux premiers points s'effectue comme l'exemple de\nla section 7.2.1. Nous ne détaillerons que le premier point.\nSupposons que $|\\xi|$ soit plus petit que $p$, le \"vrai\" nombre de\nvariables entrant dans le modèle. Nous avons pour estimateur de $\\beta$\n$$\n\\hat \\beta_{\\xi} = (X_{\\xi}'X_{\\xi})^{-1}X_{\\xi}'Y = P_{X_{\\xi}}Y.\n$$\nLe vrai modèle étant obtenu avec $p$ variables, $\\E(Y)=X_p \\beta$.\nNous avons alors\n$$\n\\begin{eqnarray*}\n\\E(\\hat \\beta_{\\xi})&=&P_{X_{\\xi}}X_p \\beta\\\\\n&=& P_{X_{\\xi}}X_{\\xi} \\beta_{\\xi} +P_{X_{\\xi}}X_{\\bar \\xi} \\beta_{\\bar \\xi}.\n\\end{eqnarray*}\n$$\nCette dernière quantité n'est pas nulle sauf si\n$\\Im(X_{\\xi}) \\perp \\Im(X_{\\bar \\xi})$. Comme $\\hat \\beta_{\\xi}$\nest en général biaisé, il en est de même pour la valeur prévue\n$\\hat y_{\\xi}$ dont l'espérance ne vaudra pas $X\\beta$.\n:::\n\n::: {#exr-8-3 name=\"Variance des estimateurs\"}\nL'estimateur obtenu avec les $|\\xi|$ variables est noté $\\hat \\beta_{\\xi}$\net l'estimateur obtenu dans le modèle complet $\\hat \\beta$. Ces vecteurs\nne sont pas de même taille, le premier est de longueur $|\\xi|$, le second\nde longueur $p$. Nous comparons les $|\\xi|$ composantes communes, c'est-à-dire\nque nous comparons $\\hat \\beta_{\\xi}$ et $[\\hat \\beta]_{\\xi}$.\nPartitionnons la matrice $X$ en $X_{\\xi}$ et $X_{\\bar \\xi}$.\nNous avons alors\n$$\n\\begin{eqnarray*}\n\\V(\\hat \\beta) &=& \\sigma^2 \\left(\n\\begin{array}{cc}\nX'_{\\xi}X_{\\xi} &X'_{\\xi}X_{\\bar \\xi}\\\\\nX'_{\\bar \\xi}X_{\\xi} &X'_{\\bar \\xi}X_{\\bar \\xi}\n\\end{array}\n\\right)^{-1}.\n\\end{eqnarray*}\n$$\nEn utilisant la formule d'inverse par bloc, donnée en annexe A,\nnous obtenons\n$$\n\\begin{eqnarray*}\n\\V([\\hat \\beta]_{\\xi}) &=& \\sigma^2 \\left[X_{\\xi}'X_{\\xi}-X_{\\xi}'X_{\\bar \\xi}(X_{\\bar \\xi}'\nX_{\\bar \\xi})^{-1}X_{\\bar \\xi}'X_{\\xi}\\right]^{-1},\n\\end{eqnarray*}\n$$\nalors que la variance de $\\hat \\beta_{\\xi}$ vaut\n$$\n\\begin{eqnarray*}\n\\V(\\hat \\beta_{\\xi}) &=& \\sigma^2 \\left[X_{\\xi}'X_{\\xi}\\right]^{-1}.\n\\end{eqnarray*}\n$$\nNous devons comparer $\\V([\\hat \\beta]_{\\xi})$ et $\\V(\\hat \\beta_{\\xi})$.\nNous avons\n$$\n\\begin{eqnarray*}\nX_{\\xi}'X_{\\xi}-X_{\\xi}'X_{\\bar \\xi}(X_{\\bar \\xi}'\nX_{\\bar \\xi})^{-1}X_{\\bar \\xi}'X_{\\xi}=X_{\\xi}'(I-P_{X_{\\bar \\xi}})X_{\\xi}\n=X_{\\xi}'P_{X_{\\bar \\xi}^\\perp}X_{\\xi}.\n\\end{eqnarray*}\n$$\nLa matrice $P_{X_{\\bar \\xi}^\\perp}$ est la matrice d'un projecteur, alors\nelle est semi-définie positive (SDP) (cf. annexe A), donc\n$X_{\\xi}'P_{X_{\\bar \\xi}^\\perp}X_{\\xi}$ est également SDP. La\nmatrice $X_{\\xi}'P_{X_{\\bar \\xi}^\\perp}X_{\\xi}-X_{\\xi}'P_{X_{\\bar \\xi}^\\perp}X_{\\xi}$ est définie positive (DP) puisque c'est $\\V([\\hat \\beta]_{\\xi})/ \\sigma^2$.\nUtilisons le changement de notation suivant :\n$$\n\\begin{eqnarray*}\nA=X_{\\xi}'X_{\\xi}-X_{\\xi}'P_{X_{\\bar \\xi}}X_{\\xi} \\quad \\hbox{et} \\quad\nB=X_{\\xi}'P_{X_{\\bar \\xi}}X_{\\xi}.\n\\end{eqnarray*}\n$$\nLa matrice $A$ est DP et la matrice $B$ SDP. La propriété donnée en\nannexe A indique que $A^{-1}-(A+B)^{-1}$ est SDP, or\n$$\n\\begin{eqnarray*}\n\\V([\\hat \\beta]_{\\xi})-\\V(\\hat \\beta_{\\xi}) = \\sigma^2 (A^{-1}-(A+B)^{-1}).\n\\end{eqnarray*}\n$$\nDonc la quantité $\\V([\\hat \\beta]_{\\xi})-\\V(\\hat \\beta_{\\xi})$ est SDP.\nLe résultat est démontré. L'estimation, en terme de variance, de $\\xi$\ncomposantes est plus précise que les mêmes $\\xi$ composantes extraites\nd'une estimation obtenue avec $p$ composantes.\n\nLa variance des valeurs ajustées dépend de la variance de $\\hat \\beta$,\nle point 2 de la proposition se démontre de façon similaire.\n\n**Remarque** : nous venons de comparer deux estimateurs de même taille\n*via* leur matrice de variance. Pour cela, nous montrons que la différence\nde ces deux matrices est une matrice SDP. Que pouvons-nous dire alors\nsur la variance de chacune des coordonnées ? Plus précisément, pour\nsimplifier les notations, notons le premier estimateur (de taille $p$)\n$\\tilde \\beta$ de\nvariance $\\V(\\tilde \\beta)$ et le second estimateur $\\hat \\beta$ de\nvariance $\\V(\\hat \\beta)$. Si $\\V(\\tilde \\beta)-\\V(\\hat \\beta)$ est SDP,\npouvons-nous dire que $\\V(\\tilde \\beta_i)-\\V(\\hat \\beta_i)$ est un\nnombre positif pour $i$ variant de $1$ à $p$ ? Considérons par exemple\nle vecteur $u_1'=(1,0,\\dotsc,0)$ de $\\R^p$. Nous avons alors\n$$\nu_1' \\hat \\beta = \\hat \\beta_1 \\quad \\hbox{et}\n\\quad u_1' \\tilde \\beta = \\tilde \\beta_1.\n$$\nComme $\\V(\\tilde \\beta)-\\V(\\hat \\beta)$ est SDP, nous avons pour\ntout vecteur $u$ de $\\R^p$ que $u'(\\V(\\tilde \\beta)-\\V(\\hat \\beta))u\\geq 0$,\nc'est donc vrai en particulier pour $u_1$. Nous avons donc\n$$\n\\begin{eqnarray*}\nu_1'(\\V(\\tilde \\beta)-\\V(\\hat \\beta))u_1&\\geq& 0\\\\\nu_1'\\V(\\tilde \\beta)u_1-u_1'\\V(\\hat \\beta)u_1&\\geq& 0\\\\\n\\V(u_1'\\tilde \\beta)-\\V(u_1'\\hat \\beta)&\\geq& 0\\\\\n\\V(\\tilde \\beta_1) &\\geq & \\V(\\hat \\beta_1).\n\\end{eqnarray*}\n$$\nNous pouvons retrouver ce résultat pour les autres coordonnées\ndes vecteurs estimés ou encore pour des combinaisons linéaires\nquelconques de ces coordonnées.\n:::\n\n::: {#exr-8-4 name=\"Choix de variables\"}\n\nTous les modèles possibles ont été étudiés, la recherche est donc exhaustive.\nEn prenant comme critère l'AIC ou le BIC, le modèle retenu est le modèle M134.\nComme prévu, le $\\leR$ indique le modèle conservant toutes les variables. Cependant\nle $\\leR$ peut être utilisé pour tester des modèles emboîtés. Dans ce cas, le\nmodèle retenu est également le M134.\n\nPour une procédure avec test nous devons démarrer d'un modèle.\nDémarrons par exemple du modèle avec uniquement la constante. Nous\najoutons une variable après l'autre. Nous ajoutons celle avec la\nstatistique de test la plus élevée et cette valeur doit être plus\ngrande que 2.3 (sinon aucune variable n'est ajoutée et le modèle courant\nest conservé). Dans les modèles à une variable, c'est le modèle M1 qui\nest choisi (statistique la plus élevée de 41.9 et supérieure à 2.3).\nEnsuite nous ajoutons à M1 une variable (2 ou 3 ou 4) et la meilleure est\nla 4 mais la statistique est de 0.9 (< 2.3) on n'ajoute pas de variable\net on choisit M1.\n\nDémarrons du modèle complet 1234, on enlève la moins significative (la\nvaleur de la statistique de test la plus faible en dehors de la\nconstante et qui doit être plus faible que 2.3). Ici nous enlevons la\nvariable 2 et nous avons donc le modèle M134. Dans ce modèle la\nvariable la moins significative est la 3 mais sa statistique est plus\ngrande que 2.3, on conserve donc le modèle M134.\n:::\n\n::: {#exr-8-5 name=\"Utilisation du $R^2$\"}\nPlaçons nous dans le cas pratique où la constante fait partie des modèles, elle est donc dans une des colonnes de $Z$ (par exemple la première). Le $\\leR$ est par définition\n$$\n\\begin{align*}\n\\leR(Z)&=\\frac{\\|P_{Z}Y - P_{\\un}Y\\|^{2}}{\\|Y - P_{\\un}Y\\|^{2}}\\\\\n\\leR(X)&=\\frac{\\|P_{X}Y - P_{\\un}Y\\|^{2}}{\\|Y - P_{\\un}Y\\|^{2}}\n\\end{align*}\n$$\nComme $\\Im(Z)\\subset \\Im(X)$ on peut décomposer en deux $\\Im(X)$: la partie $\\Im(Z)$ puis le reste ce qui se note\n$$\n\\Im(X)=\\Im(Z) \\stackrel{\\perp}{\\oplus} (\\Im(X)\\cap \\Im(Z)^{\\perp})\n$$\nAu niveau des projecteurs on a donc\n$$\nP_{X}=P_{Z} + P_{X\\cap Z^{\\perp}}\n$$\nce qui permet d'écrire le $\\leR$ du modèle avec $X$ comme suit puis par pythagore ($(P_{Z}Y - P_{\\un}Y)\\in \\Im(Z)$ et $P_{X\\cap Z^{\\perp}}Y\\in \\Im(Z)^{\\perp}$)\n$$\n\\begin{align*}\n\\leR(X)&=\\frac{\\|P_{Z}Y + P_{X\\cap Z^{\\perp}}Y - P_{\\un}Y\\|^{2}}{\\|Y - P_{\\un}Y\\|^{2}} \\\\\n&= \\frac{\\|(P_{Z}Y - P_{\\un}Y) + P_{X\\cap Z^{\\perp}}Y \\|^{2}}{\\|Y - P_{\\un}Y\\|^{2}}\\\\\n&=\\frac{\\|(P_{Z}Y - P_{\\un}Y) \\|^{2}}{\\|Y - P_{\\un}Y\\|^{2}} + \\frac{\\|P_{X\\cap Z^{\\perp}}Y \\|^{2}}{\\|Y - P_{\\un}Y\\|^{2}}= \\leR2(Z) + \\frac{\\|P_{X\\cap Z^{\\perp}}Y \\|^{2}}{\\|Y - P_{\\un}Y\\|^{2}}\n\\end{align*}\n$$\nComme la seconde partie est positive ou nulle on a que $\\leR(X)$ est au moins aussi grand que $\\leR(Z)$. Il y a égalité dans le cas rare où $Y\\perp (\\Im(X)\\cap \\Im(Z)^{\\perp})$ c'est à dire que l'on a ajouté des variables qui ne sont pas entièrement reliées aux variables de $Z$ mais dont la partie non redondante avec celle de $Z$ (qui existe car le rang de $X$ est $p$) a une corrélation empirique avec $Y$ de 0 exactement. Comme le $\\leR$ augmente en ajoutant des variables le modèle sélectionné sera celui avec toutes les variables.\n\nDonc si nous cherchons à expliquer la concentration d'ozone à Rennes et si nous avons en même temps la consommation de nouille au Viêt Nam les mêmes jours, le $\\leR$ nous conduira à sélectionner aussi  la consommation de nouille au Viet-Nam comme variable explicative, variable dont on sent le peu d'influence sur la concentration d'ozone.\n:::\n\n::: {#exr-8-6 name=\"Cas orthogonal\"}\n1.  Les variables sont orthogonales donc on a $X'X=I_{p}$ et l'estimateur des MCO s'écrit\n$$\n\\hat \\beta=(X'X)^{-1}X'Y=X'Y\n$$\n    En remplaçant $Y$ par le modèle ($Y=X\\beta + \\varepsilon$) on a\n$$\n\\hat \\beta=X'X\\beta + X'\\varepsilon = \\beta + X'\\varepsilon.\n$$\n\n2.  La somme des résidus vaut ici\n$$\n\\begin{align*}\n\\SCR&=\\|Y-X\\hat\\beta\\|^{2}=(Y-X\\hat\\beta)'(Y-X\\hat\\beta)= Y'Y - 2Y'X\\hat\\beta + \\hat\\beta'X'X\\hat\\beta\\\\\n&= \\sum_{i=1}^n y_{i}^2- 2Y'X\\hat\\beta + \\hat\\beta'X'X\\hat\\beta.\n\\end{align*}\n$$\n    Évaluons le second terme du membre de droite: comme $X\\hat\\beta=P_{X}Y$ et en utilisant le  fait que $Y=P_{X}Y + P_{X^{\\perp}}Y$ on obtient\n$$\nY'X\\hat\\beta= (P_{X}Y + P_{X^{\\perp}}Y)'P_{X}Y= Y' P_{X}' P_{X}Y = \\hat\\beta'X'X\\hat \\beta\n$$\n    car $P_{X}Y$ et $P_{X^{\\perp}}Y$ sont orthogonaux et leur produit scalaire $Y'P_{X^{\\perp}}'P_{X}Y$ vaut 0. On a donc\n$$\n\\SCR=\\sum_{i=1}^n y_{i}^2 - \\hat\\beta'X'X\\hat \\beta\n$$\n    Comme ici la matrice $X$ est orthogonale on a $X'X=I_{p}$ et l'expression devient\n$$\n\\SCR=\\sum_{i=1}^n y_{i}^2 - \\sum_{j=1}^p \\hat \\beta_{j}\n$${#eq-scr-ortho}\n    La somme du carré des résidus est d'autant plus faible que les coefficients estimés sont grands en valeur absolue.\n\n3.  Prenons un modèle $\\xi$ qui regroupe les $k$ premières variables (ce qui est pratique pour les notations). Comme la matrice $X$ est orthogonale on a au niveau des sous espaces que toutes les variables engendrent des sous-espaces vectoriels orthogonaux que l'on peut regrouper:\n$$\n\\begin{align*}\n\\Im(X) &= \\Im(X_{1})\\stackrel{\\perp}{\\oplus} \\Im(X_{2})\\stackrel{\\perp}{\\oplus} \\cdots \\stackrel{\\perp}{\\oplus}\\Im(X_{p})\\\\\n&= \\Im(X_{\\xi})\\stackrel{\\perp}{\\oplus}\\Im(X_{\\bar\\xi})\n\\end{align*}\n$$\n    Ce qui donne au niveau des projecteurs\n$$\nP_{X}= P_{X_{\\xi}} + P_{X_{\\bar\\xi}}\n$$\n    Écrivons par ailleurs l'ajustement:\n$$\n\\begin{align*}\nP_{X}Y&= X\\hat \\beta= X_{1}\\hat \\beta_{1} + X_{2}\\hat \\beta_{2} + \\cdots +X_{p}\\hat \\beta_{p}\\\\\nP_{X_{\\xi}}Y + P_{X_{\\bar\\xi}}Y&= X_{1}\\hat \\beta_{1} + X_{2}\\hat \\beta_{2} + \\cdots +X_{p}\\hat \\beta_{p}\n\\end{align*}\n$$\n    Si je projette sur $\\Im(X_{\\xi})$ l'équation ci-dessus cela nous donne ($\\Im(X_{\\bar\\xi})\\subset \\Im(X_{\\xi})^{\\perp}$)\n$$\nP_{X_{\\xi}}Y= X_{1}\\hat \\beta_{1}  + \\cdots +X_{k}\\hat \\beta_{k}\n$$\n    Or nous savons que les MCO dans le modèle $\\xi$ donne l'ajustement $P_{X_{\\xi}}Y=X_{\\xi}\\hat\\beta_{\\xi}$ qui est unique, ce qui donne en identifiant les deux écritures que\n$$\nX_{1}\\hat \\beta_{1}  + \\cdots +X_{k}\\hat \\beta_{k}= X_{\\xi}\n$$\n    Comme une base de $\\Im(X_{\\bar\\xi})$ est donnée par les colonnes orthonormées de $X_{\\xi}$ on en déduit que les coefficients dans la base sont uniques d'où\n$$\n[\\hat \\beta]_{\\xi}= \\hat\\beta_{\\xi}.\n$$\n4.  Les critères AIC et BIC valent pour le modèle $\\xi$\n$$\n-2\\mathcal{L} + 2 |\\xi + 1| f(n)\n$$\n    et quand on compare le modèle $\\xi$ et le modèle $\\{\\xi, l\\}$ on compare donc\n$$\n-2\\mathcal{L}(\\xi) + 2 |\\xi + 1| f(n) \\ \\ \\mathrm{et} \\ \\ -2\\mathcal{L}(\\{\\xi, l\\}) + 2 |\\xi + 2| f(n)\n$$\n    Comme $\\mathcal{L}(\\xi)$ vaut $-\\sfrac{n}{2}\\log\\sigma^{2} -\\sfrac{n}{2}-\\sfrac{1}{2\\sigma^{2}}.\\SCR(\\xi)$ lorsque l'on compare on peut éliminer les termes identiques et il reste donc\n$$\n\\frac{1}{\\sigma^{2}}\\SCR(\\xi) \\ \\ \\mathrm{et} \\ \\  \\frac{1}{\\sigma^{2}}\\SCR(\\{\\xi, l\\}) + 2  f(n)\n$$\n    Effectuons la différence:\n$$\n\\Delta =  \\frac{1}{\\sigma^{2}}(\\SCR(\\{\\xi, l\\}) - \\SCR(\\xi)) + 2  f(n)\n$$\n    et en utilisant @eq-scr-ortho et la question 3 on obtient\n$$\n\\begin{align*}\n\\Delta &= \\frac{1}{\\sigma^{2}}(\\sum_{i=1}^n y_{i}^2 - \\sum_{\\xi} \\hat \\beta_{j}^2 - \\hat \\beta_{l}^2 - \\sum_{i=1}^n y_{i}^2 + \\sum_{\\xi} \\hat \\beta_{j}^2)+ 2  f(n)\\\\\n&= -\\frac{\\hat \\beta_{l}^2}{\\sigma^{2}} + 2f(n)\n\\end{align*}\n$$\n    Quand $\\Delta$ est négatif l'AIC (ou le BIC) du modèle $\\{\\xi, l\\}$ est plus faible que l'AIC (ou le BIC) que celui du modèle $\\xi$, c'est à dire que l'AIC (ou le BIC) du modèle $\\{\\xi, l\\}$ est meilleur: on ajoute la variable $l$. Cela donne donc\n$$\n\\begin{align*}\n\\Delta & < 0\\\\\n-\\frac{\\hat \\beta_{l}^2}{\\sigma^{2}} + 2f(n) & < 0\\\\\n\\frac{\\hat \\beta_{l}^2}{\\sigma^{2}}&> 2f(n)\n\\end{align*}\n$$\n\n5.  En écrivant que\n$$\nN=\\frac{\\SCR(\\xi) - \\SCR(\\{\\xi, l\\})}{\\sigma^{2}}\n$$\n    nous voyons qu'il faut évaluer la différence des SCR. En utilisant @eq-scr-ortho et la question 3 on a\n$$\n\\begin{align*}\n\\SCR(\\xi) - \\SCR(\\{\\xi, l\\})&=\\sum_{i=1}^n y_{i}^2  -  \\sum_{\\xi} \\hat \\beta_{j}^2 - \\sum_{i=1}^n y_{i}^2 + \\sum_{\\xi} \\hat \\beta_{j}^2 + \\hat \\beta_{l}^2  \\\\\n&= \\hat \\beta_{l}^2\n\\end{align*}\n$$\n    Nous en déduisons la valeur de $N$ (la dernière égalité découle du calcul de $\\hat \\beta$ en question 1)\n$$\nN=\\frac{\\hat\\beta_{l}^{2}}{\\sigma^{2}}= \\frac{(\\beta_{l} + [X' \\varepsilon]_{l})^{2}}{\\sigma^{2}}\n$$\n    Le dernier membre nous indique que la partie aléatoire est $[X' \\varepsilon]_{l}$. D'après $\\HH_{3}$ on a que $X' \\varepsilon$ est gaussien d'espérance $\\E(X'\\varepsilon)=X'\\E(\\varepsilon)=0$ et de variance $\\V(X'\\varepsilon)=X'\\V(\\varepsilon)X=\\sigma^{2}I_{p}$. Sa coordonnée $l$ notée $[X' \\varepsilon]_{l}$ est donc une loi normale $N(0, \\sigma^{2})$ et on en déduit que\n$$\n\\frac{\\hat \\beta_{l}}{\\sigma} \\sim N(\\frac{\\beta_{l}}{\\sigma}, 1).\n$$\n\n    Le carré de cette loi normale (noté $N$) est donc un $\\chi^{2}(1)$ décentré de paramètre de décentrage $\\sfrac{\\beta_{l}^{2}}{\\sigma^{2}}$. On connait donc la loi de la statistique de test $N$ (un $\\chi^{2}(1)$ décentré de paramètre de décentrage $\\beta_{l}^{2}$). Quand $\\Hz$ est vrai, c'est-à-dire le modèle $\\xi$ est valide, il n'y a pas la variable $l$ dans le modèle, donc $\\beta_{l}=0$ et le paramètre de décentrage vaut $0$. Le seuil de rejet basé sur la statistique $N$ est son quantile de niveau 0.95 sous $\\Hz$' c'est à dire celui d'un $\\chi^{2}(1)$ à 0.95 :\n\n\n    ::: {#230bd912 .cell execution_count=2}\n    ``` {.python .cell-code}\n    from scipy.stats import chi2\n    print(chi2.ppf(0.95, df=1))\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    3.841458820694124\n    ```\n    :::\n    :::\n    \n    \n6.  Par définition le $\\radeux$ vaut\n$$\n\\radeux(\\xi) =1-\\frac{n-1}{\\SCT}\\frac{\\SCR(\\xi)}{n-|\\xi|}\n$$\n    Calculons la différence des $\\radeux$\n$$\n\\Delta\\radeux = \\frac{n-1}{\\SCT}(\\frac{\\SCR(\\xi)}{n-|\\xi|} - \\frac{\\SCR(\\{\\xi, l\\})}{n-|\\xi| -1})\n$$\n    Cette différence est positive (on ajoute la variable $l$) si\n$$\n\\begin{align*}\n\\frac{\\SCR(\\xi)}{n-|\\xi|} - \\frac{\\SCR(\\{\\xi, l\\})}{n-|\\xi| -1}&>0\\\\\n\\frac{\\SCR(\\xi)}{n-|\\xi|}- \\frac{\\SCR(\\xi) -\\hat\\beta^{2}_{l} }{n-|\\xi| -1}&>0\\\\\n\\frac{\\SCR(\\xi)}{n-|\\xi|} - \\frac{\\SCR(\\xi)}{n-|\\xi|}.\\frac{n-|\\xi|}{n-|\\xi|-1}+ \\frac{\\hat\\beta^{2}_{l}}{n-|\\xi| -1} &>0\\\\\n\\frac{\\SCR(\\xi)}{n-|\\xi|}\\frac{-1}{n-|\\xi| -1}+ \\frac{\\hat\\beta^{2}_{l}}{n-|\\xi| -1}&>0\n\\end{align*}\n$$\n      En utilisant l'approximation on a donc approximativement\n$$\n\\begin{align*}\n   \\frac{1}{n-|\\xi| -1}(\\hat\\beta^{2}_{l}- \\sigma^{2})  &>0\\\\\n  \\frac{\\sigma^{2}}{n-|\\xi| -1} (\\frac{\\hat\\beta^{2}_{l}}{\\sigma^{2}}- 1)&>0\\\\\n \\end{align*}\n$$\n    Le premier terme étant positif on retrouve que\n$$\n\\begin{align*}\n\\frac{\\hat\\beta^{2}_{l}}{\\sigma^{2}}- 1&>0\\\\\n\\frac{\\hat\\beta^{2}_{l}}{\\sigma^{2}}&>1.\n\\end{align*}\n$$\n\n7.  Comme\n$$\n\\Cp(\\xi)=\\frac{\\SCR(\\xi)}{\\sigma^2}-n+2|\\xi|.\n$$\n    on fait là encore la différence des $\\Cp$\n$$\n\\begin{align*}\n\\Delta\\Cp &= \\frac{\\SCR(\\{\\xi, l\\})-\\SCR(\\xi)}{\\sigma^2}+2 =\n\\frac{\\SCR(\\xi) -\\hat\\beta^{2}_{l} -\\SCR(\\xi)}{\\sigma^2} +2\\\\\n&=-\\frac{\\hat\\beta^{2}_{l}}{\\sigma^2} +2\n\\end{align*}\n$$\n    Cette différence est négative (on ajoute la variable $l$) si\n$$\n-\\frac{\\hat\\beta^{2}_{l}}{\\sigma^2} +2<0 \\quad\\Longleftrightarrow\\quad\n\\frac{\\hat\\beta^{2}_{l}}{\\sigma^2}>2.\n$$\n\n9.  D'après la question 3 nous n'avons besoin d'estimer qu'une fois le modèle complet et les coordonnées donnent les estimateurs dans les modèles restreints.\n\n    **Algorithme** :\n    1.    Estimer $\\beta$ dans le modèle complet $\\hat \\beta = X'Y$  .\n    2.    Déduire $\\hat\\sigma^{2}$ dans le modèle complet $\\hat\\sigma^{2}=\\|Y - X\\hat \\beta\\|^{2}/(n-p)$  .\n    3.    Ordonner les coordonnées dans l'ordre décroissant:\n$$\n\\hat \\beta_{{(1)}} \\geq \\hat \\beta_{{(2)}}\\geq \\cdots \\geq \\hat \\beta_{{(p)}}.\n$$\n          Les colonnes correspondantes seront notées $(k)$.\n    4.    Pour $k=1$ à $p$\n    \n          -   Si $\\frac{\\hat\\beta^{2}_{(k)}}{\\hat\\sigma^2}$ > Seuil alors ajout  la variable $(k)$\n          -   Sinon Sortie de la boucle, plus de variable à ajouter\n          \n          Le seuil vaut $2f(n)$ pour l'AIC et le BIC, il vaut 3.84 pour un test, 1 pour le $\\radeux$ et 2 pour le $\\Cp$.\n\n:::\n\n",
    "supporting": [
      "chap8_files"
    ],
    "filters": [],
    "includes": {}
  }
}