{
  "hash": "ed3635d16b7ffeb9e7954402bbef5d2e",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"12 Régression logistique\"\ntoc: true\n---\n\n::: {#7f572ac0 .cell execution_count=1}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\nimport statsmodels.regression.linear_model as smlm\nfrom patsy import dmatrix\nfrom scipy.stats import norm, chi2\nimport sys\nsys.path.append('../modules')\nimport choixglmstats\n```\n:::\n\n\n#   Présentation du modèle\n\n::: {#a2389a78 .cell execution_count=2}\n``` {.python .cell-code}\nartere = pd.read_csv('../donnees/artere.txt', header=0, index_col=0, sep=' ')\n```\n:::\n\n\n::: {#862a9f0b .cell execution_count=3}\n``` {.python .cell-code}\nfig = plt.figure()\nplt.plot(artere.age, artere.chd, 'o')\nplt.ylabel('chd')\nplt.xlabel('age')\nfig.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![](chap12_files/figure-html/cell-4-output-1.png){width=661 height=468}\n:::\n:::\n\n\n::: {#8601324f .cell execution_count=4}\n``` {.python .cell-code}\nartere_summary = pd.crosstab(artere.agrp, artere.chd)\nartere_summary.columns = ['chd0', 'chd1']\nartere_summary['Effectifs'] = artere_summary.chd0 + artere_summary.chd1\nartere_summary['Frequence'] = artere_summary.chd1 / artere_summary.Effectifs\nartere_summary['age_min'] = [artere[artere.agrp == agrp].age.min() - 1 for agrp in artere_summary.index]\nartere_summary['age_min'] = artere.groupby([\"agrp\"]).age.min() +1\nartere_summary['age_max'] = [artere[artere.agrp == agrp].age.max() for agrp in artere_summary.index]\nartere_summary['age_max'] = artere.groupby([\"agrp\"]).age.max()\nartere_summary['age'] = [f']{artere_summary.age_min[i]};{artere_summary.age_max[i]}]' for i in artere_summary.index]\nartere_summary['age'] = \"]\" + artere_summary.age_min.astype(str) + \";\" + artere_summary.age_max.astype(str) + \"]\"\nartere_summary.reset_index()[['age', 'Effectifs', 'chd0', 'chd1', 'Frequence']].to_string(index=False)\n\n\nfig = plt.figure()\nplt.plot(artere.age, artere.chd, 'o')\nplt.ylabel('chd')\nplt.xlabel('age')\nplt.hlines(artere_summary.Frequence,artere_summary.age_min, artere_summary.age_max, 'k')\nfig.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![](chap12_files/figure-html/cell-5-output-1.png){width=661 height=468}\n:::\n:::\n\n\n::: {#d3e57cac .cell execution_count=5}\n``` {.python .cell-code}\nmodele = smf.glm('chd~age', data=artere, family=sm.families.Binomial()).fit()\nprint(modele.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                    chd   No. Observations:                  100\nModel:                            GLM   Df Residuals:                       98\nModel Family:                Binomial   Df Model:                            1\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -53.677\nDate:                Tue, 04 Feb 2025   Deviance:                       107.35\nTime:                        18:32:49   Pearson chi2:                     102.\nNo. Iterations:                     4   Pseudo R-squ. (CS):             0.2541\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P>|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -5.3095      1.134     -4.683      0.000      -7.531      -3.088\nage            0.1109      0.024      4.610      0.000       0.064       0.158\n==============================================================================\n```\n:::\n:::\n\n\n::: {#0b0a0ced .cell execution_count=6}\n``` {.python .cell-code}\nplt.plot(artere.age, artere.chd, 'o')\nplt.ylabel('chd')\nplt.xlabel('age')\nplt.hlines(artere_summary.Frequence, artere_summary.age_min, artere_summary.age_max, 'k')\nx = np.arange(artere_summary.age_min.min(), artere_summary.age_max.max(), step=0.01)\ny = np.exp(modele.params.Intercept + modele.params.age * x) / (1.0 + np.exp(modele.params.Intercept + modele.params.age * x))\nplt.plot(x, y, 'b--')\nfig.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![](chap12_files/figure-html/cell-7-output-1.png){width=589 height=429}\n:::\n:::\n\n\n::: {#fb854c7e .cell execution_count=7}\n``` {.python .cell-code}\nX = np.random.choice(['A', 'B', 'C'], 100)\nY = np.zeros(X.shape, dtype=int)\nY[X=='A'] = np.random.binomial(1, 0.9, (X=='A').sum())\nY[X=='B'] = np.random.binomial(1, 0.1, (X=='B').sum())\nY[X=='C'] = np.random.binomial(1, 0.9, (X=='C').sum())\ndon = pd.DataFrame({'X': X, 'Y': Y})\n```\n:::\n\n\n::: {#962fbe76 .cell execution_count=8}\n``` {.python .cell-code}\nmod = smf.glm(\"Y~X\", data=don, family=sm.families.Binomial()).fit()\nprint(mod.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                      Y   No. Observations:                  100\nModel:                            GLM   Df Residuals:                       97\nModel Family:                Binomial   Df Model:                            2\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -13.869\nDate:                Tue, 04 Feb 2025   Deviance:                       27.738\nTime:                        18:32:49   Pearson chi2:                     32.0\nNo. Iterations:                    23   Pseudo R-squ. (CS):             0.6689\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P>|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     24.5661   2.57e+04      0.001      0.999   -5.03e+04    5.04e+04\nX[T.B]       -49.1321   3.27e+04     -0.002      0.999   -6.41e+04     6.4e+04\nX[T.C]       -22.8797   2.57e+04     -0.001      0.999   -5.04e+04    5.03e+04\n==============================================================================\n```\n:::\n:::\n\n\n::: {#451b37ce .cell execution_count=9}\n``` {.python .cell-code}\nmod1 = smf.glm(\"Y~C(X, Sum)\", data=don, family=sm.families.Binomial()).fit()\nprint(mod1.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                      Y   No. Observations:                  100\nModel:                            GLM   Df Residuals:                       97\nModel Family:                Binomial   Df Model:                            2\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -13.869\nDate:                Tue, 04 Feb 2025   Deviance:                       27.738\nTime:                        18:32:49   Pearson chi2:                     32.0\nNo. Iterations:                    23   Pseudo R-squ. (CS):             0.6689\nCovariance Type:            nonrobust                                         \n==================================================================================\n                     coef    std err          z      P>|z|      [0.025      0.975]\n----------------------------------------------------------------------------------\nIntercept          0.5621   1.09e+04   5.16e-05      1.000   -2.14e+04    2.14e+04\nC(X, Sum)[S.A]    24.0039   1.84e+04      0.001      0.999   -3.61e+04    3.61e+04\nC(X, Sum)[S.B]   -25.1282    1.6e+04     -0.002      0.999   -3.13e+04    3.13e+04\n==================================================================================\n```\n:::\n:::\n\n\n#   Estimation\n\n::: {#d1ae3798 .cell execution_count=10}\n``` {.python .cell-code}\nSAh = pd.read_csv(\"../donnees/SAh.csv\", header=0, sep=\",\")\nnewSAh = SAh.iloc[[1,407,34],]\nnewSAh = newSAh.reset_index().drop(\"index\",axis=1)\nSAh = SAh.drop([1,407,34]).reset_index().drop(\"index\",axis=1)\n```\n:::\n\n\n::: {#a0306e96 .cell execution_count=11}\n``` {.python .cell-code}\nform =\"chd ~ \" + \"+\".join(SAh.columns[ :-1 ])\nmod = smf.glm(form, data=SAh, family=sm.families.Binomial()).fit()\nmod.summary()\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```{=html}\n<table class=\"simpletable\">\n<caption>Generalized Linear Model Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>          <td>chd</td>       <th>  No. Observations:  </th>  <td>   459</td> \n</tr>\n<tr>\n  <th>Model:</th>                  <td>GLM</td>       <th>  Df Residuals:      </th>  <td>   449</td> \n</tr>\n<tr>\n  <th>Model Family:</th>        <td>Binomial</td>     <th>  Df Model:          </th>  <td>     9</td> \n</tr>\n<tr>\n  <th>Link Function:</th>         <td>Logit</td>      <th>  Scale:             </th> <td>  1.0000</td>\n</tr>\n<tr>\n  <th>Method:</th>                <td>IRLS</td>       <th>  Log-Likelihood:    </th> <td> -234.36</td>\n</tr>\n<tr>\n  <th>Date:</th>            <td>Tue, 04 Feb 2025</td> <th>  Deviance:          </th> <td>  468.72</td>\n</tr>\n<tr>\n  <th>Time:</th>                <td>18:32:49</td>     <th>  Pearson chi2:      </th>  <td>  449.</td> \n</tr>\n<tr>\n  <th>No. Iterations:</th>          <td>5</td>        <th>  Pseudo R-squ. (CS):</th>  <td>0.2339</td> \n</tr>\n<tr>\n  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n           <td></td>             <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>Intercept</th>          <td>   -6.0837</td> <td>    1.314</td> <td>   -4.629</td> <td> 0.000</td> <td>   -8.659</td> <td>   -3.508</td>\n</tr>\n<tr>\n  <th>famhist[T.Present]</th> <td>    0.9325</td> <td>    0.229</td> <td>    4.069</td> <td> 0.000</td> <td>    0.483</td> <td>    1.382</td>\n</tr>\n<tr>\n  <th>sbp</th>                <td>    0.0065</td> <td>    0.006</td> <td>    1.127</td> <td> 0.260</td> <td>   -0.005</td> <td>    0.018</td>\n</tr>\n<tr>\n  <th>tobacco</th>            <td>    0.0814</td> <td>    0.027</td> <td>    3.023</td> <td> 0.003</td> <td>    0.029</td> <td>    0.134</td>\n</tr>\n<tr>\n  <th>ldl</th>                <td>    0.1794</td> <td>    0.060</td> <td>    2.989</td> <td> 0.003</td> <td>    0.062</td> <td>    0.297</td>\n</tr>\n<tr>\n  <th>adiposity</th>          <td>    0.0184</td> <td>    0.030</td> <td>    0.622</td> <td> 0.534</td> <td>   -0.039</td> <td>    0.076</td>\n</tr>\n<tr>\n  <th>typea</th>              <td>    0.0392</td> <td>    0.012</td> <td>    3.184</td> <td> 0.001</td> <td>    0.015</td> <td>    0.063</td>\n</tr>\n<tr>\n  <th>obesity</th>            <td>   -0.0637</td> <td>    0.045</td> <td>   -1.430</td> <td> 0.153</td> <td>   -0.151</td> <td>    0.024</td>\n</tr>\n<tr>\n  <th>alcohol</th>            <td>    0.0002</td> <td>    0.004</td> <td>    0.035</td> <td> 0.972</td> <td>   -0.009</td> <td>    0.009</td>\n</tr>\n<tr>\n  <th>age</th>                <td>    0.0439</td> <td>    0.012</td> <td>    3.592</td> <td> 0.000</td> <td>    0.020</td> <td>    0.068</td>\n</tr>\n</table>\n```\n:::\n:::\n\n\n::: {#ade06db5 .cell execution_count=12}\n``` {.python .cell-code}\nprint(mod.conf_int(alpha=0.05))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                           0         1\nIntercept          -8.659355 -3.507984\nfamhist[T.Present]  0.483354  1.381573\nsbp                -0.004798  0.017773\ntobacco             0.028628  0.134174\nldl                 0.061771  0.297043\nadiposity          -0.039461  0.076187\ntypea               0.015090  0.063396\nobesity            -0.151055  0.023612\nalcohol            -0.008640  0.008950\nage                 0.019931  0.067793\n```\n:::\n:::\n\n\n::: {#0f9a745f .cell execution_count=13}\n``` {.python .cell-code}\ndon = pd.read_csv(\"../donnees/logit_donnees.csv\", sep=\",\", header=0)\nmodsim = smf.logit(\"Y ~ X1 + X2 + X3\", data=don).fit()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOptimization terminated successfully.\n         Current function value: 0.366463\n         Iterations 7\n```\n:::\n:::\n\n\n::: {#2ed58f12 .cell execution_count=14}\n``` {.python .cell-code}\nmodsim.wald_test_terms()\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\n<class 'statsmodels.stats.contrast.WaldTestResults'>\n                             chi2                 P>chi2  df constraint\nIntercept   [[28.46981615119592]]  9.517067345514757e-08              1\nX1         [[212.50601519640435]]  7.159869628652175e-47              2\nX2         [[210.39004424749865]]  1.129196632385863e-47              1\nX3         [[0.3095790886927727]]     0.5779385882309931              1\n```\n:::\n:::\n\n\n::: {#43e3aebc .cell execution_count=15}\n``` {.python .cell-code}\nmodsim01 = smf.logit(\"Y~X2+X3\",data=don).fit()\nmodsim02 = smf.logit(\"Y~X1+X3\",data=don).fit()\nmodsim03 = smf.logit(\"Y~X1+X2\",data=don).fit()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOptimization terminated successfully.\n         Current function value: 0.554834\n         Iterations 6\nOptimization terminated successfully.\n         Current function value: 0.575293\n         Iterations 5\nOptimization terminated successfully.\n         Current function value: 0.366618\n         Iterations 7\n```\n:::\n:::\n\n\n::: {#45e69746 .cell execution_count=16}\n``` {.python .cell-code}\nimport statsmodels.regression.linear_model as smlm\nsmlm.RegressionResults.compare_lr_test(modsim,modsim01)\n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n```\n(376.7417147005359, 1.554447652669738e-82, 2.0)\n```\n:::\n:::\n\n\n::: {#37e886d6 .cell execution_count=17}\n``` {.python .cell-code}\nsmlm.RegressionResults.compare_lr_test(modsim,modsim02)\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```\n(417.66072160440774, 7.881723945480014e-93, 1.0)\n```\n:::\n:::\n\n\n::: {#52ae55dc .cell execution_count=18}\n``` {.python .cell-code}\nsmlm.RegressionResults.compare_lr_test(modsim,modsim03)\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```\n(0.3097619772612461, 0.5778262823265808, 1.0)\n```\n:::\n:::\n\n\n::: {#431f3ec3 .cell execution_count=19}\n``` {.python .cell-code}\nprint(newSAh)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   sbp  tobacco   ldl  adiposity  famhist  typea  obesity  alcohol  age  chd\n0  144     0.01  4.41      28.61   Absent     55    28.87     2.06   63    1\n1  200    19.20  4.43      40.60  Present     55    32.04    36.00   60    1\n2  148     5.50  7.10      25.31   Absent     56    29.84     3.60   48    0\n```\n:::\n:::\n\n\n::: {#e6be8c07 .cell execution_count=20}\n``` {.python .cell-code}\nprint(mod.predict(newSAh))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0    0.320889\n1    0.881177\n2    0.369329\ndtype: float64\n```\n:::\n:::\n\n\n::: {#2f48e67b .cell execution_count=21}\n``` {.python .cell-code}\nvarbetac = mod.cov_params().values\nbetac = mod.params.values\nff = mod.model.formula.split(\"~\")[1]\nxetoile =  dmatrix(\"~\"+ff, data=newSAh, return_type=\"dataframe\").to_numpy()\nprev_fit = np.dot(xetoile,betac)\nprev_se = np.diag(np.dot(np.dot(xetoile,varbetac), np.transpose(xetoile)))**0.5\ncl_inf = prev_fit-norm.ppf(0.975)*prev_se\ncl_sup = prev_fit+norm.ppf(0.975)*prev_se\nbinf = np.exp(cl_inf)/(1+np.exp(cl_inf))\nbsup = np.exp(cl_sup)/(1+np.exp(cl_sup))\nprint(pd.DataFrame({\"binf\": binf, \"bsup\": bsup}))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       binf      bsup\n0  0.199717  0.472200\n1  0.713881  0.956601\n2  0.246181  0.512220\n```\n:::\n:::\n\n\n::: {#d0f29e2d .cell execution_count=22}\n``` {.python .cell-code}\ng = artere.groupby([\"age\"])\ndfsat = pd.concat([g[\"chd\"].mean(), g[\"chd\"].count()], axis=1)\ndfsat.columns = [\"p\", \"n\"]\nprint(dfsat.iloc[0:5])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       p  n\nage        \n20   0.0  1\n23   0.0  1\n24   0.0  1\n25   0.5  2\n26   0.0  2\n```\n:::\n:::\n\n\n::: {#45f377e0 .cell execution_count=23}\n``` {.python .cell-code}\nplt.plot(artere.age, artere.chd, 'o', dfsat.index, dfsat.p, \"-\")\nplt.ylabel('chd')\nplt.xlabel('age')\nfig.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![](chap12_files/figure-html/cell-24-output-1.png){width=589 height=429}\n:::\n:::\n\n\n::: {#5d8abf3b .cell execution_count=24}\n``` {.python .cell-code}\nK=10\najust = pd.DataFrame({\"ajust\": mod.predict()}, index=SAh.index)\najust[\"Y\"] = SAh[\"chd\"]\najust['decile'] = pd.qcut(ajust[\"ajust\"], K)\nok = ajust['Y'].groupby(ajust.decile).sum()\nmuk = ajust[\"ajust\"].groupby(ajust.decile).mean()\nmk = ajust['Y'].groupby(ajust.decile).count()\nC2 = ((ok - mk*muk)**2/(mk*muk*(1-muk))).sum()\n```\n:::\n\n\n::: {#d2321194 .cell execution_count=25}\n``` {.python .cell-code}\nprint('chi-square: {:.3f}'.format(C2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nchi-square: 6.659\n```\n:::\n:::\n\n\n::: {#d92902fb .cell execution_count=26}\n``` {.python .cell-code}\npvalue=1-chi2.cdf(C2, K-2)\nprint('p-value: {:.3f}'.format(pvalue))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\np-value: 0.574\n```\n:::\n:::\n\n\n::: {#85d7e7c8 .cell execution_count=27}\n``` {.python .cell-code}\nform =\"chd ~ \" + \"+\".join(SAh.columns[ :-1 ])\nmod = smf.glm(form, data=SAh, family=sm.families.Binomial()).fit()\n```\n:::\n\n\n::: {#9bb0b5b7 .cell execution_count=28}\n``` {.python .cell-code}\nresdev = mod.resid_deviance/np.sqrt(1-mod.get_hat_matrix_diag())\nrespea = mod.resid_pearson/np.sqrt(1-mod.get_hat_matrix_diag())\n```\n:::\n\n\n::: {#b8f777eb .cell execution_count=29}\n``` {.python .cell-code}\nfig = plt.figure()\nplt.plot(resdev, 'o')\nplt.ylabel('residus deviance')\nfig.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![](chap12_files/figure-html/cell-30-output-1.png){width=661 height=469}\n:::\n:::\n\n\n::: {#d8a79694 .cell execution_count=30}\n``` {.python .cell-code}\nfig = plt.figure()\nplt.plot(respea, 'o')\nplt.ylabel('residus Pearson')\nfig.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![](chap12_files/figure-html/cell-31-output-1.png){width=661 height=469}\n:::\n:::\n\n\n#   Choix de variables\n\n::: {#5d15951a .cell execution_count=31}\n``` {.python .cell-code}\nmod0 = smf.glm(\"chd~sbp+ldl\", data=SAh, family=sm.families.Binomial()).fit()\nmod1 = smf.glm(\"chd~sbp+ldl+famhist+alcohol\", data=SAh, family=sm.families.Binomial()).fit()\ndef lr_test(restr, full):\n    from scipy import stats\n    lr_df = (restr.df_resid - full.df_resid)\n    lr_stat = -2*(restr.llf - full.llf)\n    lr_pvalue = stats.chi2.sf(lr_stat, df=lr_df)\n    return {\"lr\": lr_stat, \"pvalue\": lr_pvalue, \"df\": lr_df}\n\nlr_test(mod0, mod1)\n```\n\n::: {.cell-output .cell-output-display execution_count=31}\n```\n{'lr': 25.5447172394405, 'pvalue': 2.838148600168801e-06, 'df': 2}\n```\n:::\n:::\n\n\n::: {#d218b7a6 .cell execution_count=32}\n``` {.python .cell-code}\nmod_sel = choixglmstats.bestglm(SAh, upper=form)\n```\n:::\n\n\n::: {#96b6aa66 .cell execution_count=33}\n``` {.python .cell-code}\nprint(mod_sel.sort_values(by=[\"BIC\",\"nb_var\"]).iloc[:5,[1,3]])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                                        var_added         BIC\n176           (tobacco, famhist, ldl, typea, age)  509.100392\n287                (tobacco, famhist, typea, age)  512.495444\n284                  (tobacco, famhist, ldl, age)  512.537897\n91   (tobacco, famhist, ldl, typea, obesity, age)  513.424654\n358                    (famhist, ldl, typea, age)  513.471151\n```\n:::\n:::\n\n\n::: {#7e294839 .cell execution_count=34}\n``` {.python .cell-code}\nprint(mod_sel.sort_values(by=[\"AIC\",\"nb_var\"]).iloc[:5,[1,2]])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                                             var_added         AIC\n176                (tobacco, famhist, ldl, typea, age)  484.326091\n91        (tobacco, famhist, ldl, typea, obesity, age)  484.521302\n36   (tobacco, famhist, ldl, typea, obesity, age, sbp)  485.117363\n93            (tobacco, famhist, ldl, typea, age, sbp)  485.311962\n31   (tobacco, famhist, adiposity, ldl, typea, obes...  486.031360\n```\n:::\n:::\n\n\n",
    "supporting": [
      "chap12_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}