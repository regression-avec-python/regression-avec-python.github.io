{
  "hash": "701d65e2e4cc5794338a4b9b262721c3",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"15 Comparaison des différentes méthodes en classification supervisée, étude de cas réels\"\ntoc: true\n---\n\n::: {#900aaa4d .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd; import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression, \\\n    LogisticRegressionCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import StratifiedKFold\nimport sklearn.metrics as sklm\nfrom patsy import dmatrix\n\nimport sys\nsys.path.append('../modules')\nimport logistic_step_sk as lss \n```\n:::\n\n\n#   Analyse des données `chd`\n\n::: {#a26903af .cell execution_count=2}\n``` {.python .cell-code}\ndon = pd.read_csv(\"../donnees/SAh.csv\", header=0,  sep=\",\")\ndon.rename(columns={\"chd\": \"Y\"},inplace=True)\ndon.Y.value_counts()\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\nY\n0    302\n1    160\nName: count, dtype: int64\n```\n:::\n:::\n\n\n::: {#95043c17 .cell execution_count=3}\n``` {.python .cell-code}\nY = don[\"Y\"].to_numpy()\nPROB = pd.DataFrame({\"Y\":Y,\"log\":0.0,\"BIC\":0.0,\"AIC\":0.0,\n                    \"ridge\":0.0,\"lasso\":0.0,\"elast\":0.0})\n```\n:::\n\n\n::: {#ac05ba8e .cell execution_count=4}\n``` {.python .cell-code}\nnb=10\nskf = StratifiedKFold(n_splits=nb, shuffle=True, random_state=123)\n```\n:::\n\n\n::: {#546a8c00 .cell execution_count=5}\n``` {.python .cell-code}\nnomsvar = don.columns.difference([\"Y\"])\nformule = \"~\" + \"+\".join(nomsvar)\nX = dmatrix(formule, don, return_type=\"dataframe\").\\\n                                            iloc[:,1:].to_numpy()\n```\n:::\n\n\n::: {#5d8351dc .cell execution_count=6}\n``` {.python .cell-code}\ndef grille(X, y, type = \"lasso\", ng=400):\n    scalerX = StandardScaler().fit(X)\n    Xcr= scalerX.transform(X)\n    l0 = np.abs(Xcr.transpose().dot((y-y.mean()))).max()/X.shape[0]\n    llc = np.linspace(0,-4,ng)\n    ll = l0*10**llc\n    if type==\"lasso\":\n        Cs = 1/ 0.9/ X.shape[0] / (l0*10**(llc))\n    elif type==\"ridge\":\n        Cs = 1/ 0.9/ X.shape[0] / ((l0*10**(llc)) * 100)\n    elif type==\"enet\":\n        Cs = 1/ 0.9/ X.shape[0] / ((l0*10**(llc)) * 2)\n    return Cs\n```\n:::\n\n\n::: {#sah_cv_class .cell execution_count=7}\n``` {.python .cell-code}\nfor app_index, val_index in skf.split(X,Y):\n    Xapp = X[app_index,:]\n    Xtest = X[val_index,:]\n    Yapp = Y[app_index]\n    log = LogisticRegression(penalty=None,solver=\"newton-cholesky\").fit(Xapp,Yapp)\n    PROB.loc[val_index,\"log\"] = log.predict_proba(Xtest)[:,1]\n    ### bic\n    choixbic = lss.LogisticRegressionSelectionFeatureIC(start=[], \\\n        direction=\"forward\",crit=\"bic\").fit(Xapp,Yapp)\n    PROB.loc[val_index, \"BIC\"] = choixbic.predict_proba(Xtest)[:,1]\n    ### aic\n    choixaic = lss.LogisticRegressionSelectionFeatureIC(start=[], \\\n        direction=\"forward\",crit=\"aic\").fit(Xapp,Yapp)\n    PROB.loc[val_index, \"AIC\"] = choixaic.predict_proba(Xtest)[:,1]\n    ### lasso\n    cr = StandardScaler()\n    Cs_lasso = grille(Xapp,Yapp, \"lasso\")\n    lassocv =  LogisticRegressionCV(cv=10, penalty=\"l1\", n_jobs=10,\\\n                 Cs=Cs_lasso,  solver=\"saga\", max_iter=2000)\n    pipe_lassocv = Pipeline(steps=[(\"cr\", cr), (\"lassocv\", lassocv)])\n    pipe_lassocv.fit(Xapp,Yapp)\n    PROB.loc[val_index,\"lasso\"] = pipe_lassocv.predict_proba(Xtest)[:,1]\n    ### elastic net\n    cr = StandardScaler()\n    Cs_enet = grille(Xapp,Yapp,\"enet\")\n    enetcv=LogisticRegressionCV(cv=10,penalty=\"elasticnet\",n_jobs=10,\\\n          l1_ratios=[0.5],Cs=Cs_enet,solver=\"saga\",max_iter=2000)\n    pipe_enetcv = Pipeline(steps=[(\"cr\", cr), (\"enetcv\", enetcv)])\n    pipe_enetcv.fit(Xapp,Yapp)\n    PROB.loc[val_index,\"elast\"] = pipe_enetcv.predict_proba(Xtest)[:,1] \n    ### ridge\n    cr = StandardScaler()\n    Cs_ridge = grille(Xapp,Yapp,\"ridge\")\n    ridgecv = LogisticRegressionCV(cv=10, penalty=\"l2\", \\\n            Cs=Cs_ridge,  max_iter=1000)\n    pipe_ridgecv = Pipeline(steps=[(\"cr\", cr), (\"ridgecv\", ridgecv)])\n    pipe_ridgecv.fit(Xapp,Yapp)\n    PROB.loc[val_index,\"ridge\"] = pipe_ridgecv.predict_proba(Xtest)[:,1]\n```\n:::\n\n\n::: {#b4596908 .cell execution_count=8}\n``` {.python .cell-code}\nround(PROB.iloc[0:4,:],3)\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Y</th>\n      <th>log</th>\n      <th>BIC</th>\n      <th>AIC</th>\n      <th>ridge</th>\n      <th>lasso</th>\n      <th>elast</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0.742</td>\n      <td>0.689</td>\n      <td>0.689</td>\n      <td>0.675</td>\n      <td>0.587</td>\n      <td>0.598</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0.292</td>\n      <td>0.362</td>\n      <td>0.333</td>\n      <td>0.321</td>\n      <td>0.373</td>\n      <td>0.358</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0.251</td>\n      <td>0.313</td>\n      <td>0.230</td>\n      <td>0.286</td>\n      <td>0.316</td>\n      <td>0.297</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>0.719</td>\n      <td>0.720</td>\n      <td>0.681</td>\n      <td>0.676</td>\n      <td>0.694</td>\n      <td>0.682</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#08999d2c .cell execution_count=9}\n``` {.python .cell-code}\nmc = pd.Series(0.0, index=PROB.columns[1:])\ns = 0.5\nfor i in range(mc.shape[0]):\n    mc.iloc[i] = sklm.zero_one_loss(PROB.Y, PROB.iloc[:,i+1]>s)\nround(mc,3)\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\nlog      0.281\nBIC      0.281\nAIC      0.279\nridge    0.266\nlasso    0.279\nelast    0.271\ndtype: float64\n```\n:::\n:::\n\n\nOn ajoute les méthodes régularisées avec sélection du paramètre par opposé de la log-vraisemblance : \n\n::: {#900a3707 .cell execution_count=10}\n``` {.python .cell-code}\nPROB = PROB.assign(LassoL=0.0)\nPROB = PROB.assign(RidgeL=0.0)\nPROB = PROB.assign(EnetL=0.0)\nfor app_index, val_index in skf.split(X,Y):\n    Xapp = X[app_index,:-1]\n    Yapp = Y[app_index]\n    Xval = X[val_index,:-1]\n    # grille\n    Cs_lasso = grille(Xapp, Yapp, \"lasso\")\n    Cs_ridge = grille(Xapp, Yapp, \"ridge\")\n    Cs_enet = grille(Xapp, Yapp, \"enet\")\n    # instanciation\n    cr = StandardScaler()\n    lassocv =  LogisticRegressionCV(cv=10, penalty=\"l1\", n_jobs=10, Cs=Cs_lasso,  solver=\"saga\", max_iter=2000, scoring=\"neg_log_loss\")\n    ridgecv = LogisticRegressionCV(cv=10, penalty=\"l2\", n_jobs=10, Cs=Cs_ridge,  max_iter=1000, scoring=\"neg_log_loss\")\n    enetcv = LogisticRegressionCV(cv=10, penalty=\"elasticnet\", l1_ratios = [0.5], n_jobs=10, Cs=Cs_enet,  solver=\"saga\", max_iter=2000, scoring=\"neg_log_loss\")\n    pipe_lassocv = Pipeline(steps=[(\"cr\", cr), (\"lassocv\", lassocv)])\n    pipe_ridgecv = Pipeline(steps=[(\"cr\", cr), (\"ridgecv\", ridgecv)])\n    pipe_enetcv = Pipeline(steps=[(\"cr\", cr), (\"enet\", enetcv)])\n    # fit brut\n    pipe_lassocv.fit(Xapp, Yapp)\n    pipe_ridgecv.fit(Xapp, Yapp)\n    pipe_enetcv.fit(Xapp, Yapp)\n    # prediction\n    PROB.loc[val_index,\"LassoL\"] = pipe_lassocv.predict_proba(Xval)[:,1]\n    PROB.loc[val_index,\"RidgeL\"] = pipe_ridgecv.predict_proba(Xval)[:,1]\n    PROB.loc[val_index,\"EnetL\"] = pipe_enetcv.predict_proba(Xval)[:,1]\n```\n:::\n\n\net par AUC \n\n::: {#a347bef3 .cell execution_count=11}\n``` {.python .cell-code}\nPROB = PROB.assign(LassoA=0.0)\nPROB = PROB.assign(RidgeA=0.0)\nPROB = PROB.assign(EnetA=0.0)\nfor app_index, val_index in skf.split(X,Y):\n    Xapp = X[app_index,:-1]\n    Yapp = Y[app_index]\n    Xval = X[val_index,:-1]\n    # grille\n    Cs_lasso = grille(Xapp, Yapp, \"lasso\")\n    Cs_ridge = grille(Xapp, Yapp, \"ridge\")\n    Cs_enet = grille(Xapp, Yapp, \"enet\")\n    # instanciation\n    cr = StandardScaler()\n    lassocv =  LogisticRegressionCV(cv=10, penalty=\"l1\", n_jobs=10, Cs=Cs_lasso,  solver=\"saga\", max_iter=2000, scoring=\"roc_auc\")\n    ridgecv = LogisticRegressionCV(cv=10, penalty=\"l2\", n_jobs=10, Cs=Cs_ridge,  max_iter=1000, scoring=\"roc_auc\")\n    enetcv = LogisticRegressionCV(cv=10, penalty=\"elasticnet\", l1_ratios = [0.5], n_jobs=10, Cs=Cs_enet,  solver=\"saga\", max_iter=2000, scoring=\"roc_auc\")\n    pipe_lassocv = Pipeline(steps=[(\"cr\", cr), (\"lassocv\", lassocv)])\n    pipe_ridgecv = Pipeline(steps=[(\"cr\", cr), (\"ridgecv\", ridgecv)])\n    pipe_enetcv = Pipeline(steps=[(\"cr\", cr), (\"enet\", enetcv)])\n    # fit brut\n    pipe_lassocv.fit(Xapp, Yapp)\n    pipe_ridgecv.fit(Xapp, Yapp)\n    pipe_enetcv.fit(Xapp, Yapp)\n    # prediction\n    PROB.loc[val_index,\"LassoA\"] = pipe_lassocv.predict_proba(Xval)[:,1]\n    PROB.loc[val_index,\"RidgeA\"] = pipe_ridgecv.predict_proba(Xval)[:,1]\n    PROB.loc[val_index,\"EnetA\"] = pipe_enetcv.predict_proba(Xval)[:,1]\n```\n:::\n\n\n::: {#e77c6d23 .cell execution_count=12}\n``` {.python .cell-code}\nround(PROB.iloc[0:4,:],2)\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Y</th>\n      <th>log</th>\n      <th>BIC</th>\n      <th>AIC</th>\n      <th>ridge</th>\n      <th>lasso</th>\n      <th>elast</th>\n      <th>LassoL</th>\n      <th>RidgeL</th>\n      <th>EnetL</th>\n      <th>LassoA</th>\n      <th>RidgeA</th>\n      <th>EnetA</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0.74</td>\n      <td>0.69</td>\n      <td>0.69</td>\n      <td>0.68</td>\n      <td>0.59</td>\n      <td>0.60</td>\n      <td>0.67</td>\n      <td>0.72</td>\n      <td>0.66</td>\n      <td>0.67</td>\n      <td>0.58</td>\n      <td>0.57</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0.29</td>\n      <td>0.36</td>\n      <td>0.33</td>\n      <td>0.32</td>\n      <td>0.37</td>\n      <td>0.36</td>\n      <td>0.33</td>\n      <td>0.28</td>\n      <td>0.31</td>\n      <td>0.28</td>\n      <td>0.27</td>\n      <td>0.28</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0.25</td>\n      <td>0.31</td>\n      <td>0.23</td>\n      <td>0.29</td>\n      <td>0.32</td>\n      <td>0.30</td>\n      <td>0.29</td>\n      <td>0.29</td>\n      <td>0.29</td>\n      <td>0.32</td>\n      <td>0.33</td>\n      <td>0.32</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>0.72</td>\n      <td>0.72</td>\n      <td>0.68</td>\n      <td>0.68</td>\n      <td>0.69</td>\n      <td>0.68</td>\n      <td>0.70</td>\n      <td>0.70</td>\n      <td>0.70</td>\n      <td>0.60</td>\n      <td>0.64</td>\n      <td>0.63</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#18a0d9d1 .cell execution_count=13}\n``` {.python .cell-code}\nauc = pd.Series(0.0, index=PROB.columns[1:])\nfor i in range(auc.shape[0]):\n    auc.iloc[i] = sklm.roc_auc_score(PROB.Y, PROB.iloc[:,i+1])\nround(auc.sort_values(ascending=False)[:6],3)\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\nridge    0.779\nelast    0.776\nlog      0.773\nAIC      0.772\nBIC      0.772\nEnetL    0.770\ndtype: float64\n```\n:::\n:::\n\n\n\n\n::: {#9f452cdc .cell execution_count=15}\n``` {.python .cell-code}\nfig, ax = plt.subplots(1,1)\nnoms = auc.sort_values(ascending=False)\nfor i, nom in enumerate(noms.index):\n    if i<4:\n        roc = sklm.RocCurveDisplay.from_predictions(PROB.Y, \\\n            PROB.loc[:,nom], ax=ax, name=nom, plot_chance_level=(i==0))\nfig.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![](chap15_files/figure-html/cell-16-output-1.png){width=481 height=468}\n:::\n:::\n\n\n::: {#7516cfe5 .cell execution_count=16}\n``` {.python .cell-code}\nnoms = PROB.columns[1:]\nmatsB = pd.DataFrame({\"seuil\": pd.Series(0.0, index=noms)})\ns = .5\nfor nom in noms:\n    matsB.loc[nom,\"seuil\"] = s\n    confmat = sklm.confusion_matrix(PROB.Y, PROB.loc[:,nom]>=s)\n    matsB.loc[nom, \"tn\"] = confmat[0,0]\n    matsB.loc[nom, \"tp\"] = confmat[1,1]\n    matsB.loc[nom, \"fn\"] = confmat[1,0]\n    matsB.loc[nom, \"fp\"] = confmat[0,1]\n    matsB.loc[nom,\"sensitivity\"] = confmat[1,1]/(confmat[1,1]+confmat[1,0])\n    matsB.loc[nom,\"specificity\"] = confmat[0,0]/(confmat[0,0]+confmat[0,1])\n    matsB.loc[nom,\"accuracy\"] = sklm.accuracy_score(PROB.Y, PROB.loc[:,nom]>=s)\nprint(matsB.round(3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        seuil     tn    tp     fn    fp  sensitivity  specificity  accuracy\nlog       0.5  250.0  82.0   78.0  52.0        0.512        0.828     0.719\nBIC       0.5  250.0  82.0   78.0  52.0        0.512        0.828     0.719\nAIC       0.5  250.0  83.0   77.0  52.0        0.519        0.828     0.721\nridge     0.5  262.0  77.0   83.0  40.0        0.481        0.868     0.734\nlasso     0.5  269.0  64.0   96.0  33.0        0.400        0.891     0.721\nelast     0.5  268.0  69.0   91.0  34.0        0.431        0.887     0.729\nLassoL    0.5  256.0  77.0   83.0  46.0        0.481        0.848     0.721\nRidgeL    0.5  257.0  77.0   83.0  45.0        0.481        0.851     0.723\nEnetL     0.5  260.0  76.0   84.0  42.0        0.475        0.861     0.727\nLassoA    0.5  265.0  73.0   87.0  37.0        0.456        0.877     0.732\nRidgeA    0.5  271.0  60.0  100.0  31.0        0.375        0.897     0.716\nEnetA     0.5  269.0  68.0   92.0  33.0        0.425        0.891     0.729\n```\n:::\n:::\n\n\n::: {#974c4c0a .cell execution_count=17}\n``` {.python .cell-code}\nmatsN = pd.DataFrame({\"seuil\": pd.Series(0.0, index=noms)})\nnbr0 = don.Y.value_counts()[0]\nfor nom in noms:\n    tmp = PROB.loc[:,nom].sort_values(ascending=True)\n    s = (tmp.iloc[nbr0-1]+tmp.iloc[nbr0])/2\n    confmat = sklm.confusion_matrix(PROB.Y, PROB.loc[:,nom]>=s)\n    matsN.loc[nom,\"seuil\"] = s\n    matsN.loc[nom, \"tn\"] = confmat[0,0]\n    matsN.loc[nom, \"tp\"] = confmat[1,1]\n    matsN.loc[nom, \"fn\"] = confmat[1,0]\n    matsN.loc[nom, \"fp\"] = confmat[0,1]\n    matsN.loc[nom,\"sensitivity\"] = confmat[1,1]/(confmat[1,1]+confmat[1,0])\n    matsN.loc[nom,\"specificity\"] = confmat[0,0]/(confmat[0,0]+confmat[0,1])\n    matsN.loc[nom,\"accuracy\"] = sklm.accuracy_score(PROB.Y, PROB.loc[:,nom]>=s)\nprint(matsN.round(3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        seuil     tn    tp    fn    fp  sensitivity  specificity  accuracy\nlog     0.431  236.0  94.0  66.0  66.0        0.588        0.781     0.714\nBIC     0.444  235.0  93.0  67.0  67.0        0.581        0.778     0.710\nAIC     0.441  236.0  94.0  66.0  66.0        0.588        0.781     0.714\nridge   0.418  237.0  95.0  65.0  65.0        0.594        0.785     0.719\nlasso   0.428  236.0  94.0  66.0  66.0        0.588        0.781     0.714\nelast   0.425  238.0  96.0  64.0  64.0        0.600        0.788     0.723\nLassoL  0.442  235.0  93.0  67.0  67.0        0.581        0.778     0.710\nRidgeL  0.432  235.0  93.0  67.0  67.0        0.581        0.778     0.710\nEnetL   0.436  235.0  93.0  67.0  67.0        0.581        0.778     0.710\nLassoA  0.431  236.0  94.0  66.0  66.0        0.588        0.781     0.714\nRidgeA  0.418  236.0  94.0  66.0  66.0        0.588        0.781     0.714\nEnetA   0.425  233.0  91.0  69.0  69.0        0.569        0.772     0.701\n```\n:::\n:::\n\n\n::: {#cf42686c .cell execution_count=18}\n``` {.python .cell-code}\nmatsY = pd.DataFrame({\"seuil\": pd.Series(0.0, index=noms)})\nfor nom in noms:\n    fpr, tpr, thr  = sklm.roc_curve(PROB.Y, PROB.loc[:,nom])\n    ii = (tpr-fpr).argmax()\n    s = thr[ii]\n    matsY.loc[nom,\"seuil\"] = s\n    confmat = sklm.confusion_matrix(PROB.Y, PROB.loc[:,nom]>=s)\n    matsY.loc[nom, \"tn\"] = confmat[0,0]\n    matsY.loc[nom, \"tp\"] = confmat[1,1]\n    matsY.loc[nom, \"fn\"] = confmat[1,0]\n    matsY.loc[nom, \"fp\"] = confmat[0,1]\n    matsY.loc[nom,\"sensitivity\"] = tpr[ii]\n    matsY.loc[nom,\"sensitivity\"] = 1-fpr[ii]\n    matsY.loc[nom,\"accuracy\"] = sklm.accuracy_score(PROB.Y, PROB.loc[:,nom]>=s)\nprint(matsY.round(3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        seuil     tn     tp    fn     fp  sensitivity  accuracy\nlog     0.292  188.0  128.0  32.0  114.0        0.623     0.684\nBIC     0.280  182.0  129.0  31.0  120.0        0.603     0.673\nAIC     0.300  196.0  124.0  36.0  106.0        0.649     0.693\nridge   0.311  192.0  129.0  31.0  110.0        0.636     0.695\nlasso   0.360  210.0  118.0  42.0   92.0        0.695     0.710\nelast   0.362  212.0  116.0  44.0   90.0        0.702     0.710\nLassoL  0.348  202.0  118.0  42.0  100.0        0.669     0.693\nRidgeL  0.330  197.0  121.0  39.0  105.0        0.652     0.688\nEnetL   0.278  173.0  133.0  27.0  129.0        0.573     0.662\nLassoA  0.367  209.0  113.0  47.0   93.0        0.692     0.697\nRidgeA  0.346  197.0  121.0  39.0  105.0        0.652     0.688\nEnetA   0.364  205.0  115.0  45.0   97.0        0.679     0.693\n```\n:::\n:::\n\n\n#   Feature engineering\n\n##  Interactions\n\n::: {#349af8dc .cell execution_count=19}\n``` {.python .cell-code}\nformuleI = \"1 + (\" + \"+\".join(nomsvar) + \")**2\"\nPROB = pd.DataFrame({\"Y\":Y,\"log\":0.0,\"BIC\":0.0,\"AIC\":0.0,\n                    \"ridge\":0.0,\"lasso\":0.0,\"elast\":0.0})\n\n\nXinter = dmatrix(formuleI, don, return_type=\"dataframe\").iloc[:,1:].to_numpy()\nXinter.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```\n(462, 45)\n```\n:::\n:::\n\n\n::: {#f81449fc .cell execution_count=20}\n``` {.python .cell-code}\ncr = StandardScaler()\nlassocv =  LogisticRegressionCV(cv=10, penalty=\"l1\", n_jobs=10, Cs=Cs_lasso,  solver=\"saga\", max_iter=2000)\nenetcv = LogisticRegressionCV(cv=10, penalty=\"elasticnet\", l1_ratios = [0.5], n_jobs=10,  Cs=Cs_enet, solver=\"saga\", max_iter=2000)\nridgecv = LogisticRegressionCV(cv=10, penalty=\"l2\", n_jobs=10, Cs=Cs_ridge,  max_iter=1000)\npipe_lassocv = Pipeline(steps=[(\"cr\", cr), (\"lassocv\", lassocv)])\npipe_enetcv = Pipeline(steps=[(\"cr\", cr), (\"enetcv\", enetcv)])\npipe_ridgecv = Pipeline(steps=[(\"cr\", cr), (\"ridgecv\", ridgecv)])\n\nnb=10\nskf = StratifiedKFold(n_splits=nb, shuffle=True, random_state=1234)\n```\n:::\n\n\n::: {#52e1cf9b .cell execution_count=21}\n``` {.python .cell-code}\nfor app_index, val_index in skf.split(X,Y):\n    Xapp = Xinter[app_index,:]\n    Xtest = Xinter[val_index,:]\n    Yapp = Y[app_index]\n    ### log\n    log = LogisticRegression(penalty=None,solver=\"newton-cholesky\").fit(Xapp,Yapp)\n    PROB.loc[val_index,\"log\"] = log.predict_proba(Xtest)[:,1]\n    ### bic\n    choixbic = lss.LogisticRegressionSelectionFeatureIC(start=[], \\\n        direction=\"forward\",crit=\"bic\").fit(Xapp,Yapp)\n    PROB.loc[val_index, \"BIC\"] = choixbic.predict_proba(Xtest)[:,1]\n    ### aic\n    choixaic = lss.LogisticRegressionSelectionFeatureIC(start=[], \\\n        direction=\"forward\",crit=\"aic\").fit(Xapp,Yapp)\n    PROB.loc[val_index, \"AIC\"] = choixaic.predict_proba(Xtest)[:,1]\n    ### lasso\n    cr = StandardScaler()\n    Cs_lasso = grille(Xapp,Yapp, \"lasso\")\n    lassocv =  LogisticRegressionCV(cv=10, penalty=\"l1\", n_jobs=10,\\\n                 Cs=Cs_lasso,  solver=\"saga\", max_iter=2000)\n    pipe_lassocv = Pipeline(steps=[(\"cr\", cr), (\"lassocv\", lassocv)])\n    pipe_lassocv.fit(Xapp,Yapp)\n    PROB.loc[val_index,\"lasso\"] = pipe_lassocv.predict_proba(Xtest)[:,1]\n    ### elastic net\n    cr = StandardScaler()\n    Cs_enet = grille(Xapp,Yapp,\"enet\")\n    enetcv=LogisticRegressionCV(cv=10,penalty=\"elasticnet\",n_jobs=10,\\\n          l1_ratios=[0.5],Cs=Cs_enet,solver=\"saga\",max_iter=2000)\n    pipe_enetcv = Pipeline(steps=[(\"cr\", cr), (\"enetcv\", enetcv)])\n    pipe_enetcv.fit(Xapp,Yapp)\n    PROB.loc[val_index,\"elast\"] = pipe_enetcv.predict_proba(Xtest)[:,1] \n    ### ridge\n    cr = StandardScaler()\n    Cs_ridge = grille(Xapp,Yapp,\"ridge\")\n    ridgecv = LogisticRegressionCV(cv=10, penalty=\"l2\", \\\n            Cs=Cs_ridge,  max_iter=1000)\n    pipe_ridgecv = Pipeline(steps=[(\"cr\", cr), (\"ridgecv\", ridgecv)])\n    pipe_ridgecv.fit(Xapp,Yapp)\n    PROB.loc[val_index,\"ridge\"] = pipe_ridgecv.predict_proba(Xtest)[:,1]\n```\n:::\n\n\n::: {#78298ac7 .cell execution_count=22}\n``` {.python .cell-code}\nmc = pd.Series(0.0, index=PROB.columns[1:])\ns = 0.5\nfor i in range(mc.shape[0]):\n    mc.iloc[i] = sklm.zero_one_loss(PROB.Y, PROB.iloc[:,i+1]>s)\n\nround(mc.sort_values(ascending=True),3)\n```\n\n::: {.cell-output .cell-output-display execution_count=21}\n```\nAIC      0.245\nBIC      0.251\nridge    0.260\nelast    0.264\nlasso    0.268\nlog      0.288\ndtype: float64\n```\n:::\n:::\n\n\n##  Polynôme\n\n::: {#e379d586 .cell execution_count=23}\n``` {.python .cell-code}\nXquanti = don.drop(columns=\"Y\").\\\n                    select_dtypes(include=[np.number]).to_numpy()\nXcar = Xquanti**2\nXcub = Xquanti**3\nformule = \"~\" + \"+\".join(nomsvar)\nX = dmatrix(formule, don, return_type=\"dataframe\").\\\n                                            iloc[:,1:].to_numpy()\nXpol = np.concatenate((X, Xcar, Xcub), axis=1)\nXpol.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=22}\n```\n(462, 25)\n```\n:::\n:::\n\n\n::: {#23f3f4a0 .cell execution_count=24}\n``` {.python .cell-code}\ncr = StandardScaler()\nlassocv =  LogisticRegressionCV(cv=10, penalty=\"l1\", n_jobs=10, Cs=Cs_lasso,  solver=\"saga\", max_iter=2000)\nenetcv = LogisticRegressionCV(cv=10, penalty=\"elasticnet\", l1_ratios = [0.5], n_jobs=10,  Cs=Cs_enet, solver=\"saga\", max_iter=2000)\nridgecv = LogisticRegressionCV(cv=10, penalty=\"l2\", n_jobs=10, Cs=Cs_ridge,  max_iter=1000)\npipe_lassocv = Pipeline(steps=[(\"cr\", cr), (\"lassocv\", lassocv)])\npipe_enetcv = Pipeline(steps=[(\"cr\", cr), (\"enetcv\", enetcv)])\npipe_ridgecv = Pipeline(steps=[(\"cr\", cr), (\"ridgecv\", ridgecv)])\n\nnb=10\nskf = StratifiedKFold(n_splits=nb, shuffle=True, random_state=1234)\n```\n:::\n\n\n::: {#54ed2bc7 .cell execution_count=25}\n``` {.python .cell-code}\nfor app_index, val_index in skf.split(X,Y):\n    Xapp = Xpol[app_index,:]\n    Xtest = Xpol[val_index,:]\n    Yapp = Y[app_index]\n    ### log\n    log = LogisticRegression(penalty=None,solver=\"newton-cholesky\").fit(Xapp,Yapp)\n    PROB.loc[val_index,\"log\"] = log.predict_proba(Xtest)[:,1]\n    ### bic\n    choixbic = lss.LogisticRegressionSelectionFeatureIC(start=[], \\\n        direction=\"forward\",crit=\"bic\").fit(Xapp,Yapp)\n    PROB.loc[val_index, \"BIC\"] = choixbic.predict_proba(Xtest)[:,1]\n    ### aic\n    choixaic = lss.LogisticRegressionSelectionFeatureIC(start=[], \\\n        direction=\"forward\",crit=\"aic\").fit(Xapp,Yapp)\n    PROB.loc[val_index, \"AIC\"] = choixaic.predict_proba(Xtest)[:,1]\n    ### lasso\n    cr = StandardScaler()\n    Cs_lasso = grille(Xapp,Yapp, \"lasso\")\n    lassocv =  LogisticRegressionCV(cv=10, penalty=\"l1\", n_jobs=10,\\\n                 Cs=Cs_lasso,  solver=\"saga\", max_iter=2000)\n    pipe_lassocv = Pipeline(steps=[(\"cr\", cr), (\"lassocv\", lassocv)])\n    pipe_lassocv.fit(Xapp,Yapp)\n    PROB.loc[val_index,\"lasso\"] = pipe_lassocv.predict_proba(Xtest)[:,1]\n    ### elastic net\n    cr = StandardScaler()\n    Cs_enet = grille(Xapp,Yapp,\"enet\")\n    enetcv=LogisticRegressionCV(cv=10,penalty=\"elasticnet\",n_jobs=10,\\\n          l1_ratios=[0.5],Cs=Cs_enet,solver=\"saga\",max_iter=2000)\n    pipe_enetcv = Pipeline(steps=[(\"cr\", cr), (\"enetcv\", enetcv)])\n    pipe_enetcv.fit(Xapp,Yapp)\n    PROB.loc[val_index,\"elast\"] = pipe_enetcv.predict_proba(Xtest)[:,1] \n    ### ridge\n    cr = StandardScaler()\n    Cs_ridge = grille(Xapp,Yapp,\"ridge\")\n    ridgecv = LogisticRegressionCV(cv=10, penalty=\"l2\", \\\n            Cs=Cs_ridge,  max_iter=1000)\n    pipe_ridgecv = Pipeline(steps=[(\"cr\", cr), (\"ridgecv\", ridgecv)])\n    pipe_ridgecv.fit(Xapp,Yapp)\n    PROB.loc[val_index,\"ridge\"] = pipe_ridgecv.predict_proba(Xtest)[:,1]\n```\n:::\n\n\n::: {#047bdc89 .cell execution_count=26}\n``` {.python .cell-code}\nmc = pd.Series(0.0, index=PROB.columns[1:])\ns = 0.5\nfor i in range(mc.shape[0]):\n    mc.iloc[i] = sklm.zero_one_loss(PROB.Y, PROB.iloc[:,i+1]>s)\n\nround(mc.sort_values(ascending=True),3)\n```\n\n::: {.cell-output .cell-output-display execution_count=25}\n```\nBIC      0.266\nlasso    0.266\nridge    0.271\nelast    0.277\nAIC      0.279\nlog      0.286\ndtype: float64\n```\n:::\n:::\n\n\n##  Splines\n\n::: {#dc976f41 .cell execution_count=27}\n``` {.python .cell-code}\nnomsquanti = don.columns[np.isin(don.dtypes, [\"float64\", \\\n                          \"int64\"])].difference([\"Y\"])\nnomsquali = don.columns[np.isin(don.dtypes, [\"object\", \\\n                          \"category\"])].difference([\"Y\"])\nformule = \"~\" + \"+\".join(nomsquali)\nXspline = dmatrix(formule, don, return_type=\"dataframe\").\\\n                                           iloc[:,1:].to_numpy()\nfor i in nomsquanti:\n    xi = don.loc[:,i].quantile([0.25, 0.5, 0.75])\n    formule = \"-1 + bs(\" + i + \",knots=xi, degree=3)\"\n    BX = dmatrix(formule, don, return_type=\"dataframe\").to_numpy()\n    Xspline = np.concatenate((Xspline, BX), axis=1)\n\nXspline.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=26}\n```\n(462, 49)\n```\n:::\n:::\n\n\n::: {#c8941249 .cell execution_count=28}\n``` {.python .cell-code}\ncr = StandardScaler()\nlassocv =  LogisticRegressionCV(cv=10, penalty=\"l1\", n_jobs=10, Cs=Cs_lasso,  solver=\"saga\", max_iter=2000)\nenetcv = LogisticRegressionCV(cv=10, penalty=\"elasticnet\", l1_ratios = [0.5], n_jobs=10,  Cs=Cs_enet, solver=\"saga\", max_iter=2000)\nridgecv = LogisticRegressionCV(cv=10, penalty=\"l2\", n_jobs=10, Cs=Cs_ridge,  max_iter=1000)\npipe_lassocv = Pipeline(steps=[(\"cr\", cr), (\"lassocv\", lassocv)])\npipe_enetcv = Pipeline(steps=[(\"cr\", cr), (\"enetcv\", enetcv)])\npipe_ridgecv = Pipeline(steps=[(\"cr\", cr), (\"ridgecv\", ridgecv)])\n\nnb=10\nskf = StratifiedKFold(n_splits=nb, shuffle=True, random_state=1234)\n```\n:::\n\n\n::: {#320eb919 .cell execution_count=29}\n``` {.python .cell-code}\nfor app_index, val_index in skf.split(X,Y):\n    Xapp = Xspline[app_index,:]\n    Xtest = Xspline[val_index,:]\n    Yapp = Y[app_index]\n    ### log\n    log = LogisticRegression(penalty=None,solver=\"newton-cholesky\").fit(Xapp,Yapp)\n    PROB.loc[val_index,\"log\"] = log.predict_proba(Xtest)[:,1]\n    ### bic\n    choixbic = lss.LogisticRegressionSelectionFeatureIC(start=[], \\\n        direction=\"forward\",crit=\"bic\").fit(Xapp,Yapp)\n    PROB.loc[val_index, \"BIC\"] = choixbic.predict_proba(Xtest)[:,1]\n    ### aic\n    choixaic = lss.LogisticRegressionSelectionFeatureIC(start=[], \\\n        direction=\"forward\",crit=\"aic\").fit(Xapp,Yapp)\n    PROB.loc[val_index, \"AIC\"] = choixaic.predict_proba(Xtest)[:,1]\n    ### lasso\n    cr = StandardScaler()\n    Cs_lasso = grille(Xapp,Yapp, \"lasso\")\n    lassocv =  LogisticRegressionCV(cv=10, penalty=\"l1\", n_jobs=10,\\\n                 Cs=Cs_lasso,  solver=\"saga\", max_iter=2000)\n    pipe_lassocv = Pipeline(steps=[(\"cr\", cr), (\"lassocv\", lassocv)])\n    pipe_lassocv.fit(Xapp,Yapp)\n    PROB.loc[val_index,\"lasso\"] = pipe_lassocv.predict_proba(Xtest)[:,1]\n    ### elastic net\n    cr = StandardScaler()\n    Cs_enet = grille(Xapp,Yapp,\"enet\")\n    enetcv=LogisticRegressionCV(cv=10,penalty=\"elasticnet\",n_jobs=10,\\\n          l1_ratios=[0.5],Cs=Cs_enet,solver=\"saga\",max_iter=2000)\n    pipe_enetcv = Pipeline(steps=[(\"cr\", cr), (\"enetcv\", enetcv)])\n    pipe_enetcv.fit(Xapp,Yapp)\n    PROB.loc[val_index,\"elast\"] = pipe_enetcv.predict_proba(Xtest)[:,1] \n    ### ridge\n    cr = StandardScaler()\n    Cs_ridge = grille(Xapp,Yapp,\"ridge\")\n    ridgecv = LogisticRegressionCV(cv=10, penalty=\"l2\", \\\n            Cs=Cs_ridge,  max_iter=1000)\n    pipe_ridgecv = Pipeline(steps=[(\"cr\", cr), (\"ridgecv\", ridgecv)])\n    pipe_ridgecv.fit(Xapp,Yapp)\n    PROB.loc[val_index,\"ridge\"] = pipe_ridgecv.predict_proba(Xtest)[:,1]\n```\n:::\n\n\n::: {#f0c5c152 .cell execution_count=30}\n``` {.python .cell-code}\nmc = pd.Series(0.0, index=PROB.columns[1:])\ns = 0.5\nfor i in range(mc.shape[0]):\n    mc.iloc[i] = sklm.zero_one_loss(PROB.Y, PROB.iloc[:,i+1]>s)\n\nround(mc.sort_values(ascending=True),3)\n```\n\n::: {.cell-output .cell-output-display execution_count=29}\n```\nridge    0.279\nAIC      0.286\nelast    0.290\nlog      0.292\nlasso    0.297\nBIC      0.305\ndtype: float64\n```\n:::\n:::\n\n\n",
    "supporting": [
      "chap15_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}