[
  {
    "objectID": "codes/chap16.html",
    "href": "codes/chap16.html",
    "title": "16 Données déséquilibrées",
    "section": "",
    "text": "import pandas as pd; import numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\nfrom sklearn.linear_model import LogisticRegression,\\\n    LogisticRegressionCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import StratifiedKFold\nfrom imblearn.under_sampling import RandomUnderSampler, TomekLinks\nfrom imblearn.over_sampling import RandomOverSampler, SMOTE\nfrom sklearn.metrics import accuracy_score, f1_score, \\\n    balanced_accuracy_score, cohen_kappa_score, roc_auc_score\nimport sklearn.metrics as sklm",
    "crumbs": [
      "Codes",
      "IV Le modèle linéaire généralisé",
      "16 Données déséquilibrées"
    ]
  },
  {
    "objectID": "codes/chap16.html#quelques-méthodes-de-rééquilibrages",
    "href": "codes/chap16.html#quelques-méthodes-de-rééquilibrages",
    "title": "16 Données déséquilibrées",
    "section": "Quelques méthodes de rééquilibrages",
    "text": "Quelques méthodes de rééquilibrages\n\ndf = pd.read_csv('../donnees/dd_ex_ech_des1.csv', header=0, sep=';')\ndf.Y.value_counts()\ny = df.Y\nX = df.loc[:,[\"X1\", \"X2\"]]\n\n\nfig = plt.figure()\nplt.plot(df.loc[df.Y==0, \"X1\"], df.loc[df.Y==0, \"X2\"], 'o', df.loc[df.Y==1, \"X1\"], df.loc[df.Y==1, \"X2\"], '^')\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nros1 = RandomOverSampler(random_state=0)\nXreech1, yreech1 = ros1.fit_resample(X, y)\nprint(yreech1.value_counts())\n\nY\n0    80\n1    80\nName: count, dtype: int64\n\n\n\nros2 = RandomOverSampler(random_state=0, sampling_strategy={0: 80, 1: 40})\nXreech2, yreech2 = ros2.fit_resample(X, y)\nprint(yreech2.value_counts())\n\nY\n0    80\n1    40\nName: count, dtype: int64\n\n\n\nover1 = pd.DataFrame(Xreech1)\nover1[\"Y\"] = yreech1\n\n\nsmote1 = SMOTE(random_state=42, k_neighbors=4)\nXreech1, yreech1 = smote1.fit_resample(X, y)\nprint(yreech1.value_counts())\n\nY\n0    80\n1    80\nName: count, dtype: int64\n\n\n\nsmote2 = SMOTE(random_state=423, k_neighbors=4, \\\n               sampling_strategy={0: 80, 1: 40})\nXreech2, yreech2 = smote2.fit_resample(X, y)\nprint(yreech2.value_counts())\n\nY\n0    80\n1    40\nName: count, dtype: int64\n\n\n\ndf1 = Xreech1.assign(Yreech = yreech1)\ntmp = df1.merge(df, how=\"outer\", on=['X1', 'X2'])\nnouv1 = tmp.loc[tmp.Y.isna(), :]\nnouv1.Yreech.value_counts()\n\n\ndf2 = Xreech2.assign(Yreech = yreech2)\ntmp = df2.merge(df, how=\"outer\", on=['X1', 'X2'])\nnouv2 = tmp.loc[tmp.Y.isna(), :]\nnouv2.Yreech.value_counts()\n\n\nplt.rc(\"lines\", markersize=2)\ncoul = [\"C0\", \"C1\"]\nmark = [\"^\", \"o\"]\nfig, (ax1, ax2) = plt.subplots(1,2)\nfor i in range(0,2):\n    ax1.plot(df.loc[df.Y==i, \"X1\"], df.loc[df.Y==i, \"X2\"], marker=mark[i], c=coul[i], ls='')\n    ax1.plot(nouv1.loc[nouv1.Yreech==i, \"X1\"], nouv1.loc[nouv1.Yreech==i, \"X2\"], marker=mark[i], c=coul[i], ls='')\n    ax1.plot(nouv1.loc[nouv1.Yreech==i, \"X1\"], nouv1.loc[nouv1.Yreech==i, \"X2\"],\\\n             marker = mark[1], ms=8, mec=coul[i], mfc='#ffffff00', ls='')\n    ax2.plot(df.loc[df.Y==i, \"X1\"], df.loc[df.Y==i, \"X2\"], marker=mark[i], c=coul[i], ls='')\n    ax2.plot(nouv2.loc[nouv2.Yreech==i, \"X1\"], nouv2.loc[nouv2.Yreech==i, \"X2\"], marker=mark[i], c=coul[i], ls='')\n    ax2.plot(nouv2.loc[nouv2.Yreech==i, \"X1\"], nouv2.loc[nouv2.Yreech==i, \"X2\"],\\\n             marker = mark[1], ms=8, mec=coul[i], mfc=\"#ffffff00\", ls='')\n\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\ny = df.Y\nX = df.loc[:,[\"X1\", \"X2\"]]\nrus1 = RandomUnderSampler(random_state=38)\nXreech1, yreech1 = rus1.fit_resample(X, y)\nprint(yreech1.value_counts())\n\nY\n0    20\n1    20\nName: count, dtype: int64\n\n\n\nrus2 = RandomUnderSampler(random_state=38, sampling_strategy={0: 40, 1: 20})\nXreech2, yreech2 = rus2.fit_resample(X, y)\nprint(yreech2.value_counts())\n\nY\n0    40\n1    20\nName: count, dtype: int64\n\n\n\ntl1 = TomekLinks(sampling_strategy='all')\nXreech1, yreech1 = tl1.fit_resample(X, y)\nprint(yreech1.value_counts())\n\nY\n0    76\n1    16\nName: count, dtype: int64\n\n\n\ntl2 = TomekLinks(sampling_strategy='majority')\nXreech2, yreech2 = tl2.fit_resample(X, y)\nprint(yreech2.value_counts())\n\nY\n0    76\n1    20\nName: count, dtype: int64\n\n\n\ndf1 = Xreech1.assign(Yreech = yreech1)\ntmp = df.merge(df1, how=\"outer\", on=['X1', 'X2'])\nnouv1 = tmp.loc[tmp.Yreech.isna(), :]\nnouv1.Y.value_counts()\n\ndf2 = Xreech2.assign(Yreech = yreech2)\ntmp = df.merge(df2, how=\"outer\", on=['X1', 'X2'])\nnouv2 = tmp.loc[tmp.Yreech.isna(), :]\nnouv2.Y.value_counts()\n\n\nplt.rc(\"lines\", markersize=3)\ncoul = [\"C0\", \"C1\"]\nmark = [\"^\", \"o\"]\nfig, (ax1, ax2) = plt.subplots(1,2)\nfor i in range(0,2):\n    ax1.plot(df.loc[df.Y==i, \"X1\"], df.loc[df.Y==i, \"X2\"], marker=mark[i], c=coul[i], ls='')\n    ax1.plot(nouv1.loc[nouv1.Y==i, \"X1\"], nouv1.loc[nouv1.Y==i, \"X2\"], marker=mark[i], c=coul[i], ls='')\n    ax1.plot(nouv1.loc[nouv1.Y==i, \"X1\"], nouv1.loc[nouv1.Y==i, \"X2\"],\\\n             marker = mark[1], ms=8, mec=coul[i], mfc='#ffffff00', ls='')\n    ax2.plot(df.loc[df.Y==i, \"X1\"], df.loc[df.Y==i, \"X2\"], marker=mark[i], c=coul[i], ls='')\n    ax2.plot(nouv2.loc[nouv2.Y==i, \"X1\"], nouv2.loc[nouv2.Y==i, \"X2\"], marker=mark[i], c=coul[i], ls='')\n    ax2.plot(nouv2.loc[nouv2.Y==i, \"X1\"], nouv2.loc[nouv2.Y==i, \"X2\"],\\\n             marker = mark[1], ms=8, mec=coul[i], mfc=\"#ffffff00\", ls='')\n\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\ncoul = [\"C0\", \"C1\"]\nmark = [\"^\", \"o\"]\nfig, (ax1, ax2) = plt.subplots(1,2)\nfor i in range(0,2):\n    ax1.plot(df1.loc[df1.Yreech==i, \"X1\"], df1.loc[df1.Yreech==i, \"X2\"], marker=mark[i], c=coul[i], ls='')\n    ax2.plot(df2.loc[df2.Yreech==i, \"X1\"], df2.loc[df2.Yreech==i, \"X2\"], marker=mark[i], c=coul[i], ls='')\n\nfig.tight_layout()",
    "crumbs": [
      "Codes",
      "IV Le modèle linéaire généralisé",
      "16 Données déséquilibrées"
    ]
  },
  {
    "objectID": "codes/chap16.html#critères-pour-données-déséquilibrées",
    "href": "codes/chap16.html#critères-pour-données-déséquilibrées",
    "title": "16 Données déséquilibrées",
    "section": "Critères pour données déséquilibrées",
    "text": "Critères pour données déséquilibrées\n\ndf = pd.read_csv(\"../donnees/donnees_dondesequilib.csv\", header=0, sep=';')\nprint(pd.crosstab(index=df.Y, columns=df.P1))\n\nP1    0  1\nY         \n0   468  0\n1    31  1\n\n\n\nprint(pd.crosstab(index=df.Y, columns=df.P2))\n\nP2    0   1\nY          \n0   407  61\n1     4  28\n\n\n\nfrom sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, cohen_kappa_score\nprint(np.round(accuracy_score(df.Y, df.P2), 3))\nprint(np.round(balanced_accuracy_score(df.Y, df.P2), 3))\nprint(np.round(f1_score(df.Y, df.P2), 3))\nprint(np.round(cohen_kappa_score(df.Y, df.P2), 3))\n\n0.87\n0.872\n0.463\n0.407",
    "crumbs": [
      "Codes",
      "IV Le modèle linéaire généralisé",
      "16 Données déséquilibrées"
    ]
  },
  {
    "objectID": "codes/chap12.html",
    "href": "codes/chap12.html",
    "title": "12 Régression logistique",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\nimport statsmodels.regression.linear_model as smlm\nfrom patsy import dmatrix\nfrom scipy.stats import norm, chi2\nimport sys\nsys.path.append('../modules')\nimport choixglmstats\n\n\nPrésentation du modèle\n\nartere = pd.read_csv('../donnees/artere.txt', header=0, index_col=0, sep=' ')\n\n\nfig = plt.figure()\nplt.plot(artere.age, artere.chd, 'o')\nplt.ylabel('chd')\nplt.xlabel('age')\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nartere_summary = pd.crosstab(artere.agrp, artere.chd)\nartere_summary.columns = ['chd0', 'chd1']\nartere_summary['Effectifs'] = artere_summary.chd0 + artere_summary.chd1\nartere_summary['Frequence'] = artere_summary.chd1 / artere_summary.Effectifs\nartere_summary['age_min'] = [artere[artere.agrp == agrp].age.min() - 1 for agrp in artere_summary.index]\nartere_summary['age_min'] = artere.groupby([\"agrp\"]).age.min() +1\nartere_summary['age_max'] = [artere[artere.agrp == agrp].age.max() for agrp in artere_summary.index]\nartere_summary['age_max'] = artere.groupby([\"agrp\"]).age.max()\nartere_summary['age'] = [f']{artere_summary.age_min[i]};{artere_summary.age_max[i]}]' for i in artere_summary.index]\nartere_summary['age'] = \"]\" + artere_summary.age_min.astype(str) + \";\" + artere_summary.age_max.astype(str) + \"]\"\nartere_summary.reset_index()[['age', 'Effectifs', 'chd0', 'chd1', 'Frequence']].to_string(index=False)\n\n\nfig = plt.figure()\nplt.plot(artere.age, artere.chd, 'o')\nplt.ylabel('chd')\nplt.xlabel('age')\nplt.hlines(artere_summary.Frequence,artere_summary.age_min, artere_summary.age_max, 'k')\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nmodele = smf.glm('chd~age', data=artere, family=sm.families.Binomial()).fit()\nprint(modele.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                    chd   No. Observations:                  100\nModel:                            GLM   Df Residuals:                       98\nModel Family:                Binomial   Df Model:                            1\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -53.677\nDate:                Tue, 04 Feb 2025   Deviance:                       107.35\nTime:                        18:32:49   Pearson chi2:                     102.\nNo. Iterations:                     4   Pseudo R-squ. (CS):             0.2541\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -5.3095      1.134     -4.683      0.000      -7.531      -3.088\nage            0.1109      0.024      4.610      0.000       0.064       0.158\n==============================================================================\n\n\n\nplt.plot(artere.age, artere.chd, 'o')\nplt.ylabel('chd')\nplt.xlabel('age')\nplt.hlines(artere_summary.Frequence, artere_summary.age_min, artere_summary.age_max, 'k')\nx = np.arange(artere_summary.age_min.min(), artere_summary.age_max.max(), step=0.01)\ny = np.exp(modele.params.Intercept + modele.params.age * x) / (1.0 + np.exp(modele.params.Intercept + modele.params.age * x))\nplt.plot(x, y, 'b--')\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nX = np.random.choice(['A', 'B', 'C'], 100)\nY = np.zeros(X.shape, dtype=int)\nY[X=='A'] = np.random.binomial(1, 0.9, (X=='A').sum())\nY[X=='B'] = np.random.binomial(1, 0.1, (X=='B').sum())\nY[X=='C'] = np.random.binomial(1, 0.9, (X=='C').sum())\ndon = pd.DataFrame({'X': X, 'Y': Y})\n\n\nmod = smf.glm(\"Y~X\", data=don, family=sm.families.Binomial()).fit()\nprint(mod.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                      Y   No. Observations:                  100\nModel:                            GLM   Df Residuals:                       97\nModel Family:                Binomial   Df Model:                            2\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -13.869\nDate:                Tue, 04 Feb 2025   Deviance:                       27.738\nTime:                        18:32:49   Pearson chi2:                     32.0\nNo. Iterations:                    23   Pseudo R-squ. (CS):             0.6689\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     24.5661   2.57e+04      0.001      0.999   -5.03e+04    5.04e+04\nX[T.B]       -49.1321   3.27e+04     -0.002      0.999   -6.41e+04     6.4e+04\nX[T.C]       -22.8797   2.57e+04     -0.001      0.999   -5.04e+04    5.03e+04\n==============================================================================\n\n\n\nmod1 = smf.glm(\"Y~C(X, Sum)\", data=don, family=sm.families.Binomial()).fit()\nprint(mod1.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                      Y   No. Observations:                  100\nModel:                            GLM   Df Residuals:                       97\nModel Family:                Binomial   Df Model:                            2\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -13.869\nDate:                Tue, 04 Feb 2025   Deviance:                       27.738\nTime:                        18:32:49   Pearson chi2:                     32.0\nNo. Iterations:                    23   Pseudo R-squ. (CS):             0.6689\nCovariance Type:            nonrobust                                         \n==================================================================================\n                     coef    std err          z      P&gt;|z|      [0.025      0.975]\n----------------------------------------------------------------------------------\nIntercept          0.5621   1.09e+04   5.16e-05      1.000   -2.14e+04    2.14e+04\nC(X, Sum)[S.A]    24.0039   1.84e+04      0.001      0.999   -3.61e+04    3.61e+04\nC(X, Sum)[S.B]   -25.1282    1.6e+04     -0.002      0.999   -3.13e+04    3.13e+04\n==================================================================================\n\n\n\n\nEstimation\n\nSAh = pd.read_csv(\"../donnees/SAh.csv\", header=0, sep=\",\")\nnewSAh = SAh.iloc[[1,407,34],]\nnewSAh = newSAh.reset_index().drop(\"index\",axis=1)\nSAh = SAh.drop([1,407,34]).reset_index().drop(\"index\",axis=1)\n\n\nform =\"chd ~ \" + \"+\".join(SAh.columns[ :-1 ])\nmod = smf.glm(form, data=SAh, family=sm.families.Binomial()).fit()\nmod.summary()\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\nchd\nNo. Observations:\n459\n\n\nModel:\nGLM\nDf Residuals:\n449\n\n\nModel Family:\nBinomial\nDf Model:\n9\n\n\nLink Function:\nLogit\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-234.36\n\n\nDate:\nTue, 04 Feb 2025\nDeviance:\n468.72\n\n\nTime:\n18:32:49\nPearson chi2:\n449.\n\n\nNo. Iterations:\n5\nPseudo R-squ. (CS):\n0.2339\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-6.0837\n1.314\n-4.629\n0.000\n-8.659\n-3.508\n\n\nfamhist[T.Present]\n0.9325\n0.229\n4.069\n0.000\n0.483\n1.382\n\n\nsbp\n0.0065\n0.006\n1.127\n0.260\n-0.005\n0.018\n\n\ntobacco\n0.0814\n0.027\n3.023\n0.003\n0.029\n0.134\n\n\nldl\n0.1794\n0.060\n2.989\n0.003\n0.062\n0.297\n\n\nadiposity\n0.0184\n0.030\n0.622\n0.534\n-0.039\n0.076\n\n\ntypea\n0.0392\n0.012\n3.184\n0.001\n0.015\n0.063\n\n\nobesity\n-0.0637\n0.045\n-1.430\n0.153\n-0.151\n0.024\n\n\nalcohol\n0.0002\n0.004\n0.035\n0.972\n-0.009\n0.009\n\n\nage\n0.0439\n0.012\n3.592\n0.000\n0.020\n0.068\n\n\n\n\n\n\nprint(mod.conf_int(alpha=0.05))\n\n                           0         1\nIntercept          -8.659355 -3.507984\nfamhist[T.Present]  0.483354  1.381573\nsbp                -0.004798  0.017773\ntobacco             0.028628  0.134174\nldl                 0.061771  0.297043\nadiposity          -0.039461  0.076187\ntypea               0.015090  0.063396\nobesity            -0.151055  0.023612\nalcohol            -0.008640  0.008950\nage                 0.019931  0.067793\n\n\n\ndon = pd.read_csv(\"../donnees/logit_donnees.csv\", sep=\",\", header=0)\nmodsim = smf.logit(\"Y ~ X1 + X2 + X3\", data=don).fit()\n\nOptimization terminated successfully.\n         Current function value: 0.366463\n         Iterations 7\n\n\n\nmodsim.wald_test_terms()\n\n&lt;class 'statsmodels.stats.contrast.WaldTestResults'&gt;\n                             chi2                 P&gt;chi2  df constraint\nIntercept   [[28.46981615119592]]  9.517067345514757e-08              1\nX1         [[212.50601519640435]]  7.159869628652175e-47              2\nX2         [[210.39004424749865]]  1.129196632385863e-47              1\nX3         [[0.3095790886927727]]     0.5779385882309931              1\n\n\n\nmodsim01 = smf.logit(\"Y~X2+X3\",data=don).fit()\nmodsim02 = smf.logit(\"Y~X1+X3\",data=don).fit()\nmodsim03 = smf.logit(\"Y~X1+X2\",data=don).fit()\n\nOptimization terminated successfully.\n         Current function value: 0.554834\n         Iterations 6\nOptimization terminated successfully.\n         Current function value: 0.575293\n         Iterations 5\nOptimization terminated successfully.\n         Current function value: 0.366618\n         Iterations 7\n\n\n\nimport statsmodels.regression.linear_model as smlm\nsmlm.RegressionResults.compare_lr_test(modsim,modsim01)\n\n(376.7417147005359, 1.554447652669738e-82, 2.0)\n\n\n\nsmlm.RegressionResults.compare_lr_test(modsim,modsim02)\n\n(417.66072160440774, 7.881723945480014e-93, 1.0)\n\n\n\nsmlm.RegressionResults.compare_lr_test(modsim,modsim03)\n\n(0.3097619772612461, 0.5778262823265808, 1.0)\n\n\n\nprint(newSAh)\n\n   sbp  tobacco   ldl  adiposity  famhist  typea  obesity  alcohol  age  chd\n0  144     0.01  4.41      28.61   Absent     55    28.87     2.06   63    1\n1  200    19.20  4.43      40.60  Present     55    32.04    36.00   60    1\n2  148     5.50  7.10      25.31   Absent     56    29.84     3.60   48    0\n\n\n\nprint(mod.predict(newSAh))\n\n0    0.320889\n1    0.881177\n2    0.369329\ndtype: float64\n\n\n\nvarbetac = mod.cov_params().values\nbetac = mod.params.values\nff = mod.model.formula.split(\"~\")[1]\nxetoile =  dmatrix(\"~\"+ff, data=newSAh, return_type=\"dataframe\").to_numpy()\nprev_fit = np.dot(xetoile,betac)\nprev_se = np.diag(np.dot(np.dot(xetoile,varbetac), np.transpose(xetoile)))**0.5\ncl_inf = prev_fit-norm.ppf(0.975)*prev_se\ncl_sup = prev_fit+norm.ppf(0.975)*prev_se\nbinf = np.exp(cl_inf)/(1+np.exp(cl_inf))\nbsup = np.exp(cl_sup)/(1+np.exp(cl_sup))\nprint(pd.DataFrame({\"binf\": binf, \"bsup\": bsup}))\n\n       binf      bsup\n0  0.199717  0.472200\n1  0.713881  0.956601\n2  0.246181  0.512220\n\n\n\ng = artere.groupby([\"age\"])\ndfsat = pd.concat([g[\"chd\"].mean(), g[\"chd\"].count()], axis=1)\ndfsat.columns = [\"p\", \"n\"]\nprint(dfsat.iloc[0:5])\n\n       p  n\nage        \n20   0.0  1\n23   0.0  1\n24   0.0  1\n25   0.5  2\n26   0.0  2\n\n\n\nplt.plot(artere.age, artere.chd, 'o', dfsat.index, dfsat.p, \"-\")\nplt.ylabel('chd')\nplt.xlabel('age')\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nK=10\najust = pd.DataFrame({\"ajust\": mod.predict()}, index=SAh.index)\najust[\"Y\"] = SAh[\"chd\"]\najust['decile'] = pd.qcut(ajust[\"ajust\"], K)\nok = ajust['Y'].groupby(ajust.decile).sum()\nmuk = ajust[\"ajust\"].groupby(ajust.decile).mean()\nmk = ajust['Y'].groupby(ajust.decile).count()\nC2 = ((ok - mk*muk)**2/(mk*muk*(1-muk))).sum()\n\n\nprint('chi-square: {:.3f}'.format(C2))\n\nchi-square: 6.659\n\n\n\npvalue=1-chi2.cdf(C2, K-2)\nprint('p-value: {:.3f}'.format(pvalue))\n\np-value: 0.574\n\n\n\nform =\"chd ~ \" + \"+\".join(SAh.columns[ :-1 ])\nmod = smf.glm(form, data=SAh, family=sm.families.Binomial()).fit()\n\n\nresdev = mod.resid_deviance/np.sqrt(1-mod.get_hat_matrix_diag())\nrespea = mod.resid_pearson/np.sqrt(1-mod.get_hat_matrix_diag())\n\n\nfig = plt.figure()\nplt.plot(resdev, 'o')\nplt.ylabel('residus deviance')\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nfig = plt.figure()\nplt.plot(respea, 'o')\nplt.ylabel('residus Pearson')\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\n\nChoix de variables\n\nmod0 = smf.glm(\"chd~sbp+ldl\", data=SAh, family=sm.families.Binomial()).fit()\nmod1 = smf.glm(\"chd~sbp+ldl+famhist+alcohol\", data=SAh, family=sm.families.Binomial()).fit()\ndef lr_test(restr, full):\n    from scipy import stats\n    lr_df = (restr.df_resid - full.df_resid)\n    lr_stat = -2*(restr.llf - full.llf)\n    lr_pvalue = stats.chi2.sf(lr_stat, df=lr_df)\n    return {\"lr\": lr_stat, \"pvalue\": lr_pvalue, \"df\": lr_df}\n\nlr_test(mod0, mod1)\n\n{'lr': 25.5447172394405, 'pvalue': 2.838148600168801e-06, 'df': 2}\n\n\n\nmod_sel = choixglmstats.bestglm(SAh, upper=form)\n\n\nprint(mod_sel.sort_values(by=[\"BIC\",\"nb_var\"]).iloc[:5,[1,3]])\n\n                                        var_added         BIC\n176           (tobacco, famhist, ldl, typea, age)  509.100392\n287                (tobacco, famhist, typea, age)  512.495444\n284                  (tobacco, famhist, ldl, age)  512.537897\n91   (tobacco, famhist, ldl, typea, obesity, age)  513.424654\n358                    (famhist, ldl, typea, age)  513.471151\n\n\n\nprint(mod_sel.sort_values(by=[\"AIC\",\"nb_var\"]).iloc[:5,[1,2]])\n\n                                             var_added         AIC\n176                (tobacco, famhist, ldl, typea, age)  484.326091\n91        (tobacco, famhist, ldl, typea, obesity, age)  484.521302\n36   (tobacco, famhist, ldl, typea, obesity, age, sbp)  485.117363\n93            (tobacco, famhist, ldl, typea, age, sbp)  485.311962\n31   (tobacco, famhist, adiposity, ldl, typea, obes...  486.031360\n\n\n\n\n\n\n Retour au sommet",
    "crumbs": [
      "Codes",
      "IV Le modèle linéaire généralisé",
      "12 Régression logistique"
    ]
  },
  {
    "objectID": "codes/chap7.html",
    "href": "codes/chap7.html",
    "title": "Variables qualitatives : ANCOVA et ANOVA",
    "section": "",
    "text": "import pandas as pd; import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport seaborn as sns\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom statsmodels.graphics import regressionplots\nfrom statsmodels.graphics.api import interaction_plot\nimport pandas as pd\n\n\nfig = plt.figure()\neucalypt = pd.read_csv(\"../donnees/eucalyptus.txt\", header = 0, sep = \";\")\neucalypt[\"bloc\"] = eucalypt[\"bloc\"].astype(\"category\")\nsns.scatterplot(data=eucalypt, x=\"circ\", y=\"ht\", hue=\"bloc\",style='bloc')\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nLa concentration en ozone\n\nozone = pd.read_csv(\"../donnees/ozone.txt\", header = 0, sep = \";\")\nozone[\"vent\"]=ozone[\"vent\"].astype(\"category\")\n\n\nniveau = ozone[\"vent\"].cat.categories\ncc = cm.Set1(range(niveau.size))\nmm = [\"o\",\"*\",\"8\",\"s\"]\nfig, ax = plt.subplots()\nfor i, val in enumerate(niveau):\n    print(val)\n    reg = smf.ols(\"O3 ~ 1 + T12\",data=ozone.loc[ozone[\"vent\"]==val]).fit()\n    plt.scatter(ozone.loc[ozone[\"vent\"]==val,\"T12\"],ozone.loc[ozone[\"vent\"]==val,\"O3\"],\n                color=cc[i],label=val, marker=mm[i])\n    regressionplots.abline_plot(model_results=reg, ax=ax, color=cc[i])\n\nplt.legend()\nfig.tight_layout()\n\nEST\nNORD\nOUEST\nSUD\n\n\n\n\n\n\n\n\n\n\nmod1b = smf.ols('O3 ~ -1 + vent + T12:vent', data = ozone).fit()\nmod1b.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nO3\nR-squared:\n0.675\n\n\nModel:\nOLS\nAdj. R-squared:\n0.621\n\n\nMethod:\nLeast Squares\nF-statistic:\n12.48\n\n\nDate:\nFri, 31 Jan 2025\nProb (F-statistic):\n1.61e-08\n\n\nTime:\n16:29:15\nLog-Likelihood:\n-201.01\n\n\nNo. Observations:\n50\nAIC:\n418.0\n\n\nDf Residuals:\n42\nBIC:\n433.3\n\n\nDf Model:\n7\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nvent[EST]\n45.6090\n13.934\n3.273\n0.002\n17.488\n73.730\n\n\nvent[NORD]\n106.6345\n28.034\n3.804\n0.000\n50.059\n163.209\n\n\nvent[OUEST]\n64.6840\n24.621\n2.627\n0.012\n14.997\n114.371\n\n\nvent[SUD]\n-27.0602\n26.539\n-1.020\n0.314\n-80.618\n26.498\n\n\nT12:vent[EST]\n2.7480\n0.634\n4.333\n0.000\n1.468\n4.028\n\n\nT12:vent[NORD]\n-1.6491\n1.606\n-1.027\n0.310\n-4.890\n1.592\n\n\nT12:vent[OUEST]\n0.3407\n1.205\n0.283\n0.779\n-2.091\n2.772\n\n\nT12:vent[SUD]\n5.3786\n1.150\n4.678\n0.000\n3.058\n7.699\n\n\n\n\n\n\n\n\nOmnibus:\n0.276\nDurbin-Watson:\n1.561\n\n\nProb(Omnibus):\n0.871\nJarque-Bera (JB):\n0.465\n\n\nSkew:\n0.007\nProb(JB):\n0.793\n\n\nKurtosis:\n2.528\nCond. No.\n168.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nmod1 =smf.ols('O3 ~ vent + T12:vent', data = ozone).fit()\nmod1.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nO3\nR-squared:\n0.675\n\n\nModel:\nOLS\nAdj. R-squared:\n0.621\n\n\nMethod:\nLeast Squares\nF-statistic:\n12.48\n\n\nDate:\nFri, 31 Jan 2025\nProb (F-statistic):\n1.61e-08\n\n\nTime:\n16:29:15\nLog-Likelihood:\n-201.01\n\n\nNo. Observations:\n50\nAIC:\n418.0\n\n\nDf Residuals:\n42\nBIC:\n433.3\n\n\nDf Model:\n7\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n45.6090\n13.934\n3.273\n0.002\n17.488\n73.730\n\n\nvent[T.NORD]\n61.0255\n31.306\n1.949\n0.058\n-2.153\n124.204\n\n\nvent[T.OUEST]\n19.0751\n28.290\n0.674\n0.504\n-38.017\n76.168\n\n\nvent[T.SUD]\n-72.6691\n29.975\n-2.424\n0.020\n-133.160\n-12.178\n\n\nT12:vent[EST]\n2.7480\n0.634\n4.333\n0.000\n1.468\n4.028\n\n\nT12:vent[NORD]\n-1.6491\n1.606\n-1.027\n0.310\n-4.890\n1.592\n\n\nT12:vent[OUEST]\n0.3407\n1.205\n0.283\n0.779\n-2.091\n2.772\n\n\nT12:vent[SUD]\n5.3786\n1.150\n4.678\n0.000\n3.058\n7.699\n\n\n\n\n\n\n\n\nOmnibus:\n0.276\nDurbin-Watson:\n1.561\n\n\nProb(Omnibus):\n0.871\nJarque-Bera (JB):\n0.465\n\n\nSkew:\n0.007\nProb(JB):\n0.793\n\n\nKurtosis:\n2.528\nCond. No.\n223.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nmod2 = smf.ols('O3 ~ vent + T12', data = ozone).fit()\nmod2b = smf.ols('O3 ~ -1 + vent + T12', data = ozone).fit()\nmod3 = smf.ols('O3 ~ vent:T12', data = ozone).fit()\n\nround(sm.stats.anova_lm(mod2,mod1),3)\n\n\n\n\n\n\n\n\ndf_resid\nssr\ndf_diff\nss_diff\nF\nPr(&gt;F)\n\n\n\n\n0\n45.0\n12611.951\n0.0\nNaN\nNaN\nNaN\n\n\n1\n42.0\n9087.431\n3.0\n3524.521\n5.43\n0.003\n\n\n\n\n\n\n\n\nround(sm.stats.anova_lm(mod3,mod1),3)\n\n\n\n\n\n\n\n\ndf_resid\nssr\ndf_diff\nss_diff\nF\nPr(&gt;F)\n\n\n\n\n0\n45.0\n11864.057\n0.0\nNaN\nNaN\nNaN\n\n\n1\n42.0\n9087.431\n3.0\n2776.626\n4.278\n0.01\n\n\n\n\n\n\n\n\ninfl = mod2.get_influence()\nfig = plt.figure()\nplt.plot(mod2.fittedvalues,infl.resid_studentized_external,\".\")\nplt.ylabel('résidus')\nplt.xlabel(r'$\\hat Y$')\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nfitted2 = mod2.fittedvalues\nsns.set(style=\"whitegrid\")\nfitted3 = 1/5* abs(fitted2.round(3))\ndfresid = pd.concat([fitted2, pd.Series(infl.resid_studentized_external), ozone[[\"O3\", \"vent\"]]], axis=1)\ndfresid.columns=[\"residus\", \"ajustement\", \"O3\", \"vent\"]\ng = sns.FacetGrid(dfresid, col=\"vent\")\ng = g.map(plt.scatter, \"ajustement\", \"residus\")\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nmod = smf.ols('O3 ~ vent + T12 + T12:vent', data = ozone).fit()\nmod.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nO3\nR-squared:\n0.675\n\n\nModel:\nOLS\nAdj. R-squared:\n0.621\n\n\nMethod:\nLeast Squares\nF-statistic:\n12.48\n\n\nDate:\nFri, 31 Jan 2025\nProb (F-statistic):\n1.61e-08\n\n\nTime:\n16:29:16\nLog-Likelihood:\n-201.01\n\n\nNo. Observations:\n50\nAIC:\n418.0\n\n\nDf Residuals:\n42\nBIC:\n433.3\n\n\nDf Model:\n7\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n45.6090\n13.934\n3.273\n0.002\n17.488\n73.730\n\n\nvent[T.NORD]\n61.0255\n31.306\n1.949\n0.058\n-2.153\n124.204\n\n\nvent[T.OUEST]\n19.0751\n28.290\n0.674\n0.504\n-38.017\n76.168\n\n\nvent[T.SUD]\n-72.6691\n29.975\n-2.424\n0.020\n-133.160\n-12.178\n\n\nT12\n2.7480\n0.634\n4.333\n0.000\n1.468\n4.028\n\n\nT12:vent[T.NORD]\n-4.3971\n1.726\n-2.547\n0.015\n-7.881\n-0.913\n\n\nT12:vent[T.OUEST]\n-2.4073\n1.361\n-1.768\n0.084\n-5.155\n0.340\n\n\nT12:vent[T.SUD]\n2.6306\n1.313\n2.004\n0.052\n-0.019\n5.280\n\n\n\n\n\n\n\n\nOmnibus:\n0.276\nDurbin-Watson:\n1.561\n\n\nProb(Omnibus):\n0.871\nJarque-Bera (JB):\n0.465\n\n\nSkew:\n0.007\nProb(JB):\n0.793\n\n\nKurtosis:\n2.528\nCond. No.\n407.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\nLa hauteur des eucalyptus\n\neucalypt = pd.read_csv(\"../donnees/eucalyptus.txt\", header = 0, sep = \";\")\neucalypt[\"bloc\"] = eucalypt[\"bloc\"].astype(\"category\")\nm_complet = smf.ols(\"ht ~ - 1 + bloc + bloc:circ\", data = eucalypt).fit()\nm_pente = smf.ols(\"ht ~  - 1 + bloc + circ\", data = eucalypt).fit()\nm_ordonne = smf.ols(\"ht ~ 1 + bloc:circ\", data = eucalypt).fit()\nsm.stats.anova_lm(m_pente, m_complet)\n\n\n\n\n\n\n\n\ndf_resid\nssr\ndf_diff\nss_diff\nF\nPr(&gt;F)\n\n\n\n\n0\n1425.0\n2005.895987\n0.0\nNaN\nNaN\nNaN\n\n\n1\n1423.0\n2005.048468\n2.0\n0.847519\n0.300746\n0.740313\n\n\n\n\n\n\n\n\nround(sm.stats.anova_lm(m_ordonne, m_complet),3)\n\n\n\n\n\n\n\n\ndf_resid\nssr\ndf_diff\nss_diff\nF\nPr(&gt;F)\n\n\n\n\n0\n1425.0\n2009.213\n0.0\nNaN\nNaN\nNaN\n\n\n1\n1423.0\n2005.048\n2.0\n4.165\n1.478\n0.228\n\n\n\n\n\n\n\n\nm_simple = smf.ols(\"ht ~ 1 + circ\", data = eucalypt).fit()\nround(sm.stats.anova_lm(m_simple,m_pente),3)\n\n\n\n\n\n\n\n\ndf_resid\nssr\ndf_diff\nss_diff\nF\nPr(&gt;F)\n\n\n\n\n0\n1427.0\n2052.084\n0.0\nNaN\nNaN\nNaN\n\n\n1\n1425.0\n2005.896\n2.0\n46.188\n16.406\n0.0\n\n\n\n\n\n\n\n\nplt.rc(\"lines\", markersize=3) #\ninfl = m_pente.get_influence()\nfig = plt.figure()\nplt.plot(m_pente.fittedvalues,infl.resid_studentized_external,\".\")\nplt.ylabel('résidus')\nplt.xlabel(r'$\\hat Y$')\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\n\nANOVA\n\nniveau = ozone[\"vent\"].cat.categories\nO3_parvent = []\nfor i in range(niveau.size):\n    O3_parvent.append(list(ozone.loc[ozone[\"vent\"]==niveau[i],\"O3\"]))\n    \nfig, ax = plt.subplots(1,1)\nbplot = ax.boxplot(O3_parvent,patch_artist=True,tick_labels=niveau )\nfor patch in bplot[\"boxes\"]:\n     patch.set_facecolor(\"#5875a4\")\n\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nmod1 = smf.ols(\"O3~vent-1\",data=ozone).fit()\nmod1.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nO3\nR-squared:\n0.352\n\n\nModel:\nOLS\nAdj. R-squared:\n0.310\n\n\nMethod:\nLeast Squares\nF-statistic:\n8.338\n\n\nDate:\nFri, 31 Jan 2025\nProb (F-statistic):\n0.000156\n\n\nTime:\n16:29:18\nLog-Likelihood:\n-218.28\n\n\nNo. Observations:\n50\nAIC:\n444.6\n\n\nDf Residuals:\n46\nBIC:\n452.2\n\n\nDf Model:\n3\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nvent[EST]\n103.8500\n4.963\n20.923\n0.000\n93.859\n113.841\n\n\nvent[NORD]\n78.2889\n6.618\n11.830\n0.000\n64.968\n91.610\n\n\nvent[OUEST]\n71.5778\n4.680\n15.296\n0.000\n62.158\n80.997\n\n\nvent[SUD]\n94.3429\n7.504\n12.572\n0.000\n79.238\n109.447\n\n\n\n\n\n\n\n\nOmnibus:\n1.682\nDurbin-Watson:\n1.737\n\n\nProb(Omnibus):\n0.431\nJarque-Bera (JB):\n1.184\n\n\nSkew:\n0.083\nProb(JB):\n0.553\n\n\nKurtosis:\n2.264\nCond. No.\n1.60\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nround(sm.stats.anova_lm(mod1),3)\n\n\n\n\n\n\n\n\ndf\nsum_sq\nmean_sq\nF\nPR(&gt;F)\n\n\n\n\nvent\n4.0\n382244.343\n95561.086\n242.442\n0.0\n\n\nResidual\n46.0\n18131.377\n394.160\nNaN\nNaN\n\n\n\n\n\n\n\n\nmod2 = smf.ols(\"O3 ~ vent\", data = ozone).fit()\nmod2.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nO3\nR-squared:\n0.352\n\n\nModel:\nOLS\nAdj. R-squared:\n0.310\n\n\nMethod:\nLeast Squares\nF-statistic:\n8.338\n\n\nDate:\nFri, 31 Jan 2025\nProb (F-statistic):\n0.000156\n\n\nTime:\n16:29:18\nLog-Likelihood:\n-218.28\n\n\nNo. Observations:\n50\nAIC:\n444.6\n\n\nDf Residuals:\n46\nBIC:\n452.2\n\n\nDf Model:\n3\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n103.8500\n4.963\n20.923\n0.000\n93.859\n113.841\n\n\nvent[T.NORD]\n-25.5611\n8.272\n-3.090\n0.003\n-42.212\n-8.910\n\n\nvent[T.OUEST]\n-32.2722\n6.821\n-4.731\n0.000\n-46.003\n-18.541\n\n\nvent[T.SUD]\n-9.5071\n8.997\n-1.057\n0.296\n-27.617\n8.603\n\n\n\n\n\n\n\n\nOmnibus:\n1.682\nDurbin-Watson:\n1.737\n\n\nProb(Omnibus):\n0.431\nJarque-Bera (JB):\n1.184\n\n\nSkew:\n0.083\nProb(JB):\n0.553\n\n\nKurtosis:\n2.264\nCond. No.\n4.51\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nround(sm.stats.anova_lm(mod2),3)\n\n\n\n\n\n\n\n\ndf\nsum_sq\nmean_sq\nF\nPR(&gt;F)\n\n\n\n\nvent\n3.0\n9859.843\n3286.614\n8.338\n0.0\n\n\nResidual\n46.0\n18131.377\n394.160\nNaN\nNaN\n\n\n\n\n\n\n\n\nsmf.ols(\"O3 ~ C(vent,Treatment)\", data = ozone).fit().params\n\nIntercept                      103.850000\nC(vent, Treatment)[T.NORD]     -25.561111\nC(vent, Treatment)[T.OUEST]    -32.272222\nC(vent, Treatment)[T.SUD]       -9.507143\ndtype: float64\n\n\n\nsmf.ols(\"O3 ~ C(vent, levels=['NORD', 'EST', 'OUEST', 'SUD'])\", \\\n    data = ozone).fit().params\n\nIntercept                                                   78.288889\nC(vent, levels=['NORD', 'EST', 'OUEST', 'SUD'])[T.EST]      25.561111\nC(vent, levels=['NORD', 'EST', 'OUEST', 'SUD'])[T.OUEST]    -6.711111\nC(vent, levels=['NORD', 'EST', 'OUEST', 'SUD'])[T.SUD]      16.053968\ndtype: float64\n\n\n\nfrom patsy import dmatrix, ContrastMatrix\nII = ozone[\"vent\"].cat.categories.size\nnI = ozone[\"vent\"].value_counts()[ozone[\"vent\"].cat.categories]\ncontr_mat = np.vstack([np.eye(II-1), (-nI[:(II-1)]).divide(nI[-1])])\ncontraste = ContrastMatrix(contr_mat, [\"[a1]\", \"[a2]\", \"[a3]\"])\nmod3 = smf.ols(\"O3 ~ 1 + C(vent,contraste)\",data=ozone).fit()\nmod3.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nO3\nR-squared:\n0.352\n\n\nModel:\nOLS\nAdj. R-squared:\n0.310\n\n\nMethod:\nLeast Squares\nF-statistic:\n8.338\n\n\nDate:\nFri, 31 Jan 2025\nProb (F-statistic):\n0.000156\n\n\nTime:\n16:29:18\nLog-Likelihood:\n-218.28\n\n\nNo. Observations:\n50\nAIC:\n444.6\n\n\nDf Residuals:\n46\nBIC:\n452.2\n\n\nDf Model:\n3\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n86.3000\n2.808\n30.737\n0.000\n80.648\n91.952\n\n\nC(vent, contraste)[a1]\n17.5500\n4.093\n4.288\n0.000\n9.311\n25.789\n\n\nC(vent, contraste)[a2]\n-8.0111\n5.993\n-1.337\n0.188\n-20.074\n4.052\n\n\nC(vent, contraste)[a3]\n-14.7222\n3.744\n-3.933\n0.000\n-22.258\n-7.187\n\n\n\n\n\n\n\n\nOmnibus:\n1.682\nDurbin-Watson:\n1.737\n\n\nProb(Omnibus):\n0.431\nJarque-Bera (JB):\n1.184\n\n\nSkew:\n0.083\nProb(JB):\n0.553\n\n\nKurtosis:\n2.264\nCond. No.\n3.34\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nround(sm.stats.anova_lm(mod3),3)\n\n\n\n\n\n\n\n\ndf\nsum_sq\nmean_sq\nF\nPR(&gt;F)\n\n\n\n\nC(vent, contraste)\n3.0\n9859.843\n3286.614\n8.338\n0.0\n\n\nResidual\n46.0\n18131.377\n394.160\nNaN\nNaN\n\n\n\n\n\n\n\n\nmod4 = smf.ols(\"O3 ~ 1+ C(vent,Sum)\", data = ozone).fit()\nround(sm.stats.anova_lm(mod4),3)\n\n\n\n\n\n\n\n\ndf\nsum_sq\nmean_sq\nF\nPR(&gt;F)\n\n\n\n\nC(vent, Sum)\n3.0\n9859.843\n3286.614\n8.338\n0.0\n\n\nResidual\n46.0\n18131.377\n394.160\nNaN\nNaN\n\n\n\n\n\n\n\n\nprint(ozone)\n\n        Date     O3   T12   T15  Ne12  N12  S12  E12  W12     Vx    O3v  \\\n0   19960422   63.6  13.4  15.0     7    0    0    3    0   9.35   95.6   \n1   19960429   89.6  15.0  15.7     4    3    0    0    0   5.40  100.2   \n2   19960506   79.0   7.9  10.1     8    0    0    7    0  19.30  105.6   \n3   19960514   81.2  13.1  11.7     7    7    0    0    0  12.60   95.2   \n4   19960521   88.0  14.1  16.0     6    0    0    0    6 -20.30   82.8   \n5   19960528   68.4  16.7  18.1     7    0    3    0    0  -3.69   71.4   \n6   19960605  139.0  26.8  28.2     1    0    0    3    0   8.27   90.0   \n7   19960612   78.2  18.4  20.7     7    4    0    0    0   4.93   60.0   \n8   19960619  113.8  27.2  27.7     6    0    4    0    0  -4.93  125.8   \n9   19960627   41.8  20.6  19.7     8    0    0    0    1  -3.38   62.6   \n10  19960704   65.0  21.0  21.1     6    0    0    0    7 -23.68   38.0   \n11  19960711   73.0  17.4  22.8     8    0    0    0    2  -6.24   70.8   \n12  19960719  126.2  26.9  29.5     2    0    0    4    0  14.18  119.8   \n13  19960726  127.8  25.5  27.8     3    0    0    5    0  13.79  103.6   \n14  19960802   61.6  19.4  21.5     7    6    0    0    0  -7.39   69.2   \n15  19960810   63.6  20.8  21.4     7    0    0    0    5 -13.79   48.0   \n16  19960817  134.2  29.5  30.6     2    0    3    0    0   1.88  118.6   \n17  19960824   67.2  21.7  20.3     7    0    0    0    7 -24.82   60.0   \n18  19960901   87.8  19.7  21.7     5    0    0    3    0   9.35   74.4   \n19  19960908   96.8  19.0  21.0     6    0    0    8    0  28.36  103.8   \n20  19960915   89.6  20.7  22.9     1    0    0    4    0  12.47   78.8   \n21  19960923   66.4  18.0  18.5     7    0    0    0    2  -5.52   72.2   \n22  19960930   60.0  17.4  16.4     8    0    6    0    0 -10.80   53.4   \n23  19970414   90.8  16.3  18.1     0    0    0    5    0  18.00   89.0   \n24  19970422  104.2  13.6  14.4     1    0    0    1    0   3.55   97.8   \n25  19970429   70.0  15.8  16.7     7    7    0    0    0 -12.60   61.4   \n26  19970708   96.2  26.0  27.3     2    0    0    5    0  16.91   87.4   \n27  19970715   65.6  23.5  23.7     7    0    0    0    3  -9.35   67.8   \n28  19970722  109.2  26.3  27.3     4    0    0    5    0  16.91   98.6   \n29  19970730   86.2  21.8  23.6     6    4    0    0    0   2.50  112.0   \n30  19970806   87.4  24.8  26.6     3    0    0    0    2  -7.09   49.8   \n31  19970813   84.0  25.2  27.5     3    0    0    0    3 -10.15  131.8   \n32  19970821   83.0  24.6  27.9     3    0    0    0    2  -5.52  113.8   \n33  19970828   59.6  16.8  19.0     7    0    0    0    8 -27.06   55.8   \n34  19970904   52.0  17.1  18.3     8    5    0    0    0  -3.13   65.8   \n35  19970912   73.8  18.0  18.3     7    0    5    0    0 -11.57   90.4   \n36  19970919  129.0  28.9  30.0     1    0    0    3    0   8.27  111.4   \n37  19970926  122.4  23.4  25.4     0    0    0    2    0   5.52  118.6   \n38  19980504  106.6  13.0  14.3     3    7    0    0    0  12.60   84.0   \n39  19980511  121.8  26.0  28.0     2    0    4    0    0   2.50  109.8   \n40  19980518  116.2  24.9  25.8     2    0    0    5    0  18.00  142.8   \n41  19980526   81.4  18.4  16.8     7    0    0    0    4 -14.40   80.8   \n42  19980602   88.6  18.7  19.6     5    0    0    0    5 -15.59   60.4   \n43  19980609   63.0  20.4  16.6     7    0    0    0    8 -22.06   79.8   \n44  19980617  104.0  19.6  21.2     6    0    0    0    3 -10.80   84.6   \n45  19980624   88.4  23.2  23.9     4    0    4    0    0  -7.20   92.6   \n46  19980701   83.8  19.8  20.3     8    0    0    5    0  17.73   40.2   \n47  19980709   56.4  18.9  19.3     8    0    0    0    4 -14.40   73.6   \n48  19980716   50.4  19.7  19.3     7    0    0    0    5 -17.73   59.0   \n49  19980724   79.2  21.1  21.9     3    4    0    0    0   9.26   55.2   \n\n      nebu   vent  \n0    NUAGE    EST  \n1   SOLEIL   NORD  \n2    NUAGE    EST  \n3    NUAGE   NORD  \n4    NUAGE  OUEST  \n5    NUAGE    SUD  \n6   SOLEIL    EST  \n7    NUAGE   NORD  \n8    NUAGE    SUD  \n9    NUAGE  OUEST  \n10   NUAGE  OUEST  \n11   NUAGE  OUEST  \n12  SOLEIL    EST  \n13  SOLEIL    EST  \n14   NUAGE   NORD  \n15   NUAGE  OUEST  \n16  SOLEIL    SUD  \n17   NUAGE  OUEST  \n18  SOLEIL    EST  \n19   NUAGE    EST  \n20  SOLEIL    EST  \n21   NUAGE  OUEST  \n22   NUAGE    SUD  \n23  SOLEIL    EST  \n24  SOLEIL    EST  \n25   NUAGE   NORD  \n26  SOLEIL    EST  \n27   NUAGE  OUEST  \n28  SOLEIL    EST  \n29   NUAGE   NORD  \n30  SOLEIL  OUEST  \n31  SOLEIL  OUEST  \n32  SOLEIL  OUEST  \n33   NUAGE  OUEST  \n34   NUAGE   NORD  \n35   NUAGE    SUD  \n36  SOLEIL    EST  \n37  SOLEIL    EST  \n38  SOLEIL   NORD  \n39  SOLEIL    SUD  \n40  SOLEIL    EST  \n41   NUAGE  OUEST  \n42  SOLEIL  OUEST  \n43   NUAGE  OUEST  \n44   NUAGE  OUEST  \n45  SOLEIL    SUD  \n46   NUAGE    EST  \n47   NUAGE  OUEST  \n48   NUAGE  OUEST  \n49  SOLEIL   NORD  \n\n\n\nozone = pd.read_csv(\"../donnees/ozone.txt\", header = 0, sep = \";\")\nozone[\"vent\"]=ozone[\"vent\"].astype(\"category\")\nozone[\"nebu\"]=ozone[\"nebu\"].astype(\"category\")\nplt.rcParams['font.size'] = '7'\nfrom statsmodels.graphics.api import interaction_plot\ncc2 = cm.Set1(range(ozone[\"vent\"].cat.categories.size))\nfig, axs = plt.subplots(1,2)\nplt.rcParams['font.size'] = '4'\ninteraction_plot(ozone[\"vent\"].astype(\"str\"), ozone[\"nebu\"].astype(\"str\"), ozone[\"O3\"], colors=cc2[:2], markers=['^','D'],ax=axs[0])\ninteraction_plot( ozone[\"nebu\"].astype(\"str\"), ozone[\"vent\"].astype(\"str\"),ozone[\"O3\"], colors=cc2, markers=['^','D',\"*\",\"8\"],ax=axs[1])\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nmod1 = smf.ols(\"O3 ~ vent + nebu + vent:nebu\", data = ozone).fit()\nmod2 = smf.ols(\"O3 ~ vent + nebu \", data = ozone).fit()\nsm.stats.anova_lm(mod2,mod1)\n\n\n\n\n\n\n\n\ndf_resid\nssr\ndf_diff\nss_diff\nF\nPr(&gt;F)\n\n\n\n\n0\n45.0\n11729.859077\n0.0\nNaN\nNaN\nNaN\n\n\n1\n42.0\n11246.238571\n3.0\n483.620506\n0.60204\n0.617302\n\n\n\n\n\n\n\n\nmod3 = smf.ols(\"O3 ~ vent\", data = ozone).fit()\nround(sm.stats.anova_lm(mod3,mod2),3)\n\n\n\n\n\n\n\n\ndf_resid\nssr\ndf_diff\nss_diff\nF\nPr(&gt;F)\n\n\n\n\n0\n46.0\n18131.377\n0.0\nNaN\nNaN\nNaN\n\n\n1\n45.0\n11729.859\n1.0\n6401.518\n24.559\n0.0\n\n\n\n\n\n\n\n\nround(sm.stats.anova_lm(mod3, mod2, mod1),3)\n\n\n\n\n\n\n\n\ndf_resid\nssr\ndf_diff\nss_diff\nF\nPr(&gt;F)\n\n\n\n\n0\n46.0\n18131.377\n0.0\nNaN\nNaN\nNaN\n\n\n1\n45.0\n11729.859\n1.0\n6401.518\n23.907\n0.000\n\n\n2\n42.0\n11246.239\n3.0\n483.621\n0.602\n0.617\n\n\n\n\n\n\n\n\nround(sm.stats.anova_lm(mod1),3)\n\n\n\n\n\n\n\n\ndf\nsum_sq\nmean_sq\nF\nPR(&gt;F)\n\n\n\n\nvent\n3.0\n9859.843\n3286.614\n12.274\n0.000\n\n\nnebu\n1.0\n6401.518\n6401.518\n23.907\n0.000\n\n\nvent:nebu\n3.0\n483.621\n161.207\n0.602\n0.617\n\n\nResidual\n42.0\n11246.239\n267.768\nNaN\nNaN\n\n\n\n\n\n\n\n\n\n\n\n Retour au sommet",
    "crumbs": [
      "Codes",
      "II Inférence",
      "Variables qualitatives : ANCOVA et ANOVA"
    ]
  },
  {
    "objectID": "correction/chap16.html",
    "href": "correction/chap16.html",
    "title": "16 Données déséquilibrées",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nrng = np.random.default_rng(seed=1234)\n\n\nExercice 1 (Critères pour un exemple de données déséquilibrées)  \n\n\nn = 500\np = 0.05\nY = rng.binomial(1, p=p, size=n)\n\n\nrng = np.random.default_rng(seed=123)\nP1 = rng.binomial(1, p=0.005, size=n)\n\n\nP2 = np.zeros_like(P1)\nfor yy in range(n):\n    if Y[yy]==0:\n        P2[yy] = rng.binomial(1, p=0.10, size=1)[0]\n    else:\n        P2[yy] = rng.binomial(1, p=0.85, size=1)[0]\n\n\nfrom sklearn.metrics import confusion_matrix\nprint(confusion_matrix(Y, P1))\n\n[[478   0]\n [ 22   0]]\n\n\n\nprint(confusion_matrix(Y, P2))\n\n[[432  46]\n [  4  18]]\n\n\n\ncm = confusion_matrix(Y, P2)\nacc = cm.diagonal().sum()/cm.sum()\nrec = cm[1,1]/cm[1,:].sum()\nprec = cm[1,1]/cm[:,1].sum()\nprint(acc)\nprint(rec)\nprint(prec)\n\n0.9\n0.8181818181818182\n0.28125\n\n\n\nF1 = 2*(rec*prec)/(rec+prec)\nprint(F1)\nrand = cm[:,0].sum()/n*cm[0,:].sum()/n + cm[:,1].sum()/n*cm[1,:].sum()/n\nkappa = (acc-rand)/(1-rand)\nprint(kappa)\n\n0.41860465116279066\n0.37786183555644093\n\n\n\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score\nfrom sklearn.metrics import f1_score, cohen_kappa_score\nprint(accuracy_score(Y, P2), \"**\", accuracy_score(Y, P1))\nprint(recall_score(Y, P2), \"**\", recall_score(Y, P1))\nprint(precision_score(Y, P2), \"**\", precision_score(Y, P1))\nprint(f1_score(Y, P2), \"**\", f1_score(Y, P1))\nprint(cohen_kappa_score(Y, P2), \"**\", cohen_kappa_score(Y, P1))\n\n0.9 ** 0.956\n0.8181818181818182 ** 0.0\n0.28125 ** 0.0\n0.4186046511627907 ** 0.0\n0.3778618355564404 ** 0.0\n\n\n\n\n\nExercice 2 (Échantillonnage rétrospectif) On remarque d’abord que \\(\\mathbf P(\\tilde y_i=1)=\\mathbf P(y_i=1|s_i=1)\\). De plus \\[\n\\text{logit}\\, p_\\beta(x_i)=\\log\\frac{\\mathbf P(y_i=1)}{\\mathbf P(y_i=0)}\\quad\\text{et}\\quad \\text{logit}\\, p_\\gamma(x_i)=\\log\\frac{\\mathbf P(y_i=1|s_i=1)}{\\mathbf P(y_i=0|s_i=1)}.\n\\] Or \\[\n\\mathbf P(y_i=1|s_i=1)=\\frac{\\mathbf P(y_i=1,s_i=1)}{\\mathbf P(s_i=1)}=\\frac{\\mathbf P(s_i=1|y_i=1)\\mathbf P(y_i=1)}{\\mathbf P(s_i=1)}\n\\] et \\[\n\\mathbf P(y_i=0|s_i=1)=\\frac{\\mathbf P(y_i=0,s_i=1)}{\\mathbf P(s_i=1)}=\\frac{\\mathbf P(s_i=1|y_i=0)\\mathbf P(y_i=0)}{\\mathbf P(s_i=1)}.\n\\] Donc \\[\n\\text{logit}\\, p_\\gamma(x_i)=\\log\\frac{\\mathbf P(y_i=1)}{\\mathbf P(y_i=0)}+\\log\\frac{\\mathbf P(s_i=1|y_i=1)}{\\mathbf P(s_i=1|y_i=0)}=\\text{logit}\\,p_\\beta(x_i)+\\log\\left(\\frac{\\tau_{1i}}{\\tau_{0i}}\\right).\n\\]\n\n\nExercice 3 (Rééquilibrage)  \n\n\ndf1 = pd.read_csv(\"../donnees/dd_exo3_1.csv\", header=0, sep=',')\ndf2 = pd.read_csv(\"../donnees/dd_exo3_2.csv\", header=0, sep=',')\ndf3 = pd.read_csv(\"../donnees/dd_exo3_3.csv\", header=0, sep=',')\n\n\nprint(df1.describe())\nprint(df2.describe())\nprint(df3.describe())\n\n                X1           X2            Y\ncount  1000.000000  1000.000000  1000.000000\nmean      0.514433     0.492924     0.441000\nstd       0.281509     0.291467     0.496755\nmin       0.000516     0.000613     0.000000\n25%       0.284947     0.238695     0.000000\n50%       0.518250     0.494121     0.000000\n75%       0.753628     0.739679     1.000000\nmax       0.999567     0.999829     1.000000\n                X1           X2            Y\ncount  1000.000000  1000.000000  1000.000000\nmean      0.520809     0.472473     0.308000\nstd       0.280013     0.283496     0.461898\nmin       0.002732     0.000890     0.000000\n25%       0.296167     0.225272     0.000000\n50%       0.521226     0.468858     0.000000\n75%       0.764060     0.693746     1.000000\nmax       0.996044     0.999183     1.000000\n                X1           X2            Y\ncount  1000.000000  1000.000000  1000.000000\nmean      0.538032     0.454919     0.158000\nstd       0.273863     0.271638     0.364924\nmin       0.004914     0.000613     0.000000\n25%       0.322489     0.221447     0.000000\n50%       0.545587     0.450438     0.000000\n75%       0.781116     0.663637     0.000000\nmax       0.996044     0.999829     1.000000\n\n\n\ncolo = [\"C1\", \"C2\"]\nmark = [\"o\", \"d\"]\nfor yy in [0, 1]:\n    plt.scatter(df1.loc[df1.Y==yy, \"X1\"], df1.loc[df1.Y==yy, \"X2\"], color=colo[yy], marker=mark[yy])\n\n\n\n\n\n\n\n\n\nfor yy in [0, 1]:\n    plt.scatter(df2.loc[df2.Y==yy, \"X1\"], df2.loc[df2.Y==yy, \"X2\"], color=colo[yy], marker=mark[yy])\n\n\n\n\n\n\n\n\n\nfor yy in [0, 1]:\n    plt.scatter(df3.loc[df3.Y==yy, \"X1\"], df3.loc[df3.Y==yy, \"X2\"], color=colo[yy], marker=mark[yy])\n\n\n\n\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\n## separation en matrice X, Y (et creation du produit=interaction)\nT1 = df1.drop(columns=\"Y\")\nX1 = T1.assign(inter= T1.X1 * T1.X2).to_numpy()\ny1 = df1.Y.to_numpy()\nT2 = df2.drop(columns=\"Y\")\nX2 = T2.assign(inter= T2.X1 * T2.X2).to_numpy()\ny2 = df2.Y.to_numpy()\nT3 = df3.drop(columns=\"Y\")\nX3 = T3.assign(inter= T3.X1 * T3.X2).to_numpy()\ny3 = df3.Y.to_numpy()\n## separation apprentissage/validation\nX1_app, X1_valid, y1_app, y1_valid = train_test_split(\n    X1, y1, test_size=0.33, random_state=1234)\nX2_app, X2_valid, y2_app, y2_valid = train_test_split(\n    X2, y2, test_size=0.33, random_state=1234)\nX3_app, X3_valid, y3_app, y3_valid = train_test_split(\n    X3, y3, test_size=0.33, random_state=1234)\n\n\nfrom sklearn.linear_model import LogisticRegression\nmod1 = LogisticRegression(penalty=None, solver=\"newton-cholesky\").fit(X1_app, y1_app)\nmod2 = LogisticRegression(penalty=None, solver=\"newton-cholesky\").fit(X2_app, y2_app)\nmod3 = LogisticRegression(penalty=None, solver=\"newton-cholesky\").fit(X3_app, y3_app)\n\n\nfrom sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, cohen_kappa_score\nP1 = mod1.predict(X1_valid)\nP2 = mod1.predict(X2_valid)\nP3 = mod1.predict(X3_valid)\ns1 = pd.DataFrame({\"crit\": [\"acc\", \"bal_acc\", \"F1\", \"Kappa\"]})\ns2 = pd.DataFrame({\"crit\": [\"acc\", \"bal_acc\", \"F1\", \"Kappa\"]})\ns3 = pd.DataFrame({\"crit\": [\"acc\", \"bal_acc\", \"F1\", \"Kappa\"]})\nprint(\"--- donnees 1 ---\")\ns1 = s1.assign(brut=0.0)\ns1.iloc[0,1] = accuracy_score(y1_valid, P1)\ns1.iloc[1,1] = balanced_accuracy_score(y1_valid, P1)\ns1.iloc[2,1] = f1_score(y1_valid, P1)\ns1.iloc[3,1] = cohen_kappa_score(y1_valid, P1)\nprint(s1)\nprint(\"--- donnees 2 ---\")\ns2 = s2.assign(brut=0.0)\ns2.iloc[0,1] = accuracy_score(y2_valid, P2)\ns2.iloc[1,1] = balanced_accuracy_score(y2_valid, P2)\ns2.iloc[2,1] = f1_score(y2_valid, P2)\ns2.iloc[3,1] = cohen_kappa_score(y2_valid, P2)\nprint(s2)\nprint(\"--- donnees 3 ---\")\ns3 = s3.assign(brut=0.0)\ns3.iloc[0,1] = accuracy_score(y3_valid, P3)\ns3.iloc[1,1] = balanced_accuracy_score(y3_valid, P3)\ns3.iloc[2,1] = f1_score(y3_valid, P3)\ns3.iloc[3,1] = cohen_kappa_score(y3_valid, P3)\nprint(s3)\n\n--- donnees 1 ---\n      crit      brut\n0      acc  0.657576\n1  bal_acc  0.655825\n2       F1  0.622074\n3    Kappa  0.309572\n--- donnees 2 ---\n      crit      brut\n0      acc  0.724242\n1  bal_acc  0.740841\n2       F1  0.637450\n3    Kappa  0.427280\n--- donnees 3 ---\n      crit      brut\n0      acc  0.693939\n1  bal_acc  0.729286\n2       F1  0.435754\n3    Kappa  0.278103\n\n\n\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.under_sampling import TomekLinks\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.over_sampling import SMOTE\n## RandomOverSampler\nros3 = RandomOverSampler(random_state=123)\nX3_app_reech, y3_app_reech = ros3.fit_resample(X3_app, y3_app)\nmod3_ros = LogisticRegression(penalty=None, solver=\"newton-cholesky\").fit(X3_app_reech, y3_app_reech)\n## Smote\nsm = RandomOverSampler(random_state=123)\nX3_app_reech, y3_app_reech = sm.fit_resample(X3_app, y3_app)\nmod3_sm = LogisticRegression(penalty=None, solver=\"newton-cholesky\").fit(X3_app_reech, y3_app_reech)\n## RandomUnderSampler\nrus3 = RandomUnderSampler(random_state=123)\nX3_app_reech, y3_app_reech = rus3.fit_resample(X3_app, y3_app)\nmod3_rus = LogisticRegression(penalty=None, solver=\"newton-cholesky\").fit(X3_app_reech, y3_app_reech)\n## Tomek\ntl = TomekLinks(sampling_strategy='all')\nX3_app_reech, y3_app_reech = tl.fit_resample(X3_app, y3_app)\nmod3_tl = LogisticRegression(penalty=None, solver=\"newton-cholesky\").fit(X3_app_reech, y3_app_reech)\n\n\nP3_ros = mod3_ros.predict(X3_valid)\nP3_sm = mod3_sm.predict(X3_valid)\nP3_rus = mod3_rus.predict(X3_valid)\nP3_tl = mod3_tl.predict(X3_valid)\n\n\ns3 = s3.assign(ros=[accuracy_score(y3_valid, P3_ros),\nbalanced_accuracy_score(y3_valid, P3_ros),\nf1_score(y3_valid, P3_ros),\ncohen_kappa_score(y3_valid, P3_ros)])\ns3 = s3.assign(sm=[accuracy_score(y3_valid, P3_sm),\nbalanced_accuracy_score(y3_valid, P3_sm),\nf1_score(y3_valid, P3_sm),\ncohen_kappa_score(y3_valid, P3_sm)])\ns3 = s3.assign(rus=[accuracy_score(y3_valid, P3_rus),\nbalanced_accuracy_score(y3_valid, P3_rus),\nf1_score(y3_valid, P3_rus),\ncohen_kappa_score(y3_valid, P3_rus)])\ns3 = s3.assign(tl=[accuracy_score(y3_valid, P3_tl),\nbalanced_accuracy_score(y3_valid, P3_tl),\nf1_score(y3_valid, P3_tl),\ncohen_kappa_score(y3_valid, P3_tl)])\nprint(s3)\n\n      crit      brut       ros        sm       rus        tl\n0      acc  0.693939  0.603030  0.603030  0.612121  0.854545\n1  bal_acc  0.729286  0.683929  0.683929  0.689286  0.520000\n2       F1  0.435754  0.379147  0.379147  0.384615  0.076923\n3    Kappa  0.278103  0.192415  0.192415  0.200606  0.066038\n\n\n\n\n\n\n\n Retour au sommet",
    "crumbs": [
      "Corrections",
      "IV Le modèle linéaire généralisé",
      "16 Données déséquilibrées"
    ]
  },
  {
    "objectID": "codes/chap14.html",
    "href": "codes/chap14.html",
    "title": "14 Régularisation de la vraisemblance",
    "section": "",
    "text": "import pandas as pd; import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegressionCV, LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import StratifiedKFold\nfrom patsy import dmatrix\n\n\nRegressions ridge, lasso et elastic-net\n\nSAh = pd.read_csv(\"../donnees/SAh.csv\", header=0, sep=\",\")\n\n\nnomsvar = list(SAh.columns.difference([\"chd\"]))\nformule = \"~ 1 +\" + \"+\".join(nomsvar)\ndsX = dmatrix(formule, data=SAh)\nX = dmatrix(formule, data=SAh, return_type=\"dataframe\").\\\n    iloc[:,1:].to_numpy()\nY = SAh[\"chd\"].to_numpy()\n\n\nscalerX = StandardScaler().fit(X)\nXcr= scalerX.transform(X)\nl0 = np.abs(Xcr.transpose().dot((Y  - Y.mean()))).max()/X.shape[0]\nllc = np.linspace(0,-4,100)\nll = l0*10**llc\nCs_lasso = 1/ 0.9/ X.shape[0] / (l0*10**(llc))\nCs_ridge = 1/ 0.9/ X.shape[0] / ((l0*10**(llc)) * 100)\nCs_enet =  1/ 0.9/ X.shape[0] / ((l0*10**(llc)) / 0.5)\n\n\ncoefs_lasso = []\ncoefs_enet = []\ncoefs_ridge = []\n\n\nXcr = StandardScaler().fit(X).transform(X)\nfor a, b, c in zip(Cs_lasso, Cs_ridge, Cs_enet) :\n    ## lasso\n    lasso = LogisticRegression(penalty=\"l1\", C=a, solver=\"liblinear\", warm_start=True).fit(Xcr, Y)\n    coefs_lasso.append(lasso.coef_[0])\n    ## ridge\n    ridge = LogisticRegression(penalty=\"l2\", C=b, warm_start=True).fit(Xcr, Y)\n    coefs_ridge.append(ridge.coef_[0])\n    ## enet\n    enet = LogisticRegression(penalty=\"elasticnet\", C=c, solver=\"saga\", l1_ratio=0.5, warm_start=True).fit(Xcr, Y)\n    coefs_enet.append(enet.coef_[0])\n\n\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3)\nax1.plot(np.log(Cs_lasso), coefs_lasso)\nax2.plot(np.log(Cs_enet), coefs_enet)\nax3.plot(np.log(Cs_ridge), coefs_ridge)\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\n\nChoix du paramètre de régularisation \\(\\lambda\\)\n\nskf = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n\n\ncr = StandardScaler()\nlassocvM1 =  LogisticRegressionCV(cv=skf, penalty=\"l1\", n_jobs=3, solver=\"liblinear\", Cs=Cs_lasso, scoring=\"neg_log_loss\")\nridgecvM1 = LogisticRegressionCV(cv=skf, penalty=\"l2\", n_jobs=3, Cs=Cs_ridge, scoring=\"neg_log_loss\")\nenetcvM1 = LogisticRegressionCV(cv=skf, penalty=\"elasticnet\", n_jobs=3, Cs= Cs_enet, solver=\"saga\", l1_ratios=[0.5], scoring=\"neg_log_loss\")\n\n\npipe_lassocvM1 = Pipeline(steps=[(\"cr\", cr), (\"lassocv\", lassocvM1)])\npipe_ridgecvM1 = Pipeline(steps=[(\"cr\", cr), (\"ridgecv\", ridgecvM1)])\npipe_enetcvM1 = Pipeline(steps=[(\"cr\", cr), (\"enetcv\", enetcvM1)])\n\n\npipe_lassocvM1.fit(X,Y)\npipe_ridgecvM1.fit(X,Y)\npipe_enetcvM1.fit(X,Y)\n\nPipeline(steps=[('cr', StandardScaler()),\n                ('enetcv',\n                 LogisticRegressionCV(Cs=array([6.77620047e-03, 7.43687165e-03, 8.16195745e-03, 8.95773823e-03,\n       9.83110664e-03, 1.07896274e-02, 1.18416028e-02, 1.29961444e-02,\n       1.42632524e-02, 1.56539020e-02, 1.71801381e-02, 1.88551803e-02,\n       2.06935371e-02, 2.27111314e-02, 2.49254387e-02, 2.73556382e-02,\n       3.00227792e-02, 3.29499631e-02...\n       1.67851660e+01, 1.84216989e+01, 2.02177918e+01, 2.21890016e+01,\n       2.43524018e+01, 2.67267309e+01, 2.93325542e+01, 3.21924420e+01,\n       3.53311654e+01, 3.87759104e+01, 4.25565138e+01, 4.67057214e+01,\n       5.12594715e+01, 5.62572067e+01, 6.17422149e+01, 6.77620047e+01]),\n                                      cv=StratifiedKFold(n_splits=10, random_state=0, shuffle=True),\n                                      l1_ratios=[0.5], n_jobs=3,\n                                      penalty='elasticnet',\n                                      scoring='neg_log_loss', solver='saga'))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('cr', StandardScaler()),\n                ('enetcv',\n                 LogisticRegressionCV(Cs=array([6.77620047e-03, 7.43687165e-03, 8.16195745e-03, 8.95773823e-03,\n       9.83110664e-03, 1.07896274e-02, 1.18416028e-02, 1.29961444e-02,\n       1.42632524e-02, 1.56539020e-02, 1.71801381e-02, 1.88551803e-02,\n       2.06935371e-02, 2.27111314e-02, 2.49254387e-02, 2.73556382e-02,\n       3.00227792e-02, 3.29499631e-02...\n       1.67851660e+01, 1.84216989e+01, 2.02177918e+01, 2.21890016e+01,\n       2.43524018e+01, 2.67267309e+01, 2.93325542e+01, 3.21924420e+01,\n       3.53311654e+01, 3.87759104e+01, 4.25565138e+01, 4.67057214e+01,\n       5.12594715e+01, 5.62572067e+01, 6.17422149e+01, 6.77620047e+01]),\n                                      cv=StratifiedKFold(n_splits=10, random_state=0, shuffle=True),\n                                      l1_ratios=[0.5], n_jobs=3,\n                                      penalty='elasticnet',\n                                      scoring='neg_log_loss', solver='saga'))]) StandardScaler?Documentation for StandardScalerStandardScaler() LogisticRegressionCV?Documentation for LogisticRegressionCVLogisticRegressionCV(Cs=array([6.77620047e-03, 7.43687165e-03, 8.16195745e-03, 8.95773823e-03,\n       9.83110664e-03, 1.07896274e-02, 1.18416028e-02, 1.29961444e-02,\n       1.42632524e-02, 1.56539020e-02, 1.71801381e-02, 1.88551803e-02,\n       2.06935371e-02, 2.27111314e-02, 2.49254387e-02, 2.73556382e-02,\n       3.00227792e-02, 3.29499631e-02, 3.61625438e-02, 3.96883472e-02,\n       4.35579121e-02, 4.78...\n       1.67851660e+01, 1.84216989e+01, 2.02177918e+01, 2.21890016e+01,\n       2.43524018e+01, 2.67267309e+01, 2.93325542e+01, 3.21924420e+01,\n       3.53311654e+01, 3.87759104e+01, 4.25565138e+01, 4.67057214e+01,\n       5.12594715e+01, 5.62572067e+01, 6.17422149e+01, 6.77620047e+01]),\n                     cv=StratifiedKFold(n_splits=10, random_state=0, shuffle=True),\n                     l1_ratios=[0.5], n_jobs=3, penalty='elasticnet',\n                     scoring='neg_log_loss', solver='saga') \n\n\n\ncr = StandardScaler()\nlassocvM2 =  LogisticRegressionCV(cv=skf, penalty=\"l1\", n_jobs=3, solver=\"liblinear\", Cs=Cs_lasso, scoring=\"accuracy\")\nridgecvM2 = LogisticRegressionCV(cv=skf, penalty=\"l2\", n_jobs=3, Cs=Cs_ridge, scoring=\"accuracy\")\nenetcvM2 = LogisticRegressionCV(cv=skf, penalty=\"elasticnet\", n_jobs=3, Cs= Cs_enet, solver=\"saga\", l1_ratios=[0.5], scoring=\"accuracy\")\npipe_lassocvM2 = Pipeline(steps=[(\"cr\", cr), (\"lassocv\", lassocvM2)])\npipe_ridgecvM2 = Pipeline(steps=[(\"cr\", cr), (\"ridgecv\", ridgecvM2)])\npipe_enetcvM2 = Pipeline(steps=[(\"cr\", cr), (\"enetcv\", enetcvM2)])\npipe_lassocvM2.fit(X,Y)\npipe_ridgecvM2.fit(X,Y)\npipe_enetcvM2.fit(X,Y)\n\n\ncr = StandardScaler()\nlassocvM3 =  LogisticRegressionCV(cv=skf, penalty=\"l1\", n_jobs=3, solver=\"liblinear\", Cs=Cs_lasso, scoring=\"roc_auc\")\nridgecvM3 = LogisticRegressionCV(cv=skf, penalty=\"l2\", n_jobs=3, Cs=Cs_ridge, scoring=\"roc_auc\")\nenetcvM3 = LogisticRegressionCV(cv=skf, penalty=\"elasticnet\", n_jobs=3, Cs= Cs_enet, solver=\"saga\", l1_ratios=[0.5], scoring=\"roc_auc\")\npipe_lassocvM3 = Pipeline(steps=[(\"cr\", cr), (\"lassocv\", lassocvM3)])\npipe_ridgecvM3 = Pipeline(steps=[(\"cr\", cr), (\"ridgecv\", ridgecvM3)])\npipe_enetcvM3 = Pipeline(steps=[(\"cr\", cr), (\"enetcv\", enetcvM3)])\npipe_lassocvM3.fit(X,Y)\npipe_ridgecvM3.fit(X,Y)\npipe_enetcvM3.fit(X,Y)\netape_lassoM3 = pipe_lassocvM3.named_steps[\"lassocv\"]\netape_lassoM3.Cs_\netape_lassoM3.scores_[1]\netape_lassoM3.scores_[1].mean(axis=0)\netape_lassoM3.scores_[1].std(axis=0)/np.sqrt(10)\n\netape_lassoM1 = pipe_lassocvM1.named_steps[\"lassocv\"]\netape_lassoM2 = pipe_lassocvM2.named_steps[\"lassocv\"]\netape_lassoM3 = pipe_lassocvM3.named_steps[\"lassocv\"]\netape_ridgeM1 = pipe_ridgecvM1.named_steps[\"ridgecv\"]\netape_ridgeM2 = pipe_ridgecvM2.named_steps[\"ridgecv\"]\netape_ridgeM3 = pipe_ridgecvM3.named_steps[\"ridgecv\"]\netape_enetM1 = pipe_enetcvM1.named_steps[\"enetcv\"]\netape_enetM2 = pipe_enetcvM2.named_steps[\"enetcv\"]\netape_enetM3 = pipe_enetcvM3.named_steps[\"enetcv\"]\n\n\nfig, axs = plt.subplots(3, 3)\naxs[0,0].errorbar(np.log(Cs_lasso), etape_lassoM1.scores_[1].mean(axis=0), etape_lassoM1.scores_[1].std(axis=0)/np.sqrt(10), fmt=\"-o\", mec=\"black\", mfc=\"black\", ms=0.2, color=\"#80808077\")\naxs[0,0].text(.99, .1, \"Lasso - Vraisemblance\", ha=\"right\", va=\"top\",transform=axs[0,0].transAxes)\naxs[0,1].errorbar(np.log(Cs_ridge), etape_ridgeM1.scores_[1].mean(axis=0), etape_ridgeM1.scores_[1].std(axis=0)/np.sqrt(10), fmt=\"-o\", mec=\"black\", mfc=\"black\", ms=0.2, color=\"#80808077\")\naxs[0,1].text(.99, .1, \"Ridge - Vraisemblance\", ha=\"right\", va=\"top\",transform=axs[0,1].transAxes)\naxs[0,2].errorbar(np.log(Cs_enet), etape_enetM1.scores_[1][:,:,0].mean(axis=0), etape_enetM1.scores_[1][:,:,0].std(axis=0)/np.sqrt(10), fmt=\"-o\", mec=\"black\", mfc=\"black\", ms=0.2, color=\"#80808077\")\naxs[0,2].text(.99, .1, \"ElasticNet - Vraisemblance\", ha=\"right\", va=\"top\",transform=axs[0,2].transAxes)\n##\naxs[1,0].errorbar(np.log(Cs_lasso), etape_lassoM2.scores_[1].mean(axis=0), etape_lassoM2.scores_[1].std(axis=0)/np.sqrt(10), fmt=\"-o\", mec=\"black\", mfc=\"black\", ms=0.2, color=\"#80808077\")\naxs[1,0].text(.99, .1, \"Lasso - Bien classés\", ha=\"right\", va=\"top\",transform=axs[1,0].transAxes)\naxs[1,1].errorbar(np.log(Cs_ridge), etape_ridgeM2.scores_[1].mean(axis=0), etape_ridgeM2.scores_[1].std(axis=0)/np.sqrt(10), fmt=\"-o\", mec=\"black\", mfc=\"black\", ms=0.2, color=\"#80808077\")\naxs[1,1].text(.99, .1, \"Ridge - Bien classés\", ha=\"right\", va=\"top\",transform=axs[1,1].transAxes)\naxs[1,2].errorbar(np.log(Cs_enet), etape_enetM2.scores_[1][:,:,0].mean(axis=0), etape_enetM2.scores_[1][:,:,0].std(axis=0)/np.sqrt(10), fmt=\"-o\", mec=\"black\", mfc=\"black\", ms=0.2, color=\"#80808077\")\naxs[1,2].text(.99, .1, \"ElasticNet - Bien classés\", ha=\"right\", va=\"top\",transform=axs[1,2].transAxes)\n##\naxs[2,0].errorbar(np.log(Cs_lasso), etape_lassoM3.scores_[1].mean(axis=0), etape_lassoM3.scores_[1].std(axis=0)/np.sqrt(10), fmt=\"-o\", mec=\"black\", mfc=\"black\", ms=0.2, color=\"#80808077\")\naxs[2,0].text(.99, .1, \"Lasso - AUC\", ha=\"right\", va=\"top\",transform=axs[2,0].transAxes)\naxs[2,1].errorbar(np.log(Cs_ridge), etape_ridgeM3.scores_[1].mean(axis=0), etape_ridgeM3.scores_[1].std(axis=0)/np.sqrt(10), fmt=\"-o\", mec=\"black\", mfc=\"black\", ms=0.2, color=\"#80808077\")\naxs[2,1].text(.99, .1, \"Ridge - AUC\", ha=\"right\", va=\"top\",transform=axs[2,1].transAxes)\naxs[2,2].errorbar(np.log(Cs_enet), etape_enetM3.scores_[1][:,:,0].mean(axis=0), etape_enetM3.scores_[1][:,:,0].std(axis=0)/np.sqrt(10), fmt=\"-o\", mec=\"black\", mfc=\"black\", ms=0.2, color=\"#80808077\")\naxs[2,2].text(.99, .1, \"ElasticNet - AUC\", ha=\"right\", va=\"top\",transform=axs[2,2].transAxes)\n\nText(0.99, 0.1, 'ElasticNet - AUC')\n\n\n\n\n\n\n\n\n\n\npipe_lassocvM3.predict_proba(X[[14,49],:])\n\narray([[0.46132249, 0.53867751],\n       [0.85489843, 0.14510157]])\n\n\n\n\nGroup lasso\n\nX1 = np.concatenate((np.repeat(\"A\", 60), np.repeat(\"B\", 90), np.repeat(\"C\", 50)))\nX2 = np.concatenate((np.repeat(\"E\", 40), np.repeat(\"F\", 60), np.repeat(\"G\", 55), np.repeat(\"H\",45)))\nrng = np.random.default_rng(1298)\nX3 = rng.uniform(low=0.0, high=1.0, size=200)\nrng = np.random.default_rng(2381)\nY = rng.uniform(low=0.0, high=1.0, size=200).round(0)\nY1 = np.sign(2*Y-1)\ndonnees = pd.DataFrame({\"X1\": X1, \"X2\": X2, \"X3\": X3, \"Y\": Y, \"Y1\": Y1})\n\n\n\n\n\n Retour au sommet",
    "crumbs": [
      "Codes",
      "IV Le modèle linéaire généralisé",
      "14 Régularisation de la vraisemblance"
    ]
  },
  {
    "objectID": "correction/chap13.html",
    "href": "correction/chap13.html",
    "title": "13 Régression de Poisson",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom scipy.stats import chi2_contingency\n\n\nExercice 1 (Questions de cours) C, A, B, A, B, B, C, A\n\n\nExercice 2  \n\n\nExercice 3  \n\n\nExercice 4 (Stabilisation de la variance)  \n\nlambdas = np.arange(1, 21)\nsample_size = 1000000\nvariances = []\n\nfor lam in lambdas:\n    X = np.random.poisson(lam, sample_size)\n    Z = np.sqrt(X)\n    variance_Z = np.var(Z)\n    variances.append(variance_Z)\n\nplt.plot(lambdas, variances, marker='o')\nplt.axhline(y=0.25, color='r', linestyle='--', label='Variance théorique = 1/4')\nplt.xlabel('Lambda')\nplt.ylabel('Variance empirique de Z')\nplt.title('Variance empirique de Z = sqrt(X) en fonction de Lambda')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nExercice 5 (Stabilisation de la variance (suite)) Lorsque \\(\\lambda\\) est grand, on utilise t’approximétion de Taylor à l’ordre 1 : \\[\n\\sqrt{X}\\approx \\sqrt{\\lambda}+\\frac{1}{2\\sqrt{\\lambda}}(X-\\lambda).\n\\] Comme \\(\\mathop{\\mathrm{V}}(X)=\\lambda\\), on déduit \\(\\mathop{\\mathrm{V}}(\\sqrt{X})\\approx 1/4\\).\n\n\nExercice 6 (Malaria (suite))  \n\n\nMalaria = pd.read_csv(\"../donnees/poissonData3.csv\", header=0, sep=',')\ncompt = Malaria[[\"Sexe\", \"Nmalaria\", \"Age\"]].groupby([\"Sexe\",\\\n                \"Nmalaria\"]).count()\nsexe = list(compt.index.levels[0])\nsigne = [1, -1]\nfor i, val in enumerate(sexe):\n    plt.bar(compt.loc[val].index, signe[i] * compt.loc[val].Age)\n\n\n\n\n\n\n\n\nLes barres sont superposées : les filles puis les garçons. Comme on soustrait à la hauteur totale les garçons (argument offset), on a les effectifs des filles en dessous et ceux des garçons au dessus.\nLes moyennes par groupe\n\nprint(Malaria[[\"Sexe\", \"Nmalaria\"]].groupby([\"Sexe\"]).mean())\n\n      Nmalaria\nSexe          \nF     4.579012\nM     4.794370\n\n\n::: {#7aa664f5 .cell execution_count=5} {.python .cell-code}  print(np.log(Malaria[[\"Sexe\", \"Nmalaria\"]].groupby([\"Sexe\"]).mean().loc[\"F\", :]))  print(np.log(Malaria[[\"Sexe\", \"Nmalaria\"]].groupby([\"Sexe\"]).mean()).diff().iloc[1])\n::: {.cell-output .cell-output-stdout} Nmalaria    1.521483  Name: F, dtype: float64  Nmalaria    0.045959  Name: M, dtype: float64 ::: :::\nRégression de Poisson\n\nmodSexe = smf.glm(\"Nmalaria ~ 1 + Sexe\", data=Malaria, family=sm.families.Poisson()).fit()\nmodSexe.summary()\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\nNmalaria\nNo. Observations:\n1627\n\n\nModel:\nGLM\nDf Residuals:\n1625\n\n\nModel Family:\nPoisson\nDf Model:\n1\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-5250.8\n\n\nDate:\nTue, 04 Feb 2025\nDeviance:\n5706.3\n\n\nTime:\n16:05:15\nPearson chi2:\n5.98e+03\n\n\nNo. Iterations:\n5\nPseudo R-squ. (CS):\n0.002471\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n1.5215\n0.016\n92.661\n0.000\n1.489\n1.554\n\n\nSexe[T.M]\n0.0460\n0.023\n2.006\n0.045\n0.001\n0.091\n\n\n\n\n\nNous retrouvons que le coefficient constant (Intercept) est le logarithme népérien de la moyenne du nombre de visites chez les filles. La modalité fille est la première modalité de la variable Sexe par ordre alphabétique et constitue la modalité de référence. Le coefficient constant est ici le logarithme (qui est la fonction de lien) de la moyenne du nombre de visites chez les filles. L’effet M est ici la différence des logarithmes ce que nous retrouvons dans le second coefficient.\n\n\n\nExercice 7 (Table de contingence et loi de Poisson)  \n\n\ndata_crosstab = pd.crosstab(Malaria['Prev'],Malaria['Sexe'],margins = False) \nprint(data_crosstab) \n\nSexe               F    M\nPrev                     \nAutre              2    6\nMoustiquaire     557  543\nRien             223  233\nSerpentin/Spray   28   35\n\n\n\nres = chi2_contingency(data_crosstab)\nres.pvalue\n\n0.3697755421416906\n\n\n::: {#257ea9e5 .cell execution_count=9} ``` {.python .cell-code} Malaria[“Sexe”] = Malaria[“Sexe”].astype(“category”) Malaria[“Prev”] = Malaria[“Prev”].astype(“category”)\n# Grouper les données par ‘Sexe’ et ‘Prev’ et calculer les effectifs et la somme de ‘Nmalaria’ result = Malaria.groupby([‘Sexe’, ‘Prev’]).agg(effectif=(‘Nmalaria’, ‘size’), Y=(‘Nmalaria’, ‘sum’)).reset_index() print(result) ```\n::: {.cell-output .cell-output-stdout} Sexe             Prev  effectif     Y   0    F            Autre         2     8   1    F     Moustiquaire       557  2509   2    F             Rien       223  1073   3    F  Serpentin/Spray        28   119   4    M            Autre         6    24   5    M     Moustiquaire       543  2548   6    M             Rien       233  1134   7    M  Serpentin/Spray        35   211 ::: :::\n::: {#b5309d23 .cell execution_count=10} {.python .cell-code}   mod1 = smf.glm(\"Nmalaria ~ -1 + Sexe:Prev\", data = Malaria, family=sm.families.Poisson()).fit()   mod1.summary()\n::: {.cell-output .cell-output-display execution_count=10}\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\nNmalaria\nNo. Observations:\n1627\n\n\nModel:\nGLM\nDf Residuals:\n1619\n\n\nModel Family:\nPoisson\nDf Model:\n7\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-5242.5\n\n\nDate:\nTue, 04 Feb 2025\nDeviance:\n5689.7\n\n\nTime:\n16:05:15\nPearson chi2:\n5.95e+03\n\n\nNo. Iterations:\n5\nPseudo R-squ. (CS):\n0.01263\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nSexe[F]:Prev[Autre]\n1.3863\n0.354\n3.921\n0.000\n0.693\n2.079\n\n\nSexe[M]:Prev[Autre]\n1.3863\n0.204\n6.791\n0.000\n0.986\n1.786\n\n\nSexe[F]:Prev[Moustiquaire]\n1.5051\n0.020\n75.389\n0.000\n1.466\n1.544\n\n\nSexe[M]:Prev[Moustiquaire]\n1.5460\n0.020\n78.036\n0.000\n1.507\n1.585\n\n\nSexe[F]:Prev[Rien]\n1.5710\n0.031\n51.462\n0.000\n1.511\n1.631\n\n\nSexe[M]:Prev[Rien]\n1.5825\n0.030\n53.289\n0.000\n1.524\n1.641\n\n\nSexe[F]:Prev[Serpentin/Spray]\n1.4469\n0.092\n15.784\n0.000\n1.267\n1.627\n\n\nSexe[M]:Prev[Serpentin/Spray]\n1.7965\n0.069\n26.096\n0.000\n1.662\n1.931\n\n\n\n::: :::\nLe modèle possède autant de paramètres que de points de design, il est donc saturé. On retrouve les estimations à l’aide du tableau de la question précédente. Par exemple, on a pour le premier estimateur :\n::: {#10a92712 .cell execution_count=11} {.python .cell-code}   np.log(8/2)\n::: {.cell-output .cell-output-display execution_count=11} 1.3862943611198906 ::: :::\n\nmod2 = smf.glm(\"Nmalaria ~ -1 + Sexe + Prev\", data = Malaria, family=sm.families.Poisson()).fit()\nmod2.summary()\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\nNmalaria\nNo. Observations:\n1627\n\n\nModel:\nGLM\nDf Residuals:\n1622\n\n\nModel Family:\nPoisson\nDf Model:\n4\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-5246.4\n\n\nDate:\nTue, 04 Feb 2025\nDeviance:\n5697.6\n\n\nTime:\n16:05:15\nPearson chi2:\n5.97e+03\n\n\nNo. Iterations:\n5\nPseudo R-squ. (CS):\n0.007828\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nSexe[F]\n1.3523\n0.178\n7.613\n0.000\n1.004\n1.700\n\n\nSexe[M]\n1.3974\n0.177\n7.901\n0.000\n1.051\n1.744\n\n\nPrev[T.Moustiquaire]\n0.1506\n0.177\n0.849\n0.396\n-0.197\n0.498\n\n\nPrev[T.Rien]\n0.2013\n0.178\n1.130\n0.258\n-0.148\n0.550\n\n\nPrev[T.Serpentin/Spray]\n0.2784\n0.185\n1.503\n0.133\n-0.085\n0.641\n\n\n\n\n\nCe modèle n’est pas saturé. Il est identique au modèle\n\nsmf.glm(\"Nmalaria ~ Sexe + Prev\", data = Malaria, family=sm.families.Poisson()).fit()\n\nmais propose une paramétrisation différente.\nOn calcule les AIC :\n\nprint(mod1.aic)\nprint(mod2.aic)\n\n10500.991109809473\n10502.88231091839\n\n\nOn privilégie le modèle 1 pour ce critère. Le résultat ne contredit pas celui de la question 2 puisqu’une interaction n’est pas liée à l’indépendance entre 2 variables.\n\n\n\n\n\n Retour au sommet",
    "crumbs": [
      "Corrections",
      "IV Le modèle linéaire généralisé",
      "13 Régression de Poisson"
    ]
  },
  {
    "objectID": "codes/chap1.html",
    "href": "codes/chap1.html",
    "title": "1 Régression simple",
    "section": "",
    "text": "La concentration en ozone\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.formula.api as smf\n\n\nozone = pd.read_csv(\"../donnees/ozone_simple.txt\", header=0, sep=\";\")\nplt.plot(ozone.T12, ozone.O3, '.k')\nplt.ylabel('O3')\nplt.xlabel('T12')\nplt.show()\n\n\n\n\n\n\n\n\n\nreg = smf.ols('O3 ~ T12', data=ozone).fit()\nreg.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nO3\nR-squared:\n0.279\n\n\nModel:\nOLS\nAdj. R-squared:\n0.264\n\n\nMethod:\nLeast Squares\nF-statistic:\n18.58\n\n\nDate:\nFri, 31 Jan 2025\nProb (F-statistic):\n8.04e-05\n\n\nTime:\n15:41:21\nLog-Likelihood:\n-220.96\n\n\nNo. Observations:\n50\nAIC:\n445.9\n\n\nDf Residuals:\n48\nBIC:\n449.7\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n31.4150\n13.058\n2.406\n0.020\n5.159\n57.671\n\n\nT12\n2.7010\n0.627\n4.311\n0.000\n1.441\n3.961\n\n\n\n\n\n\n\n\nOmnibus:\n2.252\nDurbin-Watson:\n1.461\n\n\nProb(Omnibus):\n0.324\nJarque-Bera (JB):\n1.348\n\n\nSkew:\n0.026\nProb(JB):\n0.510\n\n\nKurtosis:\n2.197\nCond. No.\n94.1\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\ngrille = pd.DataFrame({'T12': np.linspace(ozone.T12.min(), ozone.T12.max(), 100)})\ncalculprev = reg.get_prediction(grille)\nICdte = calculprev.conf_int(obs=False, alpha=0.05)\nprev = calculprev.predicted_mean\nICprev = calculprev.conf_int(obs=True, alpha=0.05)\n\n\nplt.plot(ozone.T12, ozone.O3, '.k')\nplt.ylabel('O3')\nplt.xlabel('T12')\nplt.plot(grille.T12,  prev, '-', lw=1, label=\"E(Y)\")\nlesic, = plt.plot(grille.T12, ICdte[:, 0], 'r--', label=r\"IC $\\mathbb{E}(Y)$\", lw=1)\nplt.plot(grille.T12, ICdte[:, 1], 'r--', lw=1)\nplt.legend(handles=[lesic,], loc='lower right')\n\n\n\n\n\n\n\n\n\nplt.plot(ozone.T12, ozone.O3, '.k')\nplt.ylabel('O3') ; plt.xlabel('T12')\nplt.plot(grille.T12,  prev, '-', lw=1, label=\"E(Y)\")\nlesic, = plt.plot(grille.T12, ICdte[:, 0], 'r--', label=r\"IC $\\mathbb{E}(Y)$\", lw=1)\nplt.plot(grille.T12, ICdte[:, 1],'r--', lw=1)\n\nlesic2, = plt.plot(grille.T12, ICprev[:, 0], 'g:', label=r\"IC $Y$\", lw=1)\nplt.plot(grille.T12, ICprev[:, 1],'g:', lw=1)\nplt.legend(handles=[lesic, lesic2], loc='lower right')\n\n\n\n\n\n\n\n\n\nICparams = reg.conf_int(alpha=0.05)\nround(ICparams, 3)\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\nIntercept\n5.159\n57.671\n\n\nT12\n1.441\n3.961\n\n\n\n\n\n\n\n\n\nLa hauteur des eucalyptus\n\neucalyptus = pd.read_csv(\"../donnees/eucalyptus.txt\", header=0, sep=\";\")\nplt.plot(eucalyptus.circ, eucalyptus.ht, '+k')\nplt.ylabel('ht')\nplt.xlabel('circ')\nplt.show()\n\n\n\n\n\n\n\n\n\nreg = smf.ols('ht ~ circ', data=eucalyptus).fit()\nreg.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nht\nR-squared:\n0.768\n\n\nModel:\nOLS\nAdj. R-squared:\n0.768\n\n\nMethod:\nLeast Squares\nF-statistic:\n4732.\n\n\nDate:\nFri, 31 Jan 2025\nProb (F-statistic):\n0.00\n\n\nTime:\n15:41:22\nLog-Likelihood:\n-2286.2\n\n\nNo. Observations:\n1429\nAIC:\n4576.\n\n\nDf Residuals:\n1427\nBIC:\n4587.\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n9.0375\n0.180\n50.264\n0.000\n8.685\n9.390\n\n\ncirc\n0.2571\n0.004\n68.792\n0.000\n0.250\n0.264\n\n\n\n\n\n\n\n\nOmnibus:\n7.943\nDurbin-Watson:\n1.067\n\n\nProb(Omnibus):\n0.019\nJarque-Bera (JB):\n8.015\n\n\nSkew:\n-0.156\nProb(JB):\n0.0182\n\n\nKurtosis:\n3.193\nCond. No.\n273.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\ngrille = pd.DataFrame({'circ' : np.linspace(eucalyptus.circ.min(), eucalyptus.circ.max(), 100)})\ncalculprev = reg.get_prediction(grille)\nICdte = calculprev.conf_int(obs=False, alpha=0.05)\nICprev = calculprev.conf_int(obs=True, alpha=0.05)\nprev = calculprev.predicted_mean\n\n\nplt.plot(eucalyptus.circ, eucalyptus.ht, '+k')\nplt.ylabel('ht') ; plt.xlabel('circ')\nplt.plot(grille.circ,  prev, '-', label=\"E(Y)\", lw=1)\nlesic, = plt.plot(grille.circ, ICdte[:, 0], 'r--', label=r\"IC $\\mathbb{E}(Y)$\", lw=1)\nplt.plot(grille.circ, ICdte[:, 1], 'r--', lw=1)\nplt.legend(handles=[lesic], loc='lower right')\n\n\n\n\n\n\n\n\n\nplt.plot(eucalyptus.circ, eucalyptus.ht, '+k')\nplt.ylabel('ht') ; plt.xlabel('circ')\nplt.plot(grille.circ,  prev, '-', label=\"E(Y)\", lw=1)\nlesic, = plt.plot(grille.circ, ICdte[:, 0], 'r--', label=r\"IC $\\mathbb{E}(Y)$\", lw=1)\nplt.plot(grille.circ, ICdte[:, 1], 'r--', lw=1)\nlesic2, = plt.plot(grille.circ, ICprev[:, 0], 'g:', label=r\"IC $Y$\", lw=1)\nplt.plot(grille.circ, ICprev[:, 1],'g:', lw=1)\nplt.legend(handles=[lesic, lesic2], loc='lower right')\n\n\n\n\n\n\n\n\n\n\n\n\n Retour au sommet",
    "crumbs": [
      "Codes",
      "I Introduction au modèle linéaire",
      "1 Régression simple"
    ]
  },
  {
    "objectID": "correction/chap14.html",
    "href": "correction/chap14.html",
    "title": "14 Régularisation de la vraisemblance",
    "section": "",
    "text": "import pandas as pd; import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegressionCV, LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import StratifiedKFold\nfrom patsy import dmatrix\n\n\nExercice 1 (Questions de cours)  \n\nA, B\nC\nA\nC, D\n\n\n\nExercice 2 (Choix du paramètre \\(\\lambda\\) sur une grille)  \n\nSAh = pd.read_csv(\"../donnees/SAh.csv\", header=0, sep=\",\")\n\n\nnomsvar = list(SAh.columns.difference([\"chd\"]))\nformule = \"~ 1 +\" + \"+\".join(nomsvar)\ndsX = dmatrix(formule, data=SAh)\nX = dmatrix(formule, data=SAh, return_type=\"dataframe\").\\\n    iloc[:,1:].to_numpy()\nY = SAh[\"chd\"].to_numpy()\n\n\nscalerX = StandardScaler().fit(X)\nXcr= scalerX.transform(X)\nl0 = np.abs(Xcr.transpose().dot((Y  - Y.mean()))).max()/X.shape[0]\nllc = np.linspace(0,-4,100)\nll = l0*10**llc\nCs_lasso = 1/ 0.9/ X.shape[0] / (l0*10**(llc))\nCs_ridge = 1/ 0.9/ X.shape[0] / ((l0*10**(llc)) * 100)\nCs_enet =  1/ 0.9/ X.shape[0] / ((l0*10**(llc)) / 0.5)\n\n\nskf = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n\n\ncr = StandardScaler()\nlassocvM1 =  LogisticRegressionCV(cv=skf, penalty=\"l1\", n_jobs=3, solver=\"liblinear\", Cs=Cs_lasso, scoring=\"neg_log_loss\")\nridgecvM1 = LogisticRegressionCV(cv=skf, penalty=\"l2\", n_jobs=3, Cs=Cs_ridge, scoring=\"neg_log_loss\")\nenetcvM1 = LogisticRegressionCV(cv=skf, penalty=\"elasticnet\", n_jobs=3, Cs= Cs_enet, solver=\"saga\", l1_ratios=[0.5], scoring=\"neg_log_loss\")\n\n\npipe_lassocvM1 = Pipeline(steps=[(\"cr\", cr), (\"lassocv\", lassocvM1)])\npipe_ridgecvM1 = Pipeline(steps=[(\"cr\", cr), (\"ridgecv\", ridgecvM1)])\npipe_enetcvM1 = Pipeline(steps=[(\"cr\", cr), (\"enetcv\", enetcvM1)])\n\n\npipe_lassocvM1.fit(X,Y)\npipe_ridgecvM1.fit(X,Y)\npipe_enetcvM1.fit(X,Y)\n\nPipeline(steps=[('cr', StandardScaler()),\n                ('enetcv',\n                 LogisticRegressionCV(Cs=array([6.77620047e-03, 7.43687165e-03, 8.16195745e-03, 8.95773823e-03,\n       9.83110664e-03, 1.07896274e-02, 1.18416028e-02, 1.29961444e-02,\n       1.42632524e-02, 1.56539020e-02, 1.71801381e-02, 1.88551803e-02,\n       2.06935371e-02, 2.27111314e-02, 2.49254387e-02, 2.73556382e-02,\n       3.00227792e-02, 3.29499631e-02...\n       1.67851660e+01, 1.84216989e+01, 2.02177918e+01, 2.21890016e+01,\n       2.43524018e+01, 2.67267309e+01, 2.93325542e+01, 3.21924420e+01,\n       3.53311654e+01, 3.87759104e+01, 4.25565138e+01, 4.67057214e+01,\n       5.12594715e+01, 5.62572067e+01, 6.17422149e+01, 6.77620047e+01]),\n                                      cv=StratifiedKFold(n_splits=10, random_state=0, shuffle=True),\n                                      l1_ratios=[0.5], n_jobs=3,\n                                      penalty='elasticnet',\n                                      scoring='neg_log_loss', solver='saga'))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('cr', StandardScaler()),\n                ('enetcv',\n                 LogisticRegressionCV(Cs=array([6.77620047e-03, 7.43687165e-03, 8.16195745e-03, 8.95773823e-03,\n       9.83110664e-03, 1.07896274e-02, 1.18416028e-02, 1.29961444e-02,\n       1.42632524e-02, 1.56539020e-02, 1.71801381e-02, 1.88551803e-02,\n       2.06935371e-02, 2.27111314e-02, 2.49254387e-02, 2.73556382e-02,\n       3.00227792e-02, 3.29499631e-02...\n       1.67851660e+01, 1.84216989e+01, 2.02177918e+01, 2.21890016e+01,\n       2.43524018e+01, 2.67267309e+01, 2.93325542e+01, 3.21924420e+01,\n       3.53311654e+01, 3.87759104e+01, 4.25565138e+01, 4.67057214e+01,\n       5.12594715e+01, 5.62572067e+01, 6.17422149e+01, 6.77620047e+01]),\n                                      cv=StratifiedKFold(n_splits=10, random_state=0, shuffle=True),\n                                      l1_ratios=[0.5], n_jobs=3,\n                                      penalty='elasticnet',\n                                      scoring='neg_log_loss', solver='saga'))]) StandardScaler?Documentation for StandardScalerStandardScaler() LogisticRegressionCV?Documentation for LogisticRegressionCVLogisticRegressionCV(Cs=array([6.77620047e-03, 7.43687165e-03, 8.16195745e-03, 8.95773823e-03,\n       9.83110664e-03, 1.07896274e-02, 1.18416028e-02, 1.29961444e-02,\n       1.42632524e-02, 1.56539020e-02, 1.71801381e-02, 1.88551803e-02,\n       2.06935371e-02, 2.27111314e-02, 2.49254387e-02, 2.73556382e-02,\n       3.00227792e-02, 3.29499631e-02, 3.61625438e-02, 3.96883472e-02,\n       4.35579121e-02, 4.78...\n       1.67851660e+01, 1.84216989e+01, 2.02177918e+01, 2.21890016e+01,\n       2.43524018e+01, 2.67267309e+01, 2.93325542e+01, 3.21924420e+01,\n       3.53311654e+01, 3.87759104e+01, 4.25565138e+01, 4.67057214e+01,\n       5.12594715e+01, 5.62572067e+01, 6.17422149e+01, 6.77620047e+01]),\n                     cv=StratifiedKFold(n_splits=10, random_state=0, shuffle=True),\n                     l1_ratios=[0.5], n_jobs=3, penalty='elasticnet',\n                     scoring='neg_log_loss', solver='saga') \n\n\n\ncr = StandardScaler()\nlassocvM2 =  LogisticRegressionCV(cv=skf, penalty=\"l1\", n_jobs=3, solver=\"liblinear\", Cs=Cs_lasso, scoring=\"accuracy\")\nridgecvM2 = LogisticRegressionCV(cv=skf, penalty=\"l2\", n_jobs=3, Cs=Cs_ridge, scoring=\"accuracy\")\nenetcvM2 = LogisticRegressionCV(cv=skf, penalty=\"elasticnet\", n_jobs=3, Cs= Cs_enet, solver=\"saga\", l1_ratios=[0.5], scoring=\"accuracy\")\npipe_lassocvM2 = Pipeline(steps=[(\"cr\", cr), (\"lassocv\", lassocvM2)])\npipe_ridgecvM2 = Pipeline(steps=[(\"cr\", cr), (\"ridgecv\", ridgecvM2)])\npipe_enetcvM2 = Pipeline(steps=[(\"cr\", cr), (\"enetcv\", enetcvM2)])\npipe_lassocvM2.fit(X,Y)\npipe_ridgecvM2.fit(X,Y)\npipe_enetcvM2.fit(X,Y)\n\n\ncr = StandardScaler()\nlassocvM3 =  LogisticRegressionCV(cv=skf, penalty=\"l1\", n_jobs=3, solver=\"liblinear\", Cs=Cs_lasso, scoring=\"roc_auc\")\nridgecvM3 = LogisticRegressionCV(cv=skf, penalty=\"l2\", n_jobs=3, Cs=Cs_ridge, scoring=\"roc_auc\")\nenetcvM3 = LogisticRegressionCV(cv=skf, penalty=\"elasticnet\", n_jobs=3, Cs= Cs_enet, solver=\"saga\", l1_ratios=[0.5], scoring=\"roc_auc\")\npipe_lassocvM3 = Pipeline(steps=[(\"cr\", cr), (\"lassocv\", lassocvM3)])\npipe_ridgecvM3 = Pipeline(steps=[(\"cr\", cr), (\"ridgecv\", ridgecvM3)])\npipe_enetcvM3 = Pipeline(steps=[(\"cr\", cr), (\"enetcv\", enetcvM3)])\npipe_lassocvM3.fit(X,Y)\npipe_ridgecvM3.fit(X,Y)\npipe_enetcvM3.fit(X,Y)\netape_lassoM3 = pipe_lassocvM3.named_steps[\"lassocv\"]\netape_lassoM3.Cs_\netape_lassoM3.scores_[1]\netape_lassoM3.scores_[1].mean(axis=0)\netape_lassoM3.scores_[1].std(axis=0)/np.sqrt(10)\n\netape_lassoM1 = pipe_lassocvM1.named_steps[\"lassocv\"]\netape_lassoM2 = pipe_lassocvM2.named_steps[\"lassocv\"]\netape_lassoM3 = pipe_lassocvM3.named_steps[\"lassocv\"]\netape_ridgeM1 = pipe_ridgecvM1.named_steps[\"ridgecv\"]\netape_ridgeM2 = pipe_ridgecvM2.named_steps[\"ridgecv\"]\netape_ridgeM3 = pipe_ridgecvM3.named_steps[\"ridgecv\"]\netape_enetM1 = pipe_enetcvM1.named_steps[\"enetcv\"]\netape_enetM2 = pipe_enetcvM2.named_steps[\"enetcv\"]\netape_enetM3 = pipe_enetcvM3.named_steps[\"enetcv\"]\n\n\nfig, axs = plt.subplots(3, 3)\naxs[0,0].errorbar(np.log(Cs_lasso), etape_lassoM1.scores_[1].mean(axis=0), etape_lassoM1.scores_[1].std(axis=0)/np.sqrt(10), fmt=\"-o\", mec=\"black\", mfc=\"black\", ms=0.2, color=\"#80808077\")\naxs[0,0].text(.99, .1, \"Lasso - Vraisemblance\", ha=\"right\", va=\"top\",transform=axs[0,0].transAxes)\naxs[0,1].errorbar(np.log(Cs_ridge), etape_ridgeM1.scores_[1].mean(axis=0), etape_ridgeM1.scores_[1].std(axis=0)/np.sqrt(10), fmt=\"-o\", mec=\"black\", mfc=\"black\", ms=0.2, color=\"#80808077\")\naxs[0,1].text(.99, .1, \"Ridge - Vraisemblance\", ha=\"right\", va=\"top\",transform=axs[0,1].transAxes)\naxs[0,2].errorbar(np.log(Cs_enet), etape_enetM1.scores_[1][:,:,0].mean(axis=0), etape_enetM1.scores_[1][:,:,0].std(axis=0)/np.sqrt(10), fmt=\"-o\", mec=\"black\", mfc=\"black\", ms=0.2, color=\"#80808077\")\naxs[0,2].text(.99, .1, \"ElasticNet - Vraisemblance\", ha=\"right\", va=\"top\",transform=axs[0,2].transAxes)\n##\naxs[1,0].errorbar(np.log(Cs_lasso), etape_lassoM2.scores_[1].mean(axis=0), etape_lassoM2.scores_[1].std(axis=0)/np.sqrt(10), fmt=\"-o\", mec=\"black\", mfc=\"black\", ms=0.2, color=\"#80808077\")\naxs[1,0].text(.99, .1, \"Lasso - Bien classés\", ha=\"right\", va=\"top\",transform=axs[1,0].transAxes)\naxs[1,1].errorbar(np.log(Cs_ridge), etape_ridgeM2.scores_[1].mean(axis=0), etape_ridgeM2.scores_[1].std(axis=0)/np.sqrt(10), fmt=\"-o\", mec=\"black\", mfc=\"black\", ms=0.2, color=\"#80808077\")\naxs[1,1].text(.99, .1, \"Ridge - Bien classés\", ha=\"right\", va=\"top\",transform=axs[1,1].transAxes)\naxs[1,2].errorbar(np.log(Cs_enet), etape_enetM2.scores_[1][:,:,0].mean(axis=0), etape_enetM2.scores_[1][:,:,0].std(axis=0)/np.sqrt(10), fmt=\"-o\", mec=\"black\", mfc=\"black\", ms=0.2, color=\"#80808077\")\naxs[1,2].text(.99, .1, \"ElasticNet - Bien classés\", ha=\"right\", va=\"top\",transform=axs[1,2].transAxes)\n##\naxs[2,0].errorbar(np.log(Cs_lasso), etape_lassoM3.scores_[1].mean(axis=0), etape_lassoM3.scores_[1].std(axis=0)/np.sqrt(10), fmt=\"-o\", mec=\"black\", mfc=\"black\", ms=0.2, color=\"#80808077\")\naxs[2,0].text(.99, .1, \"Lasso - AUC\", ha=\"right\", va=\"top\",transform=axs[2,0].transAxes)\naxs[2,1].errorbar(np.log(Cs_ridge), etape_ridgeM3.scores_[1].mean(axis=0), etape_ridgeM3.scores_[1].std(axis=0)/np.sqrt(10), fmt=\"-o\", mec=\"black\", mfc=\"black\", ms=0.2, color=\"#80808077\")\naxs[2,1].text(.99, .1, \"Ridge - AUC\", ha=\"right\", va=\"top\",transform=axs[2,1].transAxes)\naxs[2,2].errorbar(np.log(Cs_enet), etape_enetM3.scores_[1][:,:,0].mean(axis=0), etape_enetM3.scores_[1][:,:,0].std(axis=0)/np.sqrt(10), fmt=\"-o\", mec=\"black\", mfc=\"black\", ms=0.2, color=\"#80808077\")\naxs[2,2].text(.99, .1, \"ElasticNet - AUC\", ha=\"right\", va=\"top\",transform=axs[2,2].transAxes)\n\nText(0.99, 0.1, 'ElasticNet - AUC')\n\n\n\n\n\n\n\n\n\n\n\nExercice 3 (Comparaison de méthodes et courbes ROC)  \n\ndon = pd.read_csv(\"../donnees/regulglm_exo3.csv\", header=0, sep=\",\")\n\n\n\n\n\n Retour au sommet",
    "crumbs": [
      "Corrections",
      "IV Le modèle linéaire généralisé",
      "14 Régularisation de la vraisemblance"
    ]
  },
  {
    "objectID": "correction/chap9.html",
    "href": "correction/chap9.html",
    "title": "Régularisation des moindres carrés : ridge, lasso, elastic-net",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport statsmodels.api as sm\nfrom sklearn.linear_model import Lasso, LassoCV\nfrom sklearn.linear_model import Lasso, lasso_path\n\n\nExercice 1 (Questions de cours) A, B, B, B, A (pour un bon choix de \\(\\lambda\\)) et B, A, C et D.\n\n\nExercice 2 (Projection et régression ridge)  \n\n\nExercice 3 (Variance des valeurs ajustées avec une régression ridge)  \n\n\nExercice 4 (Nombre effectif de paramètres de la régression ridge)  \n\nRappelons que pour une valeur \\(\\kappa\\) donnée, le vecteur de coefficients de la régression ridge s’écrit \\[\n\\hat \\beta_{\\mathrm{ridge}}(\\kappa) = (X'X + \\kappa I)^{-1}X'Y.\n\\] et donc l’ajustement par la régression ridge est \\[\n\\hat Y_{\\mathrm{ridge}}(\\kappa)=X(X'X + \\kappa I)^{-1}X'Y=H^*(\\kappa)Y\n\\]\nSoit \\(U_i\\) le vecteur propre de \\(A\\) associé à la valeur propre \\(d^2_i\\). Nous avons donc par définition que \\[\n\\begin{eqnarray*}\nAU_i&=&d^2_iU_i\\\\\nAU_i+\\lambda U_i&=&d^2_iU_i+\\lambda U_i=(d^2_i+\\lambda) U_i\\\\\n(A+\\lambda I_p)U_i&=&(d^2_i+\\lambda) U_i,\n\\end{eqnarray*}\n\\] c’est-à-dire que \\(U_i\\) est aussi vecteur propre de \\(A+\\lambda I_p\\) associé à la valeur propre \\(\\lambda+d^2_i\\).\nNous savons que \\(X=QD P'\\) avec \\(Q\\) et \\(P\\) matrices orthogonales et \\(D=\\text{diag}(d_1,\\dotsc,d_p)\\). Puisque \\(Q\\) est orthogonale, nous avons, par définition, \\(Q'Q=I\\). Nous avons donc que \\(X'X=(QD P')'QD P'=PDQ'QDP'=PD^2P'\\). Puisque \\(P\\) est orthogonale \\(P'P=I_p\\) et \\(P^{-1}=P\\). \\[\n\\begin{eqnarray*}\n\\text{tr}(X(X'X+\\lambda I_p)^{-1}X')&=&\\text{tr}((X'X+\\lambda I_p)^{-1}X'X)\\\\\n&=&\\text{tr}((PD^2P'+\\lambda PP')^{-1}PD^2P')\\\\\n&=&\\text{tr}((P(D+\\lambda I_p )P')^{-1}PD^2P').\n\\end{eqnarray*}\n\\] Ainsi \\[\n\\begin{eqnarray*}\n\\text{tr}(X(X'X+\\lambda I_p)^{-1}X')&=&\\text{tr}( (P')^{-1}(D+\\lambda I_p )^{-1} P^{-1} PD^2P')\\\\\n&=&\\text{tr}( (P')^{-1}(D+\\lambda I_p )^{-1} D^2P')\\\\\n&=&\\text{tr}( (D+\\lambda I_p )^{-1} D^2).\n\\end{eqnarray*}\n\\] Selon la définition de \\(H^*(\\kappa)\\), nous savons que sa trace vaut donc \\[\n\\begin{eqnarray*}\n\\text{tr}( (D+\\kappa I_p )^{-1} D^2).\n\\end{eqnarray*}\n\\] Comme \\(D\\) et \\(I_p\\) sont des matrices diagonales, leur somme et produit sont simplement leur somme et produit terme à terme des éléments de la diagonale, et donc cette trace (somme des éléments de la diagonale) vaut \\[\n\\sum_{i=1}^{p}{\\frac{d_j^2}{d_j^2+\\kappa}}.\n\\]\n\n\n\nExercice 5 (Estimateurs à rétrecissement - shrinkage)  \n\nSoit le modèle de régression \\[\nY=X\\beta+\\varepsilon.\n\\] En le pré-multipliant par \\(P\\), nous avons \\[\nZ=PY=PX\\beta+P\\varepsilon=DQ\\beta+\\eta=D\\gamma+\\eta.\n\\] Puisque \\(\\varepsilon\\sim\\mathcal{N}(0,\\sigma^2 I_n)\\) et \\(P\\) fixé, nous avons que \\(\\eta=P\\varepsilon\\) suit une loi normale de moyenne \\(\\mathbf E(\\eta)=P\\mathbf E(\\varepsilon)=0\\) et de variance \\(\\mathop{\\mathrm{V}}(\\eta)=P\\mathop{\\mathrm{V}}(\\varepsilon)P'=\\sigma^2PP'=\\sigma^2I_n\\).\nPar définition, \\(Z\\) vaut \\(PY\\) et nous savons que \\(Y\\sim\\mathcal{N}(X\\beta,\\sigma^2 I_n)\\), donc \\(Z\\sim\\mathcal{N}(PX\\beta,\\sigma^2 PP')\\), c’est-à-dire \\(Z\\sim\\mathcal{N}(DQ\\beta,\\sigma^2 I_n)\\) ou encore \\(Z\\sim\\mathcal{N}(D\\gamma,\\sigma^2 I_n)\\). En utilisant la valeur de \\(D\\) nous avons \\[    \n\\begin{eqnarray*}\nD\\gamma&=&\n\\begin{pmatrix}\n  \\Delta \\gamma\\\\\n0\n\\end{pmatrix}.\n\\end{eqnarray*}\n\\] Donc \\(Z_{1:p}\\sim\\mathcal{N}(\\Delta\\gamma,\\sigma^2I_p)\\).\nSoit un estimateur de \\(\\beta\\) linéaire en \\(Y\\)~: \\(\\hat \\beta=AY\\). Soit l’estimateur de \\(\\gamma=Q\\beta\\) linéaire en \\(Y\\)~: \\(\\hat\\gamma=Q AY\\). Pour calculer leur matrice de l’EQM, nous devons calculer leur biais et leur variance. Le biais de \\(\\hat \\beta\\) est \\[\nB(\\hat \\beta)=\\mathbf E(\\hat \\beta)-\\beta=\\mathbf E(AY)-\\beta=A\\mathbf E(Y)-\\beta=AX\\beta-\\beta.\n\\] Le biais de \\(\\hat\\gamma\\) s’écrit \\[\nB(\\hat\\gamma)=\\mathbf E(\\hat \\gamma)-\\gamma=\\mathbf E(Q\\hat \\beta)-\\gamma=Q\\mathbf E(\\hat \\beta)-\\gamma=QAX\\beta-\\gamma.\n\\] Comme \\(\\gamma=Q\\beta\\) et \\(Q'Q=I_p\\) nous avons \\[\nB(\\hat\\gamma)=QAXQ'\\gamma-\\gamma.\n\\] La variance de \\(\\hat \\beta\\) s’écrit \\[\n\\mathop{\\mathrm{V}}(\\hat \\beta)=\\mathop{\\mathrm{V}}(AY)=A\\mathop{\\mathrm{V}}(Y)A'=\\sigma^2 AA',\n\\] et celle de \\(\\hat \\gamma\\) est \\[\n\\mathop{\\mathrm{V}}(\\hat\\gamma)=\\mathop{\\mathrm{V}}(Q\\hat \\beta)=Q\\mathop{\\mathrm{V}}(\\hat \\beta)Q'=\\sigma^2 QAA'Q'.\n\\] Nous en déduisons que les matrices des EQM sont respectivement \\[\n\\begin{eqnarray*}\n\\mathop{\\mathrm{EQM}}(\\hat \\beta)&=&(AX\\beta-\\beta)(AX\\beta-\\beta)'+\\sigma^2 AA',\\\\\n\\mathop{\\mathrm{EQM}}(\\hat \\gamma)&=&(QAXQ'\\gamma-\\gamma)(QAXQ'\\gamma-\\gamma)' + \\sigma^2 QAA'Q',\n\\end{eqnarray*}\n\\] et enfin les traces de ces matrices s’écrivent \\[\n\\begin{eqnarray*}\n\\text{tr}(\\mathop{\\mathrm{EQM}}(\\hat \\beta))&=&(AX\\beta-\\beta)'(AX\\beta-\\beta)+\\sigma^2\\text{tr}(AA'),\\\\\n\\text{tr}(\\mathop{\\mathrm{EQM}}(\\hat \\gamma))&=&(QAXQ'\\gamma-\\gamma)'(QAXQ'\\gamma-\\gamma)+ \\sigma^2\\text{tr}(AA').\\\\\n\\end{eqnarray*}\n\\] Rappelons que \\(\\gamma=Q\\beta\\) et que \\(Q'Q=I_p\\), nous avons donc \\[\n\\begin{eqnarray*}\n\\text{tr}(\\mathop{\\mathrm{EQM}}(\\hat \\gamma))&=&\\gamma'(QAXQ'-I_p)'(QAXQ'-I_p)\\gamma+ \\sigma^2\\text{tr}(AA')\\\\\n&=&\\beta'(QAX - Q)'(QAX - Q)\\beta+ \\sigma^2\\text{tr}(AA')\\\\\n&=&\\beta'(AX-I_p)Q'Q(AX-I_p)\\beta+ \\sigma^2\\text{tr}(AA')\\\\\n&=&\\beta'(AX-I_p)(AX-I_p)\\beta+ \\sigma^2\\text{tr}(AA')=\\text{tr}(\\mathop{\\mathrm{EQM}}(\\hat \\beta)).\n\\end{eqnarray*}\n\\] En conclusion, que l’on s’intéresse à un estimateur linéaire de \\(\\beta\\) ou à un estimateur linéaire de \\(\\gamma\\), dès que l’on passe de l’un à l’autre en multipliant par \\(Q\\) ou \\(Q'\\), matrice orthogonale, la trace de l’EQM est identique, c’est-à-dire que les performances globales des 2 estimateurs sont identiques.\nNous avons le modèle de régression suivant~: \\[\nZ_{1:p}=\\Delta\\gamma+\\eta_{1:p},\n\\] et donc, par définition de l’estimateur des MC, nous avons \\[\n\\hat \\gamma_{\\mathrm{MC}}=(\\Delta'\\Delta)^{-1}\\Delta'Z_{1:p}.\n\\] Comme \\(\\Delta\\) est une matrice diagonale, nous avons \\[\n\\hat \\gamma_{\\mathrm{MC}}=\\Delta^{-2}\\Delta'Z_{1:p}=\\Delta^{-1}Z_{1:p}.\n\\] Cet estimateur est d’expression très simple et il est toujours défini de manière unique, ce qui n’est pas forcément le cas de \\(\\hat \\beta_{\\mathrm{MC}}\\).\nComme \\(Z_{1:p}\\sim\\mathcal{N}(\\Delta\\gamma,\\sigma^2 I_p)\\) nous avons que \\(\\hat \\gamma_{\\mathrm{MC}}=\\Delta^{-1}Z_{1:p}\\) suit une loi normale d’espérance \\(\\mathbf E(\\Delta^{-1}Z_{1:p})=\\Delta^{-1}\\mathbf E(Z_{1:p})=\\gamma\\) et de variance \\(\\mathop{\\mathrm{V}}(\\hat \\gamma_{\\mathrm{MC}})=\\sigma^2\\Delta^{-2}\\). Puisque \\(\\hat \\gamma_{\\mathrm{MC}}\\) est un estimateur des MC, il est sans biais, ce qui est habituel.\nL’EQM de \\(\\hat \\gamma_{\\mathrm{MC}}\\), estimateur sans biais, est simplement sa variance. Pour la \\(i^e\\) coordonnée de \\(\\hat \\gamma_{\\mathrm{MC}}\\), l’EQM est égal à l’élément \\(i,i\\) de la matrice de variance \\(\\mathop{\\mathrm{V}}(\\hat \\gamma_{\\mathrm{MC}})\\), c’est-à-dire \\(\\sigma^2/\\delta_i^2\\). La trace de l’EQM est alors simplement la somme, sur toutes les coordonnées \\(i\\), de cet EQM obtenu.\nPar définition \\(\\hat \\gamma(c)=\\text{diag}(c_i)Z_{1:p}\\) et nous savons que \\(Z_{1:p}\\sim\\mathcal{N}(\\Delta\\gamma,\\sigma^2 I_p).\\) Nous obtenons que \\(\\hat \\gamma(c)\\) suit une loi normale d’espérance \\(\\mathbf E(\\text{diag}(c_i)Z_{1:p})=\\text{diag}(c_i)\\Delta\\gamma\\) et de variance \\[\n\\mathop{\\mathrm{V}}(\\hat \\gamma(c))= \\text{diag}(c_i)\\mathop{\\mathrm{V}}(Z_{1:p})\\text{diag}(c_i)'= \\sigma^2\\text{diag}(c_i^2).\n\\] La loi de \\(\\hat \\gamma(c)\\) étant une loi normale de matrice de variance diagonale, nous en déduisons que les coordonnées de \\(\\hat \\gamma(c)\\) sont indépendantes entre elles.\nCalculons l’EQM de la \\(i^e\\) coordonnée de \\(\\hat \\gamma(c)\\) \\[\n\\mathop{\\mathrm{EQM}}(\\hat \\gamma(c)_i)=\\mathbf E(\\hat \\gamma(c)_i -\\gamma)^2=\\mathbf E(\\hat \\gamma(c)_i^2)+\n\\mathbf E(\\gamma_i^2)-2\\mathbf E(\\hat \\gamma(c)_i \\gamma_i).\n\\] Comme \\(\\gamma_i\\) et que \\(\\mathbf E(\\hat \\gamma(c)_i^2)=\\mathop{\\mathrm{V}}(\\hat \\gamma(c)_i^2)+\\{\\mathbf E(\\hat \\gamma(c)_i^2)\\}^2\\), nous avons \\[\n\\begin{align*}\n\\mathop{\\mathrm{EQM}}(\\hat \\gamma(c)_i)&=\\sigma^2 c_i^2+(c_i\\delta_i\\gamma_i)^2+\\gamma_i^2-2\\gamma_i\\mathbf E(\\hat \\gamma(c)_i)\\\\\n&=\\sigma^2 c_i^2+(c_i\\delta_i\\gamma_i)^2+\\gamma_i^2-2\\sigma^2 c_i\\delta_i\\gamma_i= \\sigma^2c_i^2+\\gamma_i^2(c_i\\delta_i -1)^2.\n\\end{align*}\n\\]\nDe manière évidente si \\(\\gamma_i^2\\) diminue, alors l’EQM de \\(\\hat \\gamma(c)_i\\) diminue aussi. Calculons la valeur de l’EQM quand \\(\\gamma_i^2=\\frac{\\sigma^2}{\\delta_i^2}\\frac{(1/\\delta_i)+c_i}{(1/\\delta_i)-c_i}\\). Nous avons, grâce à la question précédente, \\[\n\\begin{eqnarray*}\n\\mathop{\\mathrm{EQM}}(\\hat \\gamma(c)_i)&=&\\sigma^2 c_i^2+(c_i\\delta_i -1)^2\\frac{\\sigma^2}{\\delta_i^2}\\frac{(1/\\delta_i)+c_i}{(1/\\delta_i)-c_i}\\\\\n&=&\\sigma^2 c_i^2+\\frac{\\sigma^2}{\\delta_i^2}(1 - c_i\\delta_i)^2\\frac{1+\\delta_ic_i}{1-\\delta_ic_i}\\\\\n&=&\\sigma^2 c_i^2+\\frac{\\sigma^2}{\\delta_i^2}(1 - c_i\\delta_i)(1+\\delta_ic_i)\\\\\n&=&\\sigma^2 c_i^2+\\frac{\\sigma^2}{\\delta_i^2}(1-\\delta_i^2c_i^2)\\\\\n&=&\\sigma^2 c_i^2+\\frac{\\sigma^2}{\\delta_i^2}-\\sigma^2c_i^2=\\frac{\\sigma^2}{\\delta_i^2}\\\\\n&=&\\mathop{\\mathrm{EQM}}(\\hat \\gamma_{\\mathrm{MC}}),\n\\end{eqnarray*}\n\\] d’où la conclusion demandée.\nPar définition de \\(\\hat \\gamma(c)\\), nous avons \\[\n\\begin{eqnarray*}\n\\hat \\gamma(c)&=&\\text{diag}(c_i)Z_{1:p}=\\text{diag}(\\frac{\\delta_i}{\\delta_i^2+\\kappa})Z_{1:p}\\\\\n&=&(\\Delta'\\Delta + \\kappa I_p)^{-1}\\Delta'Z_{1:p},\n\\end{eqnarray*}\n\\] puisque \\(\\Delta\\) est diagonale. De plus nous avons \\[\nD =\n\\bigl( \\begin{smallmatrix}\n  \\Delta\\\\\n0\n\\end{smallmatrix}\\bigr),\n\\] ce qui entraîne que \\(D'D=\\Delta'\\Delta\\) et \\(D'Z=\\Delta' Z_{1:p}\\). Nous obtenons donc \\[\n\\hat \\gamma(c)=(D'D+\\kappa I_p)^{-1}D'Z.\n\\] Rappelons que \\(D=PXQ'\\) avec \\(P\\) et \\(Q\\) matrices orthogonales, nous avons alors \\[\n\\begin{eqnarray*}\n\\hat \\gamma(c)&=&(QX'P'PXQ' + \\kappa I_p)^{-1} D'Z=(QX'XQ' + \\kappa QQ')^{-1}D'Z\\\\\n&=&(Q(X'X  + \\kappa I_p)Q')^{-1}D'Z=(Q')^{-1}(X'X  + \\kappa I_p)^{-1}(Q)^{-1}D'Z\\\\\n&=&Q(X'X  + \\kappa I_p)^{-1}Q'D'Z.\n\\end{eqnarray*}\n\\] Comme \\(Z=PY\\) et \\(D=PXQ'\\), nous avons \\[\n\\hat \\gamma(c)=Q(X'X  + \\kappa I_p)^{-1}Q' QX'P' PY=Q(X'X  + \\kappa I_p)^{-1}XY.\n\\] Enfin, nous savons que \\(Q\\hat\\gamma=\\hat \\beta\\), nous en déduisons que \\(\\hat\\gamma=Q'\\hat \\beta\\) et donc que dans le cas particulier où \\(c_i=\\frac{\\delta_i}{\\delta_i^2+\\kappa}\\) nous obtenons \\[\n\\hat \\beta=Q\\hat \\gamma(c)=(X'X  + \\kappa I_p)^{-1}XY,\n\\] c’est-à-dire l’estimateur de la régression ridge.\n\n\n\nExercice 6 (Coefficient constant et régression sous contraintes)  \n\n\nExercice 7 (Unicité pour la régression lasso, Giraud (2014))  \n\n\nExercice 8 (Traitement d’un signal)  \n\n\nsignal = pd.read_csv(\"../donnees/courbe_lasso.csv\")\ndonnees = pd.read_csv(\"../donnees/echan_lasso.csv\")\n\nplt.figure(figsize=(10, 6))\nplt.plot(signal['x'], signal['y'], label='Signal')\nplt.scatter(donnees['X'], donnees['Y'], color='red', label='Données')\n\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.title('Signal et Données')\n\nplt.show()\n\n\n\n\n\n\n\n\nNous cherchons à reconstruire le signal à partir de l’échantillon. Bien entendu, vu la forme du signal, un modèle linéaire de la forme \\[\ny_i=\\beta_0+\\beta_1x_i+\\varepsilon_i\n\\] n’est pas approprié. De nombreuses approches en traitement du signal proposent d’utiliser une base ou dictionnaire représentée par une collection de fonctions \\(\\{\\psi_j(x)\\}_{j=1,\\dots,K}\\) et de décomposer le signal dans cette base : \\[\nm(x)\\approx \\sum_{j=1}^K \\beta_j\\psi_j(x).\n\\] Pour un dictionnaire donné, on peut alors considérer un modèle linéaire \\[\n  y_i=\\sum_{j=1}^K \\beta_j\\psi_j(x_i)+\\varepsilon_i.\n\\tag{1}\\] Le problème est toujours d’estimer les paramètres \\(\\beta_j\\) mais les variables sont maintenant définies par les éléments du dictionnaire. Il existe différents types de dictionnaire, dans cet exercice nous proposons de considérer la base de Fourier définie par \\[\n\\psi_0(x)=1,\\quad \\psi_{2j-1}(x)=\\cos(2j\\pi x)\\quad\\text{et}\\quad \\psi_{2j}(x)=\\sin(2j\\pi x),\\quad j=1,\\dots,K.\n\\]\n\ndef mat_dict(K, x):\n    # Initialiser une matrice de zéros avec la taille appropriée\n    res = np.zeros((len(x), 2 * K))\n\n    # Remplir la matrice avec les valeurs cos et sin\n    for j in range(1, K + 1):\n        res[:, 2 * j - 2] = np.cos(2 * j * np.pi * x)\n        res[:, 2 * j - 1] = np.sin(2 * j * np.pi * x)\n\n    # Convertir la matrice en DataFrame pour un usage similaire à tibble\n    res_df = pd.DataFrame(res)\n\n    return res_df\n\nIl suffit d’ajuster le modèle linéaire où les variables explicatives sont données par le dictionnaire :\n\n# Créer le dictionnaire pour les données\nD25 = mat_dict(25, donnees['X'])\nD25['Y'] = donnees['Y']\n\n# Ajuster le modèle linéaire\nX = sm.add_constant(D25.drop(columns='Y'))\ny = D25['Y']\nmod_lin = sm.OLS(y, X).fit()\n\n# Créer le dictionnaire pour le signal\nS25 = mat_dict(25, signal['x'])\n\n# Faire des prédictions\nS25 = sm.add_constant(S25)\nprev_MCO = mod_lin.predict(S25)\n\n# Préparer les données pour le tracé\nsignal1 = signal.copy()\nsignal1['MCO'] = prev_MCO\nsignal1 = signal1.rename(columns={'y': 'signal'})\nsignal2 = signal1.melt(id_vars=['x'], value_vars=['signal', 'MCO'], var_name='meth', value_name='y')\n\n# Tracer les résultats\nplt.figure(figsize=(10, 6))\nfor key, grp in signal2.groupby('meth'):\n    plt.plot(grp['x'], grp['y'], label=key)\nplt.scatter(donnees['X'], donnees['Y'], color='red', label='Données')\nplt.ylim(-2, 2)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.title('Signal et Données')\nplt.show()\n\n\n\n\n\n\n\n\nLe signal estimé a tendance à surajuster les données. Cela vient du fait qu’on estime 51 paramètres avec seulement 60 observations.\nOn regarde tout d’abord le chemin de régularisation des estimateurs lasso\n\nX_25 = D25.drop(columns='Y').values\ny_25 = D25['Y'].values\n\n# Ajuster le modèle Lasso et obtenir le chemin de régularisation\nalphas, coefs, _ = lasso_path(X_25, y_25, alphas=np.logspace(-4, 0, 100))\n\n# Tracer le chemin de régularisation\nplt.figure(figsize=(10, 6))\n# Tracer les coefficients pour chaque alpha\nfor coef in coefs:\n    plt.plot(-np.log10(alphas), coef)\n\nplt.xlabel('-log(alpha)')\nplt.ylabel('Coefficients')\nplt.title('Lasso Paths')\nplt.axis('tight')\nplt.show()\n\n\n\n\n\n\n\n\nIl semble que quelques coefficients quittent la valeur 0 bien avant les autres. On effectue maintenant la validation croisée pour sélectionner le paramètre \\(\\lambda\\).\n\nlasso_cv = LassoCV(cv=10, random_state=1234).fit(X_25, y_25)\n\n# Tracer les résultats de la validation croisée\nm_log_alphas = -np.log10(lasso_cv.alphas_)\n\nplt.figure(figsize=(10, 6))\nplt.plot(m_log_alphas, lasso_cv.mse_path_, ':')\nplt.plot(m_log_alphas, lasso_cv.mse_path_.mean(axis=-1), 'k', label='Average across the folds', linewidth=2)\nplt.axvline(-np.log10(lasso_cv.alpha_), linestyle='--', color='k', label='alpha: CV estimate')\n\nplt.xlabel('-log(alpha)')\nplt.ylabel('Mean square error')\nplt.title('Lasso CV Paths')\nplt.legend()\nplt.axis('tight')\nplt.show()\n\n\n\n\n\n\n\n\n\nS25 = mat_dict(25, signal['x'])\nprev_lasso = lasso_cv.predict(S25)\nsignal1['lasso'] = prev_lasso\nsignal1 = signal1.rename(columns={'y': 'signal'})\nsignal2 = signal1.melt(id_vars=['x'], value_vars=['signal','MCO' ,'lasso'], var_name='meth', value_name='y')\n# Tracer les résultats\nplt.figure(figsize=(10, 6))\nfor key, grp in signal2.groupby('meth'):\n    plt.plot(grp['x'], grp['y'], label=key)\nplt.scatter(donnees['X'], donnees['Y'], color='red', label='Données')\nplt.ylim(-2, 2)  # Fixer les limites de l'axe des ordonnées\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.title('Signal et Données avec Lasso')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nExercice 9 (GridSearchCV)  \n\n\nExercice 10 (Variablilité de la validation croisée \\(K\\) blocs)  \n\nIl suffit d’écrire \\[\n\\bar{\\mathcal{R}}=\\frac{1}{n}\\sum_{k=1}^K \\sum_{i\\in\\mathcal I_k}(y_i-\\hat m_k(x_i))^2=\\frac{1}{K}\\sum_{k=1}^K\\frac{n}{K}\\sum_{i\\in\\mathcal I_k}(y_i-\\hat m_k(x_i))^2.\n\\]\nLes données étant i.i.d. et les blocs étant de taille égale, on a \\(\\mathop{\\mathrm{V}}(\\mathcal R_1)=\\mathop{\\mathrm{V}}(\\mathcal R_2)=\\dots=\\mathop{\\mathrm{V}}(\\mathcal R_K)\\). On obtient donc le résutat demandé en négligeant les covariances.\nOn estime la variance d’un bloc \\(\\mathop{\\mathrm{V}}(\\mathcal R_1)\\) par la variance empirique des erreurs sur chaque bloc : \\[\n\\frac{1}{K-1}\\sum_{k=1}^K(\\mathcal{R}_k - \\bar{\\mathcal{R}})^{2}.\n\\] En utilisant la question précédente, on estime la variance de \\(\\bar{\\mathcal{R}}\\) par \\[\n\\mathop{\\mathrm{V}}(\\bar{\\mathcal{R}}) = \\frac{1}{K(K-1)}\\sum_{k=1}^K(\\mathcal{R}_k - \\bar{\\mathcal{R}})^{2}.\n\\]\n\n\n\n\n\n Retour au sommet",
    "crumbs": [
      "Corrections",
      "III Réduction de dimension",
      "Régularisation des moindres carrés : ridge, lasso, elastic-net"
    ]
  },
  {
    "objectID": "correction/chap5.html",
    "href": "correction/chap5.html",
    "title": "5 Régression polynomiale et régression spline",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\n\nExercice 1 (Questions de cours)  \n\nC\nA\nA\nB\n\n\n\nExercice 2 (Fonction polyreg)  \n\nOn importe les données :\n\nozone = pd.read_csv(\"../donnees/ozone_simple.txt\", sep=\";\")\nsdT12 = np.std(ozone['T12'], ddof=1)\nprint(sdT12)\n\n4.674638477781911\n\n\nOn crée la grille\n\ngrillex = np.linspace(ozone['T12'].min() - sdT12, ozone['T12'].max() + sdT12, 100)\n\nOn transforme en data frame :\n\ndf = pd.DataFrame({'T12': grillex})\n\nOn effectue une regression polynomiale de degré 3 :\n\nbasepoly = np.column_stack([ozone['T12']**i for i in range(1, 4)])\nnewval = np.column_stack([df['T12']**i for i in range(1, 4)])\ndfpoly = pd.DataFrame(basepoly, columns=[f'T12^{i}' for i in range(1, 4)])\ndfpoly['O3'] = ozone['O3']\nX = sm.add_constant(dfpoly.drop(columns='O3'))\nY = dfpoly['O3']\nregpoly = sm.OLS(Y, X).fit()\nprint(regpoly.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                     O3   R-squared:                       0.536\nModel:                            OLS   Adj. R-squared:                  0.505\nMethod:                 Least Squares   F-statistic:                     17.69\nDate:                Sat, 01 Feb 2025   Prob (F-statistic):           8.92e-08\nTime:                        18:01:00   Log-Likelihood:                -209.96\nNo. Observations:                  50   AIC:                             427.9\nDf Residuals:                      46   BIC:                             435.6\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         69.2720     85.128      0.814      0.420    -102.081     240.625\nT12^1          7.4578     14.405      0.518      0.607     -21.538      36.454\nT12^2         -0.7730      0.784     -0.986      0.330      -2.352       0.806\nT12^3          0.0208      0.014      1.519      0.136      -0.007       0.048\n==============================================================================\nOmnibus:                        2.509   Durbin-Watson:                   1.466\nProb(Omnibus):                  0.285   Jarque-Bera (JB):                1.745\nSkew:                           0.251   Prob(JB):                        0.418\nKurtosis:                       2.235   Cond. No.                     4.15e+05\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 4.15e+05. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\nOn prévoit sur lagrille :\n\ndfnewval = pd.DataFrame(newval, columns=[f'T12^{i}' for i in range(1, 4)])\ndfnewval = sm.add_constant(dfnewval)\nprev = regpoly.predict(dfnewval)\nplt.scatter(ozone['T12'], ozone['O3'], label='Données')\nplt.plot(grillex, prev, color='red', label='Prédictions')\nplt.xlabel('T12')\nplt.ylabel('O3')\nplt.xlim(5,30)\nplt.ylim(40,140)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nCréation de la fonction :\n\ndef polyreg(ozone, degre=3):\n    # Calculer l'écart-type de la colonne T12\n    sdT12 = np.std(ozone['T12'], ddof=1)\n\n    # Créer la grille\n    grillex = np.linspace(ozone['T12'].min() - sdT12, ozone['T12'].max() + sdT12, 100)\n\n    # Transformer en DataFrame\n    df = pd.DataFrame({'T12': grillex})\n\n    # Créer les termes polynomiaux de degré 3\n    basepoly = np.column_stack([ozone['T12']**i for i in range(1, degre + 1)])\n\n    # Prédire les nouvelles valeurs polynomiales\n    newval = np.column_stack([df['T12']**i for i in range(1, degre + 1)])\n\n    # Créer le DataFrame pour la régression\n    dfpoly = pd.DataFrame(basepoly, columns=[f'T12^{i}' for i in range(1, degre + 1)])\n    dfpoly['O3'] = ozone['O3']\n\n    # Effectuer la régression polynomiale\n    X = sm.add_constant(dfpoly.drop(columns='O3'))\n    Y = dfpoly['O3']\n    regpoly = sm.OLS(Y, X).fit()\n\n    # Prédire les nouvelles valeurs\n    dfnewval = pd.DataFrame(newval, columns=[f'T12^{i}' for i in range(1, degre + 1)])\n    dfnewval = sm.add_constant(dfnewval)\n    prev = regpoly.predict(dfnewval)\n\n    return grillex, prev\n\n\n\n\nExercice 3 (Fonction polyreg (suite)) On applique la fonction précédente :\n\nozone = pd.read_csv(\"../donnees/ozone_simple.txt\", sep=\";\")\n# Tracer les données originales\nplt.scatter(ozone['T12'], ozone['O3'], label='Données')\nplt.xlim(0, 35)\nplt.ylim(0, 150)\n# Effectuer les régressions polynomiales et tracer les lignes\ncolors = ['blue', 'green', 'red', 'purple']\ndegrees = [1, 2, 3, 9]\nfor i, deg in enumerate(degrees):\n    grillex, prev = polyreg(ozone, degre=deg)\n    plt.plot(grillex, prev, color=colors[i], label=f'd={deg}')\n# Ajouter la légende\nplt.legend(loc='lower right')\nplt.xlabel('T12')\nplt.ylabel('O3')\nplt.xlim(5,35)\nplt.ylim(20,150)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nExercice 4 (Matrice bande) Considérons la matrice \\(X_B\\) du plan d’expérience obtenue à partir d’une variable réelle \\(X\\) transformée dans \\(\\mathcal{S}_{\\xi}^{d+1}\\) . Cette matrice est composée des \\(d+K+1\\) fonction de base notée \\(b_j\\) et où \\(K\\) est le nombre de noeuds intérieurs et \\(d\\) le degré.\nDans le cours, il est indiqué que les fonctions de base \\(b_j\\) et \\(b_{j+d+1}\\) en conservant l’ordre des fonctions. Donc \\(b_1\\) est orthogonale à toutes les fonctions \\(b_j\\) avec \\(j&gt;d+1\\), idem pour \\(b_2\\) avec \\(j&gt;d+2\\).\nEn faisant donc le calcul \\(X_B'X_B\\) on obtient une matrice bande et donc les termes \\(a_{ij}\\) sont nuls quand \\(j&gt;i+d+1\\).\nOn en déduit que les paramètres estimées \\(\\hat \\beta_k\\) ne sont pas corrélés avec les \\(\\hat \\beta_j\\) dès que \\(j&gt;k+d+1\\). XB est une matrice bande.\n\n\n\n\n Retour au sommet",
    "crumbs": [
      "Corrections",
      "I Introduction au modèle linéaire",
      "5 Régression polynomiale et régression spline"
    ]
  },
  {
    "objectID": "codes/chap3.html",
    "href": "codes/chap3.html",
    "title": "3 Validation du modèle",
    "section": "",
    "text": "import pandas as pd\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nfrom statsmodels.nonparametric.smoothers_lowess import lowess\n\n\nozone = pd.read_csv(\"../donnees/ozone_long.txt\", header = 0, sep = \";\")\nmod_lin6v = smf.ols(\"O3 ~ T6+T12+Ne12+Ne15+Vx+O3v\", data=ozone).fit()\n\n\ninfl = mod_lin6v.get_influence()\nindex = pd.Series(range(1, ozone.shape[0]+1))\nresloess = sm.nonparametric.lowess(infl.resid_studentized_external,index)\n\n\nplt.plot(index, infl.resid_studentized_external,\"+k\")\nplt.plot(resloess[:,0], resloess[:,1],\"r\")\nax = plt.gca()\nax.axhline(y=-2)\nax.axhline(y=2)\n\n\n\n\n\n\n\n\n\n#df = ozone.shape[0] - 3\nsm.qqplot(infl.resid_studentized_external, \\\n    stats.t,distargs=(ozone.shape[0]-infl.k_vars-1,), line='s')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncook, pval =infl.cooks_distance\nn, p =ozone.shape\nfig = plt.figure()\nplt.bar(index, cook, lw=2, color='k')\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nhii = infl.hat_matrix_diag\nseuil1 = 3*p/n\nseuil2 = 2*p/n\nplt.bar(index, hii, lw=2, color='k')\nax = plt.gca()\nax.axhline(y=seuil1, color='r', ls=':')\nax.axhline(y=seuil2, color='r', ls='--')\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nfig = sm.graphics.plot_ccpr_grid(mod_lin6v)\nfig.suptitle(\"\", fontsize=16)\nmarker_size = 2\nfor ax in fig.axes:\n    ax.set_ylabel('')\n    ax.lines[0].set_color('black')\n    for line in ax.get_lines():\n        line.set_markersize(marker_size)\n\nplt.subplots_adjust(top=0.99, bottom=-0.5, left=0.01, right=0.99, hspace=0.1, wspace=0.4)\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\ndef plot_ccpr_grid_with_loess(mod, exog_idx=None, grid=None, fig=None):\n    fig = sm.graphics.plot_ccpr_grid(mod, exog_idx, grid, fig)\n    fig.suptitle(\"\", fontsize=16)\n    marker_size = 2\n    for ax in fig.axes:\n        x = ax.lines[0].get_xdata()\n        y = ax.lines[0].get_ydata()\n        ax.set_ylabel('')\n        ax.lines[0].set_color('black')\n        for line in ax.get_lines():\n            line.set_markersize(marker_size)\n        # Rajout de loess en rouge\n        smooth = lowess(y, x, frac=2/3)\n        ax.plot(smooth[:, 0], smooth[:, 1], color='red', lw=2)\n    return fig\n\nfig = plot_ccpr_grid_with_loess(mod_lin6v)\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n Retour au sommet",
    "crumbs": [
      "Codes",
      "I Introduction au modèle linéaire",
      "3 Validation du modèle"
    ]
  },
  {
    "objectID": "correction/chap2.html",
    "href": "correction/chap2.html",
    "title": "2 La régression linéaire multiple",
    "section": "",
    "text": "Exercice 1 (Question de cours) A, A, B, B, B, C.\n\n\nExercice 2 (Covariance de \\(\\hat\\varepsilon\\) et \\(\\hat Y\\)) Nous allons montrer que, pour tout autre estimateur \\(\\tilde{\\beta}\\) de \\(\\beta\\) linéaire et sans biais, \\(\\mathop{\\mathrm{V}}(\\tilde{\\beta})  \\geq \\mathop{\\mathrm{V}}(\\hat \\beta)\\). Décomposons la variance de \\(\\tilde{\\beta}\\) \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}(\\tilde{\\beta})  = \\mathop{\\mathrm{V}}(\\tilde{\\beta} - \\hat \\beta+\\hat \\beta)\n=\\mathop{\\mathrm{V}}(\\tilde{\\beta} - \\hat \\beta)+\\mathop{\\mathrm{V}}(\\hat \\beta) -\n2 \\mathop{\\mathrm{Cov}}(\\tilde{\\beta} - \\hat \\beta,\\hat \\beta).\n\\end{eqnarray*}\\] Les variances étant définies positives, si nous montrons que \\(\\mathop{\\mathrm{Cov}}(\\tilde{\\beta}- \\hat \\beta,\\hat \\beta)=0\\), nous aurons fini la démonstration.\\ Puisque \\(\\tilde{\\beta}\\) est linéaire, \\(\\tilde{\\beta} = A Y\\). De plus, nous savons qu’il est sans biais, c’est-à-dire \\(\\mathbf E(\\tilde{\\beta}) = \\beta\\) pour tout \\(\\beta\\), donc \\(A X = I\\). La covariance devient : \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{Cov}}(\\tilde{\\beta} - \\hat \\beta,\\hat \\beta) &=&\n\\mathop{\\mathrm{Cov}}(A Y,(X'X)^{-1}X'Y) - \\mathop{\\mathrm{V}}(\\hat \\beta)\\\\\n&=& \\sigma^2 A X (X'X)^{-1} - \\sigma^2 (X'X)^{-1}=0.\n\\end{eqnarray*}\\]\n\n\nExercice 3 (Théorème de Gauss Markov) Nous devons montrer que, parmi tous les estimateurs linéaires sans biais, l’estimateur de MC est celui qui a la plus petite variance. La linéarité de \\(\\hat \\beta\\) est évidente. Calculons sa variance : \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}(\\hat \\beta) = \\mathop{\\mathrm{V}}((X'X)^{-1}X'Y) =\n(X'X)^{-1}X'\\mathop{\\mathrm{V}}(Y)X(X'X)^{-1}=\\sigma^2 (X'X)^{-1}.\n\\end{eqnarray*}\\] Nous allons montrer que, pour tout autre estimateur \\(\\tilde{\\beta}\\) de \\(\\beta\\) linéaire et sans biais, \\(\\mathop{\\mathrm{V}}(\\tilde{\\beta})  \\geq \\mathop{\\mathrm{V}}(\\hat \\beta)\\). Décomposons la variance de \\(\\tilde{\\beta}\\) \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}(\\tilde{\\beta})  = \\mathop{\\mathrm{V}}(\\tilde{\\beta} - \\hat \\beta+\\hat \\beta)\n=\\mathop{\\mathrm{V}}(\\tilde{\\beta} - \\hat \\beta)+\\mathop{\\mathrm{V}}(\\hat \\beta) -\n2 \\mathop{\\mathrm{Cov}}(\\tilde{\\beta} - \\hat \\beta,\\hat \\beta).\n\\end{eqnarray*}\\] Les variances étant définies positives, si nous montrons que \\(\\mathop{\\mathrm{Cov}}(\\tilde{\\beta}- \\hat \\beta,\\hat \\beta)=0\\), nous aurons fini la démonstration.\\ Puisque \\(\\tilde{\\beta}\\) est linéaire, \\(\\tilde{\\beta} = A Y\\). De plus, nous savons qu’il est sans biais, c’est-à-dire \\(\\mathbf E(\\tilde{\\beta}) = \\beta\\) pour tout \\(\\beta\\), donc \\(A X = I\\). La covariance devient : \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{Cov}}(\\tilde{\\beta} - \\hat \\beta,\\hat \\beta) &=&\n\\mathop{\\mathrm{Cov}}(A Y,(X'X)^{-1}X'Y) - \\mathop{\\mathrm{V}}(\\hat \\beta)\\\\\n&=& \\sigma^2 A X (X'X)^{-1} - \\sigma^2 (X'X)^{-1}=0.\n\\end{eqnarray*}\\]\n\n\nExercice 4 (Représentation des variables) Nous représentons les données dans \\(\\mathbb R^2\\) pour le premier jeu et dans \\(\\mathbb R^3\\) pour le second.\n\n\n\n\n\nDans le premier modèle, nous projetons \\(Y\\) sur l’espace engendré par \\(X\\), soit la droite de vecteur directeur \\(\\overrightarrow{OX}\\). Nous trouvons par le calcul \\(\\hat \\beta = 1.47\\), résultat que nous aurions pu trouver graphiquement car \\(\\overrightarrow{O \\hat Y}=\n\\hat \\beta . \\overrightarrow{OX}\\).\nConsidérons \\(\\mathbb R^3\\) muni de la base orthonormée \\((\\vec{i},\\vec{j},\\vec{k})\\). Les vecteurs \\(\\overrightarrow{OX}\\) et \\(\\overrightarrow{OZ}\\) engendrent le même plan que celui engendré par \\((\\vec{i},\\vec{j})\\). La projection de \\(Y\\) sur ce plan donne \\(\\overrightarrow{O \\hat Y}\\). Il est quasiment impossible de trouver \\(\\hat \\beta\\) et \\(\\hat \\gamma\\) graphiquement mais nous trouvons par le calcul \\(\\hat \\beta = -3.33\\) et \\(\\hat \\gamma =5\\).\n\n\nExercice 5 (Modèles emboîtés) Nous obtenons \\[\\begin{eqnarray*}\n\\hat Y_p = X \\hat \\beta \\quad  \\hbox{et} \\quad \\hat Y_q= X_q \\hat \\gamma.\n\\end{eqnarray*}\\] Par définition du \\(\\mathbb R2\\), il faut comparer la norme au carré des vecteurs \\(\\hat Y_p\\) et \\(\\hat Y_q\\). Notons les espaces engendrés par les colonnes de \\(X_q\\) et \\(X\\), \\(\\mathcal M_{X_q}\\) et \\(\\mathcal M_{X}\\), nous avons \\(\\mathcal M_{X_q} \\subset  \\mathcal M_{X}\\). Nous obtenons alors \\[\\begin{eqnarray*}\n\\hat Y_p = P_{X_p}Y\n= (P_{X_q} + P_{X^{\\perp}_q})P_{X_p}Y &=& P_{X_q}P_{X_p}Y + P_{X^{\\perp}_q}P_{X_p}Y\\\\\n&=& P_{X_q}Y + P_{X^{\\perp}_q \\cap X_p} Y\\\\\n&=& \\hat Y_q + P_{X^{\\perp}_q \\cap X_p} Y.\n\\end{eqnarray*}\\] En utilisant le théorème de Pythagore, nous avons \\[\\begin{eqnarray*}\n\\| \\hat Y_p \\|^2 &=& \\|\\hat Y_q \\|^2 + \\| P_{X^{\\perp}_q \\cap X_p} Y \\|^2\n\\geq \\|\\hat Y_q \\|^2,\n\\end{eqnarray*}\\] d’où \\[\\begin{eqnarray*}\n\\mathbb R2(p)=\\frac{\\| \\hat Y_p \\|^2}{\\| Y \\|^2} \\geq\n\\frac{\\| \\hat Y_q \\|^2}{\\| Y \\|^2} =\\mathbb R2(q).\n\\end{eqnarray*}\\]\nEn conclusion, lorsque les modèles sont emboîtés \\(\\mathcal M_{X_q} \\subset  \\mathcal M_{X}\\), le \\(\\mathbb R2\\) du modèle le plus grand (ayant le plus de variables) sera toujours plus grand que le \\(\\mathbb R2\\) du modèle le plus petit.\n\n\nExercice 6 La matrice \\(X'X\\) est symétrique, \\(n\\) vaut 30 et \\(\\bar x= \\bar z=0\\). Le coefficient de corrélation \\[\\begin{equation*}\n\\rho_{x,z} = \\frac{\\sum_{i=1}^{30} (x_i -\\bar x)(z_i - \\bar z)}\n{\\sqrt{\\sum_{i=1}^{30} (x_i -\\bar x)^2\\sum_{i=1}^{30} (z_i - \\bar z)^2}}\n=\\frac{\\sum_{i=1}^{30} x_i z_i}\n{\\sqrt{\\sum_{i=1}^{30} x_i^2 \\sum_{i=1}^{30} z_i^2}}\n=\\frac{7}{\\sqrt{150}}=0.57.\n\\end{equation*}\\] Nous avons \\[\\begin{eqnarray*}\ny_i &=& -2 +x_i+z_i+\\hat \\varepsilon_i\n\\end{eqnarray*}\\] et la moyenne vaut alors \\[\\begin{eqnarray*}\n\\bar y &=& -2 + \\bar x +\\bar z + \\frac{1}{n}\\sum_i \\hat \\varepsilon_i.\n\\end{eqnarray*}\\] La constante étant dans le modèle, la somme des résidus est nulle car le vecteur \\(\\hat \\varepsilon\\) est orthogonal au vecteur \\(\\mathbf{1}\\). Nous obtenons donc que la moyenne de \\(Y\\) vaut 2 car \\(\\bar x=0\\) et \\(\\bar z=0\\). Nous obtenons en développant \\[\\begin{eqnarray*}\n\\|\\hat Y \\|^2 &=& \\sum_{i=1}^{30}(-2+x_i+2z_i)^2\\\\\n&=& 4+10+60+14=88.\n\\end{eqnarray*}\\] Par le théorème de Pythagore, nous concluons que \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{SCT}}=\\mathop{\\mathrm{SCE}}+\\mathop{\\mathrm{SCR}}=88+12=100.\n\\end{eqnarray*}\\]\n\n\nExercice 7 (Changement d’échelle des variables explicatives) Nous avons l’estimation sur le modèle avec les variables originales qui minimise \\[\\begin{align*}\n\\mathop{\\mathrm{MCO}}(\\beta)&=\\|Y - \\sum_{j=1}^{p} X_j \\beta_j \\|^{2}\n\\end{align*}\\] Cette solution est notée \\(\\hat \\beta\\).\nNous avons l’estimation sur le modèle avec les variables prémultipliées par \\(a_{j}\\) (changement d’échelle) qui minimise\n\\[   \n\\begin{align*}\n\\tilde{\\mathop{\\mathrm{MCO}}}(\\beta)&=\\|Y - \\sum_{j=1}^{p} \\tilde X_j \\beta_j \\|^{2} = \\|Y - \\sum_{j=1}^{p} a_{j} X_j \\beta_j \\|^{2}\\\\\n&= \\|Y - \\sum_{j=1}^{p} X_j \\gamma_j \\|^{2}=\\mathop{\\mathrm{MCO}}(\\gamma),\n\\end{align*}\n\\]\nen posant en dernière ligne \\(\\gamma_{j}=a_{j} \\beta_j\\). La solution de de \\(\\mathop{\\mathrm{MCO}}(\\gamma)\\) (ou encore \\(\\mathop{\\mathrm{MCO}}(\\beta)\\)) est \\(\\hat \\beta\\). La solution de \\(\\tilde{\\mathop{\\mathrm{MCO}}}(\\beta)\\) est alors donnée par \\(\\hat \\beta_{j}=a_{j} \\tilde \\beta_j\\).\n\n\nExercice 8 (Différence entre régression multiple et régressions simples)  \n\nCalculons l’estimateur des MCO noté traditionnellement \\((X'X)^{-1}X'Y\\) avec la matrice \\(X\\) qui possède ici deux colonnes (notées ici \\(X\\) et \\(Z\\)) et \\(n\\) lignes. On a donc \\[\\begin{align*}\n  (X'X)&=\n         \\begin{pmatrix}\n           \\|X\\|^{2} & &lt;X,Z&gt;\\\\\n           &lt;X,Z&gt; & \\|Z\\|^{2} \\\\\n         \\end{pmatrix}\n\\end{align*}\\] Son déterminant est \\(\\Delta= \\|X\\|^{2}\\|Z\\|^{2} - 2 &lt;X,Z&gt;\\) et son inverse est \\[\\begin{align*}\n  \\frac{1}{\\Delta}\n  \\begin{pmatrix}\n           \\|Z\\|^{2} & -&lt;X,Z&gt;\\\\\n           -&lt;X,Z&gt; & \\|X\\|^{2} \\\\\n  \\end{pmatrix}\n\\end{align*}\\] Ensuite \\(X'Y\\) est simplement le vecteur colonne de coordonnées \\(&lt;X,Y&gt;\\) et \\(&lt;Z,Y&gt;\\). En rassemblant le tout nous avons \\[\\begin{align*}\n  \\hat \\beta_{1}&=\\frac{1}{\\Delta}(\\|Z\\|^{2} &lt;X,Y&gt; - &lt;X,Z&gt;&lt;Z,Y&gt;),\\\\\n  \\hat \\beta_{2}&=\\frac{1}{\\Delta}(\\|X\\|^{2} &lt;Z,Y&gt; - &lt;X,Z&gt;&lt;X,Y&gt;).\n\\end{align*}\\] Si \\(&lt;X,Z&gt;=0\\) (les deux vecteurs sont orthogonaux) alors cette écriture se simplifie en \\[\n  \\hat \\beta_{1}=\\frac{&lt;X,Y&gt;}{\\|X\\|^{2}},\\quad\n  \\hat \\beta_{2}=\\frac{&lt;Z,Y&gt;}{\\|Z\\|^{2}}.\n\\]\nCalculons l’estimateur des MCO noté traditionnellement \\((X'X)^{-1}X'Y\\) avec la matrice \\(X\\) qui possède ici une colonne (notée ici \\(X\\)) et \\(n\\) lignes. On a donc \\[\\begin{align*}\n  \\hat \\beta_{X} = \\frac{&lt;X,Y&gt;}{\\|X\\|^{2}}.\n\\end{align*}\\] Passons maintenant à la matrice qui possède ici une colonne (notée ici \\(Z\\)) et \\(n\\) lignes. On a donc \\[\\begin{align*}\n  \\hat \\beta_{Z} = \\frac{&lt;Z,Y&gt;}{\\|Z\\|^{2}}.\n\\end{align*}\\]\nEn général les coefficients des régressions simples ne sont pas ceux obtenus par régression multiple sauf si les variables sont orthogonales.\nNous avons ici les résidus de la première régression qui sont \\[\\begin{align}\n  \\hat \\varepsilon = Y - \\hat \\beta_{X} X.\n\\end{align}\\] La deuxième régression (sur les résidus) donne le coefficient \\[\\begin{align*}\n  \\hat \\beta_{Z} = \\frac{&lt;Z,\\hat \\varepsilon&gt;}{\\|Z\\|^{2}} = \\hat \\beta_{Z} - \\hat \\beta_{X}\\frac{&lt;Z,X&gt;}{\\|Z\\|^{2}}\n\\end{align*}\\] La régression séquentielle donne des coefficients différents des régressions univariées ou bivariées sauf si les variables sont orthogonales. \\end{enumerate}\n\n\n\nExercice 9 (TP : différence entre régression multiple et régressions simples)  \n\n\nExercice 10 (TP : régression multiple et code R)  \n\nimport pandas as pd\n#import numpy as np\n#import matplotlib\n#import matplotlib.pyplot as plt\n#from mpl_toolkits.mplot3d import Axes3D\n#from pandas import DataFrame\n#import statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n\n\nozone = pd.read_csv(\"../donnees/ozone.txt\", header=0, sep=\";\")\nnomvar = []\nfor var in ozone.columns[2:]:\n    if (var!=\"nebulosite\") & (var!=\"vent\"):\n        nomvar.append(var)\n\n\nfd = \"+\".join(nomvar[2:])\n\n\nformule= \"O3 ~ 1+\" + fd\n\n\nregmult = smf.ols(formule, data = ozone).fit()\nregmult.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nO3\nR-squared:\n0.715\n\n\nModel:\nOLS\nAdj. R-squared:\n0.668\n\n\nMethod:\nLeast Squares\nF-statistic:\n15.06\n\n\nDate:\nFri, 31 Jan 2025\nProb (F-statistic):\n1.19e-09\n\n\nTime:\n15:38:38\nLog-Likelihood:\n-197.75\n\n\nNo. Observations:\n50\nAIC:\n411.5\n\n\nDf Residuals:\n42\nBIC:\n426.8\n\n\nDf Model:\n7\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n85.0828\n12.133\n7.012\n0.000\n60.597\n109.568\n\n\nNe12\n-5.2469\n1.008\n-5.204\n0.000\n-7.281\n-3.212\n\n\nN12\n0.2932\n1.352\n0.217\n0.829\n-2.435\n3.022\n\n\nS12\n2.6226\n1.887\n1.390\n0.172\n-1.186\n6.431\n\n\nE12\n0.4653\n2.021\n0.230\n0.819\n-3.614\n4.545\n\n\nW12\n1.4222\n2.020\n0.704\n0.485\n-2.653\n5.498\n\n\nVx\n0.4364\n0.497\n0.878\n0.385\n-0.567\n1.439\n\n\nO3v\n0.2765\n0.100\n2.773\n0.008\n0.075\n0.478\n\n\n\n\n\n\n\n\nOmnibus:\n0.296\nDurbin-Watson:\n1.843\n\n\nProb(Omnibus):\n0.863\nJarque-Bera (JB):\n0.383\n\n\nSkew:\n0.169\nProb(JB):\n0.826\n\n\nKurtosis:\n2.735\nCond. No.\n549.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nExercice 11 (Régression orthogonale) Les vecteurs étant orthogonaux, nous avons \\(\\mathcal M_X = \\mathcal M_U \\stackrel{\\perp}{\\oplus} \\mathcal M_V\\). Nous pouvons alors écrire \\[\\begin{eqnarray*}\n\\hat Y_X = P_X Y &=& (P_U + P_{U^{\\perp}})P_X Y \\\\\n&=& P_U P_X Y + P_{U^{\\perp}}P_X Y =  P_U Y + P_{U^{\\perp}\\cap X} Y \\\\\n&=& \\hat Y_U + \\hat Y_V.\n\\end{eqnarray*}\\] La suite de l’exercice est identique. En conclusion, effectuer une régression multiple sur des variables orthogonales revient à effectuer \\(p\\) régressions simples.\n\n\nExercice 12 (Centrage, centrage-réduction et coefficient constant)  \n\n\nExercice 13 (Moindres carrés contraints)  \n\nL’estimateur des MC vaut \\[\\begin{eqnarray*}\n\\hat \\beta = (X'X)^{-1}X'Y,\n\\end{eqnarray*}\\]\nCalculons maintenant l’estimateur contraint. Nous pouvons procéder de deux manières différentes.\nLa première consiste à écrire le lagrangien \\[\\begin{eqnarray*}\n\\mathcal{L} = S(\\beta) - \\lambda'(R\\beta-r).\n\\end{eqnarray*}\\] Les conditions de Lagrange permettent d’obtenir un minimum \\[\\begin{eqnarray*}\n\\left\\{\n\\begin{array}{l}\n\\displaystyle\\frac{\\partial \\mathcal{L}}{\\partial \\beta} = -2X'Y+2X'X\\hat{\\beta}_c-\nR'\\hat{\\lambda}=0,\\\\\n\\displaystyle\\frac{\\partial \\mathcal{L}}{\\partial \\lambda} = R\\hat{\\beta}_c-r=0,\n\\end{array}\n\\right.\n\\end{eqnarray*}\\] Multiplions à gauche la première égalité par \\(R(X'X)^{-1}\\), nous obtenons \\[\\begin{eqnarray*}\n-2 R(X'X)^{-1}X'Y+2R(X'X)^{-1}X'X \\hat{\\beta}_c-R(X'X)^{-1}R'\\hat{\\lambda}&=&0\\\\\n-2 R(X'X)^{-1}X'Y+2R\\hat{\\beta}_c-R(X'X)^{-1}R'\\hat{\\lambda}&=&0\\\\\n-2 R(X'X)^{-1}X'Y+2r-R(X'X)^{-1}R'\\hat{\\lambda}&=&0.\n\\end{eqnarray*}\\] Nous obtenons alors pour \\(\\hat \\lambda\\) \\[\\begin{eqnarray*}\n\\hat \\lambda = 2 \\left[R(X'X)^{-1}R'\\right]^{-1}\\left[r-R(X'X)^{-1}X'Y\\right].\n\\end{eqnarray*}\\] Remplaçons ensuite \\(\\hat \\lambda\\) \\[\\begin{eqnarray*}\n-2X'Y+2X'X\\hat{\\beta}_c-R'\\hat{\\lambda}&=&0\\\\\n-2X'Y+2X'X\\hat{\\beta}_c-2R'\\left[R(X'X)^{-1}R'\\right]^{-1}\\left[r-R(X'X)^{-1}X'Y\\right]&=& 0,\n\\end{eqnarray*}\\] d’où nous calculons \\(\\hat \\beta_c\\) \\[\\begin{eqnarray*}\n\\hat \\beta_c &=& (X'X)^{-1}X'Y+(X'X)^{-1}R'\\left[R(X'X)^{-1}R'\\right]^{-1}\n(r-R\\hat \\beta)\\\\\n&=& \\hat \\beta + (X'X)^{-1}R'\\left[R(X'X)^{-1}R'\\right]^{-1}(r-R\\hat \\beta).\n\\end{eqnarray*}\\] La fonction \\(S(\\beta)\\) à minimiser est une fonction convexe sur un ensemble convexe (contraintes linéaires), le minimum est donc unique.\nUne autre façon de procéder consiste à utiliser les projecteurs. Supposons pour commencer que \\(r=0\\), la contrainte vaut donc \\(R\\beta=0\\). Calculons analytiquement le projecteur orthogonal sur \\(\\mathcal M_0\\). Rappelons que \\(\\dim(\\mathcal M_0)=p-q\\), nous avons de plus \\[\\begin{eqnarray*}\nR \\beta &=& 0 \\quad \\quad\n\\Leftrightarrow \\quad \\beta \\in Ker(R)\\\\\nR (X'X)^{-1}X'X \\beta &=& 0\\\\\nU' X \\beta &=& 0\\quad \\quad \\hbox{où} \\quad \\quad U = X (X'X)^{-1}R'.\n\\end{eqnarray*}\\] Nous avons donc que \\(\\forall \\beta \\in \\ker(R)\\), \\(U' X \\beta = 0\\), c’est-à-dire que \\(\\mathcal M_U\\), l’espace engendré par les colonnes de \\(U\\), est orthogonal à l’espace engendré par \\(X\\beta\\), \\(\\forall \\beta \\in \\ker(R)\\). Nous avons donc que \\(\\mathcal M_U \\perp \\mathcal M_0\\). Comme \\(U=X[(X'X)^{-1}R']\\), \\(\\mathcal M_U \\subset \\mathcal M_X\\). En résumé, nous avons \\[\\begin{eqnarray*}\n\\mathcal M_U \\subset \\mathcal M_X \\quad  \\hbox{et}\n\\quad  \\mathcal M_U \\perp \\mathcal M_0 \\quad \\hbox{donc}\n\\quad \\mathcal M_U \\subset  (\\mathcal M_X \\cap \\mathcal M_0^{\\perp}).\n\\end{eqnarray*}\\] Afin de montrer que les colonnes de \\(U\\) engendrent \\(\\mathcal M_X \\cap \\mathcal M_0^{\\perp}\\), il faut démontrer que la dimension des deux sous-espaces est égale. Or le rang de \\(U\\) vaut \\(q\\) (\\(R'\\) est de rang \\(q\\), \\((X'X)^{-1}\\) est de rang \\(p\\) et \\(X\\) est de rang \\(p\\)) donc la dimension de \\(\\mathcal M_U\\) vaut \\(q\\). De plus, nous avons vu que \\[\\begin{eqnarray*}\n\\mathcal M_X = \\mathcal M_0 \\stackrel{\\perp}{\\oplus}\\left(\\mathcal M_0^{\\perp} \\cap \\mathcal M_X \\right)\n\\end{eqnarray*}\\] et donc, en passant aux dimensions des sous-espaces, nous en déduisons que \\(\\dim(\\mathcal M_0^{\\perp} \\cap \\mathcal M_X )=q\\). Nous venons de démontrer que \\[\\begin{eqnarray*}\n\\mathcal M_U = \\mathcal M_X \\cap \\mathcal M_0^{\\perp}.\n\\end{eqnarray*}\\] Le projecteur orthogonal sur \\(\\mathcal M_U=\\mathcal M_X \\cap \\mathcal M_0^{\\perp}\\) s’écrit \\[\\begin{eqnarray*}\nP_{U} = U (U'U)^{-1} U'= X (X'X)^{-1} R' [R(X'X)^{-1}R']^{-1}R(X'X)^{-1}X'.\n\\end{eqnarray*}\\] Nous avons alors \\[\\begin{eqnarray*}\n\\hat Y - \\hat Y_0 &=& P_U Y\\\\\nX \\hat \\beta - X \\hat \\beta_0 &=& X (X'X)^{-1} R' [R(X'X)^{-1}R']^{-1}R(X'X)^{-1}X'Y\\\\\n&=& X (X'X)^{-1} R [R(X'X)^{-1}R']^{-1}R \\hat \\beta.\n\\end{eqnarray*}\\] Cela donne \\[\\begin{eqnarray*}\n\\hat \\beta_0 = \\hat \\beta - (X'X)^{-1} R [R(X'X)^{-1}R']^{-1}R \\hat \\beta.\n\\end{eqnarray*}\\] Si maintenant \\(r\\neq 0\\), nous avons alors un sous-espace affine défini par \\(\\{\\beta\\in \\mathbb R^p : R\\beta=r\\}\\) dans lequel nous cherchons une solution qui minimise les moindres carrés. Un sous-espace affine peut être défini de manière équivalente par un point particulier \\(\\beta_p \\in \\mathbb R^p\\) tel que \\(R\\beta_p=r\\) et le sous-espace vectoriel associé \\(\\mathcal M_0^v=\\{\\beta\\in \\mathbb R^p : R\\beta=0\\}\\). Les points du sous-espace affine sont alors \\(\\{\\beta_0 \\in \\mathbb R^p : \\beta_0=\\beta_p+\\beta_0^v, \\beta_0^v \\in \\mathcal M_0^v\n\\quad et \\quad \\beta_p : R\\beta_p=r\\}\\). La solution qui minimise les moindres carrés, notée \\(\\hat \\beta_0\\), est élément de ce sous-espace affine et est définie par \\(\\hat \\beta_0=\\beta_p+\\hat \\beta_0^v\\) où \\[\\begin{eqnarray*}\n\\hat \\beta_0^v = \\hat \\beta - (X'X)^{-1}R'[R(X'X)^{-1}R']^{-1}R\\hat \\beta.\n\\end{eqnarray*}\\] Nous savons que \\(R\\beta_p=r\\) donc \\[\\begin{eqnarray*}\nR\\beta_p = [R(X'X)^{-1}R'][R(X'X)^{-1}R']^{-1}r\n\\end{eqnarray*}\\] donc une solution particulière est \\(\\beta_p = (X'X)^{-1}R'[R(X'X)^{-1}R']^{-1}r\\). La solution \\(\\hat \\beta_0\\) qui minimise les moindres carrés sous la contrainte \\(R\\beta=r\\) est alors \\[\n\\begin{align}\n\\hat \\beta_0 &= \\beta_p+\\hat \\beta_0^v\\\\\\nonumber\n&=(X'X)^{-1}R'[R(X'X)^{-1}R']^{-1}r +\n\\hat \\beta - (X'X)^{-1}R'[R(X'X)^{-1}R']^{-1}R\\hat \\beta\\\\\\nonumber\n&=\\hat \\beta + (X'X)^{-1}R'[R(X'X)^{-1}R']^{-1}(r-R\\hat \\beta).\n\\end{align}\n\\tag{1}\\]\nCalculons l’EQM de \\(\\hat \\beta\\) qui vaut selon la formule classique: \\[\\begin{align*}\n      \\mathop{\\mathrm{EQM}}&= (\\mathbf E(\\hat \\beta) - \\beta) (\\mathbf E(\\hat \\beta) - \\beta)' + \\mathop{\\mathrm{V}}(\\hat\\beta)\n    \\end{align*}\\] avec \\(\\mathbf E(\\hat \\beta)=\\beta\\) (sous l’hypothèse que \\(Y\\) est généré par le modèle de régression) et\\(\\mathop{\\mathrm{V}}(\\hat\\beta) =\\sigma^{2} (X'X)^{-1}\\).\nPour l’EQM de \\(\\hat \\beta_0\\) qui vaut selon la formule classique: \\[\\begin{align*}\n      \\mathop{\\mathrm{EQM}}&= (\\mathbf E(\\hat \\beta_0) - \\beta_0) (\\mathbf E(\\hat \\beta_0) - \\beta_0)' + \\mathop{\\mathrm{V}}(\\hat\\beta_0)\n    \\end{align*}\\] calculons d’abord \\(\\mathbf E(\\hat \\beta_0)\\) en utilisant l’équation (équation 1): \\[\\begin{align*}\n      \\mathbf E(\\hat \\beta_0)&=\\mathbf E(\\hat \\beta) + \\mathbf E[(X'X)^{-1}R'[R(X'X)^{-1}R']^{-1}(r-R\\hat \\beta)]\\\\\n      &=\\beta + (X'X)^{-1}R'[R(X'X)^{-1}R']^{-1}(r-R\\beta)\n    \\end{align*}\\] puisque \\(\\hat \\beta\\) est sans biais. Si nous supposons que le modèle satisfait la contrainte (\\(R\\beta=r\\)) alors là encore le biais est nul.\nCalculons maintemant la variance: \\[\\begin{align*}\n      \\mathop{\\mathrm{V}}(\\hat\\beta_0) &=  \\mathop{\\mathrm{V}}[\\hat \\beta + (X'X)^{-1}R'[R(X'X)^{-1}R']^{-1}(r-R\\hat \\beta)]\\\\\n                      &=\\mathop{\\mathrm{V}}(\\hat\\beta) + \\mathop{\\mathrm{V}}[(X'X)^{-1}R'[R(X'X)^{-1}R']^{-1}(r-R\\hat \\beta)] +\\\\\n                      &\\quad \\quad 2\\mathop{\\mathrm{Cov}}[\\hat\\beta ; (X'X)^{-1}R'[R(X'X)^{-1}R']^{-1}(r-R\\hat \\beta)]\\\\\n      &=   \\sigma^{2}V_{X}^{-1} + (\\mathrm{II}) +  (\\mathrm{III})\n    \\end{align*}\\] Intéressons nous à la seconde partie \\((\\mathrm{II})\\). Comme \\(X\\) est déterministe ainsi que \\(R\\) et \\(r\\) on a en posant pour alléger \\(V_{X}=(X'X)\\) (matrice symétrique) \\[\\begin{align*}\n      (\\mathrm{II}) &= V_{X}^{-1}R'[RV_{X}^{-1}R']^{-1} \\mathop{\\mathrm{V}}(r-R\\hat \\beta)\n                        [RV_{X}^{-1}R']^{-1} R V_{X}^{-1}\\\\\n                    &= V_{X}^{-1}R'[RV_{X}^{-1}R']^{-1} \\mathop{\\mathrm{V}}(R\\hat \\beta)[RV_{X}^{-1}R']^{-1} R V_{X}^{-1}\\\\\n                    &= V_{X}^{-1}R'[RV_{X}^{-1}R']^{-1} R\\sigma^{2}V_{X}^{-1}R'[RV_{X}^{-1}R']^{-1} R V_{X}^{-1}\\\\\n       &= \\sigma^{2}V_{X}^{-1}R'[RV_{X}^{-1}R']^{-1} R V_{X}^{-1}\n    \\end{align*}\\] La troisième partie est \\[\\begin{align*}\n      (\\mathrm{III}) &= 2V_{X}^{-1}R'[RV_{X}^{-1}R']^{-1}\\mathop{\\mathrm{Cov}}[\\hat\\beta ; (r-R\\hat \\beta)]\\\\\n                     &=-2V_{X}^{-1}R'[RV_{X}^{-1}R']^{-1}R\\mathop{\\mathrm{Cov}}[\\hat\\beta ;\\hat\\beta]\\\\\n      &= -2V_{X}^{-1}R'[RV_{X}^{-1}R']^{-1}R\\sigma^{2}V_{X}^{-1}\n    \\end{align*}\\] Ce qui donne au final \\[\\begin{align*}\n      \\mathop{\\mathrm{V}}(\\hat\\beta_0) &=\\sigma^{2}(X'X)^{-1} - \\sigma^{2}(X'X)^{-1}R'[R(X'X)^{-1}R']^{-1}R(X'X)^{-1}\n    \\end{align*}\\] L’écart entre les 2 variances est donc de \\(\\sigma^{2}(X'X)^{-1}R'[R(X'X)^{-1}R']^{-1}R(X'X)^{-1}\\) qui est une matrice de la forme \\(A'A\\) donc semi-définie positive.\n\n\n\n\n\n Retour au sommet",
    "crumbs": [
      "Corrections",
      "I Introduction au modèle linéaire",
      "2 La régression linéaire multiple"
    ]
  },
  {
    "objectID": "correction/chap7.html",
    "href": "correction/chap7.html",
    "title": "7 Variables qualitatives : ANCOVA et ANOVA",
    "section": "",
    "text": "Exercice 1 (Questions de cours) A, A, C, B.\n\n\nExercice 2 (Analyse de la covariance)  \n\nNous avons pour le modèle complet la matrice suivante : \\[\nX=\\begin{bmatrix}\n1&\\cdots&0     &x_{11}&\\cdots&0\\\\\n\\vdots&\\cdots&\\vdots&\\vdots&\\vdots&\\vdots\\\\\n1&\\cdots&      0&x_{1n_1}&\\cdots&0\\\\\n\\cdots&\\cdots&\\cdots&\\cdots&\\cdots&\\cdots\\\\\n0&\\cdots&1&0&\\cdots&x_{I1}\\\\\n\\vdots&\\vdots&\\vdots&\\vdots&\\vdots&\\vdots\\\\\n0&\\cdots&1&0&\\cdots&x_{In_I}\n\\end{bmatrix}\n\\] et pour les deux sous-modèles, nous avons les matrices suivantes : \\[\nX=\\begin{bmatrix}\n1&\\cdots&0     &x_{11}\\\\\n\\vdots &\\cdots&\\vdots&\\vdots\\\\\n1&\\cdots&      0&x_{1n_1}\\\\\n\\cdots&\\cdots&\\cdots&\\cdots\\\\\n0&\\cdots&1&x_{I1}\\\\\n\\vdots&\\vdots&\\vdots&\\vdots\\\\\n0&\\cdots&1&x_{In_I}\n\\end{bmatrix}\n\\quad\nX=\\begin{bmatrix}\n1&x_{11}&\\cdots&0\\\\\n\\vdots &\\vdots&\\vdots&\\vdots\\\\\n1&x_{1n_1}&\\cdots&0\\\\\n\\cdots&\\cdots&\\cdots&\\cdots\\\\\n1&0&\\cdots&x_{I1}\\\\\n\\vdots&\\vdots&\\vdots&\\vdots\\\\\n1&0&\\cdots&x_{In_I}\n\\end{bmatrix}\n\\]\nDans le modèle complet, nous obtenons par le calcul \\[\nX'X = \\begin{bmatrix}\nn_1&0&\\cdots&\\sum x_{i1}&0&\\cdots\\\\\n&\\ddots&&&\\ddots&\\\\\n0&\\cdots&n_I&0&\\cdots&\\sum x_{iI}\\\\\n\\sum x_{i1}&0&\\cdots&\\sum x^2_{i1}&0&\\cdots\\\\\n&\\ddots&&&\\ddots&\\\\\n0&\\cdots&\\sum x_{iI}&0&\\cdots&\\sum x^2_{iI}\\\\\n\\end{bmatrix}\n  \\quad\nX'Y = \\begin{bmatrix}\n\\sum y_{i1}\\\\\n\\vdots\\\\\n\\sum y_{iI}\\\\\n\\sum x_{i1}y_{i1}\\\\\n\\vdots\\\\\n\\sum x_{iI}y_{iI}\\\\\n\\end{bmatrix}\n\\] Une inversion par bloc de \\(X'X\\) et un calcul matriciel donnent le résultat indiqué.\nUne autre façon de voir le problème est de partir du problème de minimisation \\[\\begin{eqnarray*}\n&&\\min \\sum_{i=1}^I\\sum_{j=1}^{n_i}\\left(y_{ij}-\\alpha_{j}-\\beta_{j}x_{ij}\\right)^2\\\\\n&=& \\min \\sum_{j=1}^{n_i}\\left(y_{j1}-\\alpha_1-\\beta_{1}x_{j1}\\right)^2+\\cdots\n+\\sum_{j=1}^{n_I}\\left(y_{jI}-\\alpha_I-\\beta_{I}x_{JI}\\right)^2.\n\\end{eqnarray*}\\] Cela revient donc à calculer les estimateurs des MC pour chaque modalité de la variable qualitative. Attention tout de même, des régressions pour chaque modalité donnent bien les mêmes coefficients \\(\\alpha_{i}, \\beta_{i}\\) mais les écarts-types estimés seront différents: un par modalité dans le cas des régressions pour chaque modalité, un seul écart-type estimé dans le cas de l’ANCOVA.\n\n\n\nExercice 3 (Estimateurs des MC et ANOVA à 1 facteur) La preuve de cette proposition est relativement longue et peu difficile. Nous avons toujours \\(Y\\) un vecteur de \\(\\mathbb R^n\\) à expliquer. Nous projetons \\(Y\\) sur le sous-espace engendré par les colonnes de \\(A_c\\), noté \\(\\mathcal M_{A_c}\\), de dimension I, et obtenons un unique \\(\\hat Y\\). Cependant, en fonction des contraintes utilisées, le repère de \\(\\mathcal M_{A_c}\\) va changer.\nLe cas le plus facile se retrouve lorsque \\(\\mu=0\\). Nous avons alors \\[\\begin{eqnarray*}\n(A_c'A_c) =\n\\begin{bmatrix}\nn_1&0&\\cdots&0 \\\\\n0&n_2&0&\\cdots \\\\\n\\vdots&\\vdots&\\vdots&\\vdots\\\\\n0& \\cdots & 0 & n_I\n\\end{bmatrix}\n\\quad\n(A_c'Y)=\\begin{bmatrix}\n\\sum_{j=1}^{n_1} y_{1j}\\\\\n\\sum_{j=1}^{n_2} y_{2j}\\\\\n\\vdots\\\\\n\\sum_{j=1}^{n_I} y_{Ij}\n\\end{bmatrix}\n\\end{eqnarray*}\\] d’où le résultat. La variance de \\(\\hat \\alpha\\) vaut \\(\\sigma^2 (A_c'A_c)^{-1}\\) et cette matrice est bien diagonale.\nPour les autres contraintes, nous utilisons le vecteur \\(\\vec{e}_{ij}\\) de \\(\\mathbb R^n\\) dont toutes les coordonnées sont nulles sauf celle repérée par le couple \\((i,j)\\) qui vaut 1 pour repérer un individu. Nous notons \\(\\vec{e}_{i}\\) le vecteur de \\(\\mathbb R^n\\) dont toutes les coordonnées sont nulles sauf celles repérées par les indices \\(i,j\\) pour \\(j=1,\\cdots,n_{i}\\) qui valent 1. En fait, ce vecteur repère donc les individus qui admettent la modalité \\(i\\). La somme des \\(\\vec{e}_{i}\\) vaut le vecteur \\(\\mathbf{1}\\). Les vecteurs colonnes de la matrice \\(A_c\\) valent donc \\(\\vec{e}_{1},\\cdots,\\vec{e}_{I}\\).\nConsidérons le modèle \\[\\begin{eqnarray*}\nY=\\mu \\mathbf{1} + \\alpha_1 \\vec{e_1}+ \\alpha_2 \\vec{e_2} +\n\\cdots + \\alpha_I\\vec{e_I} + \\varepsilon.\n\\end{eqnarray*}\\] Voyons comment nous pouvons récrire ce modèle lorsque les contraintes sont satisfaites.\n\n\\(\\alpha_1=0\\), le modèle devient alors \\[\\begin{eqnarray*}\nY &=&\\mu \\mathbf{1} + 0 \\vec{e_1} + \\alpha_2 \\vec{e_2} + \\cdots +\n\\alpha_I\\vec{e_I} + \\varepsilon\\\\\n&=&\\mu \\mathbf{1} + \\alpha_2 \\vec{e_2} + \\cdots + \\alpha_I\\vec{e_I} + \\varepsilon\\\\\n&=& [\\mathbf{1}, \\vec{e_2}, \\cdots, \\vec{e_I}] \\beta + \\varepsilon\\\\\n&=& X_{[\\alpha_1=0]} \\beta_{[\\alpha_1=0]} + \\varepsilon.\n\\end{eqnarray*}\\]\n\\(\\sum n_i \\alpha_i = 0\\) cela veut dire que \\(\\alpha_I= - \\sum_{j=1}^{I-1} n_j\\alpha_j/n_I\\), le modèle devient \\[\n\\begin{eqnarray*}\nY &=&\\mu \\mathbf{1}+ \\alpha_1 \\vec{e_1} + \\cdots +\\alpha_{I-1} \\vec{e_{I-1}}   \n- \\sum_{j=1}^{I-1}  \\frac{n_j\\alpha_j}{n_I}\\vec{e_I} + \\varepsilon\\\\\n&=& \\mu \\mathbf{1} + \\alpha_1(\\vec{e}_{1}-\\frac{n_1}{n_I}\\vec{e}_{I}) + \\cdots\n+ \\alpha_{I-1} (\\vec{e}_{I-1}-\\frac{n_{I-1}}{n_I}\\vec{e}_{I})+\\varepsilon\\\\\n&=&\\mu \\mathbf{1} + \\alpha_1\\vec{v}_{1}+ \\cdots + \\alpha_{I-1} \\vec{v}_{I-1}\n+ \\varepsilon \\quad \\hbox{où} \\quad \\vec{v}_{i}= (\\vec{e}_{i}-\\frac{n_i}{n_I}\\vec{e}_{I})\\\\\n&=& X_{[\\sum  n_i \\alpha_i=0]} \\beta_{[\\sum  n_i\\alpha_i=0]} + \\varepsilon.\n\\end{eqnarray*}\n\\]\n\\(\\sum  \\alpha_i = 0\\) cela veut dire que \\(\\alpha_I= - \\sum_{j=1}^{I-1} \\alpha_j\\), le modèle devient \\[\n\\begin{eqnarray*}\nY &=&\\mu \\mathbf{1} + \\alpha_1 \\vec{e_1} + \\cdots + \\alpha_{I-1} \\vec{e_{I-1}} -\n\\sum_{j=1}^{I-1}  \\alpha_j \\vec{e_I} + \\varepsilon\\\\\n&=& \\mu \\mathbf{1} + \\alpha_1(\\vec{e}_{1}-\\vec{e}_{I}) + \\cdots\n+ \\alpha_{I-1} (\\vec{e}_{I-1}-\\vec{e}_{I})+\\varepsilon\\\\\n&=&\\mu \\mathbf{1} + \\alpha_1\\vec{u}_{1}+ \\cdots + \\alpha_{I-1} \\vec{u}_{I-1}\n+ \\varepsilon \\quad \\hbox{où} \\quad \\vec{u}_{i}= (\\vec{e}_{i}-\\vec{e}_{I})\\\\\n&=& X_{[\\sum  \\alpha_i=0]} \\beta_{[\\sum  \\alpha_i=0]} + \\varepsilon.\n\\end{eqnarray*}\n\\]\n\nDans tous les cas, la matrice \\(X\\) est de taille \\(n \\times I\\), et de rang \\(I\\). La matrice \\(X'X\\) est donc inversible. Nous pouvons calculer l’estimateur \\(\\hat \\beta\\) des MC de \\(\\beta\\) par la formule \\(\\hat \\beta = (X'X)^{-1}X'Y\\) et obtenir les valeurs des estimateurs. Cependant ce calcul n’est pas toujours simple et il est plus facile de démontrer les résultats via les projections.\nLes différentes matrices \\(X\\) et la matrice \\(A\\) engendrent le même sous-espace, donc la projection de \\(Y\\), notée \\(\\hat Y\\) dans ce sous-espace, est toujours la même. La proposition 6.2 indique que \\[\\begin{eqnarray*}\n\\hat Y = \\bar{y_1} \\vec{e_1} + \\cdots + \\bar{y_I} \\vec{e_I}.\n\\end{eqnarray*}\\] Avec les différentes contraintes, nous avons les 3 cas suivants :\n\n\\(\\alpha_1=0\\), la projection s’écrit \\[\n\\begin{eqnarray*}\n\\hat Y &=&\\hat \\mu \\mathbf{1} + \\hat \\alpha_2 \\vec{e_2} + \\cdots +\n\\hat \\alpha_I\\vec{e_I}.\n\\end{eqnarray*}\n\\]\n\\(\\sum n_i \\alpha_i = 0\\), la projection s’écrit \\[\n\\begin{eqnarray*}\n\\hat Y &=&\\hat \\mu \\mathbf{1} + \\hat \\alpha_1 \\vec{e_1} + \\cdots + \\hat \\alpha_{I-1}\n\\vec{e_{I-1}}   - \\sum_{j=1}^{I-1}  \\frac{n_j \\hat \\alpha_j}{n_I}\\vec{e_I}.\n\\end{eqnarray*}\n\\]\n\\(\\sum  \\alpha_i = 0\\), la projection s’écrit \\[\n\\begin{eqnarray*}\n\\hat Y &=& \\hat \\mu \\mathbf{1} + \\hat \\alpha_1 \\vec{e_1} + \\cdots +\n\\hat \\alpha_{I-1} \\vec{e_{I-1}} - \\sum_{j=1}^{I-1} \\hat  \\alpha_j \\vec{e_I}.\n\\end{eqnarray*}\n\\]\n\nIl suffit maintenant d’écrire que la projection est identique dans chaque cas et de remarquer que le vecteur \\(\\mathbf{1}\\) est la somme des vecteurs \\(\\vec{e_i}\\) pour \\(i\\) variant de 1à\\(I\\). Cela donne\n\n\\(\\alpha_1=0\\) \\[\n\\begin{eqnarray*}\n&&\\bar{y_1} \\vec{e_1} + \\cdots + \\bar{y_I} \\vec{e_I} \\\\\n&=&\\hat \\mu\\mathbf{1}+\\hat\\alpha_2\\vec{e_2}+\\cdots+\\hat \\alpha_I\\vec{e_I}\\\\\n&=& \\hat \\mu\\vec{e_1}+(\\hat \\mu+\\hat \\alpha_2)\\vec{e_2}\n\\cdots (\\hat \\mu+\\hat \\alpha_I)\\vec{e_I}.\n\\end{eqnarray*}\n\\]\n\\(\\sum n_i \\alpha_i = 0\\) \\[\n\\begin{eqnarray*}\n&&\\bar{y_1} \\vec{e_1} + \\cdots + \\bar{y_I} \\vec{e_I} \\\\\n&=& \\hat \\mu \\mathbf{1} + \\hat \\alpha_1 \\vec{e_1} + \\cdots + \\hat \\alpha_{I-1}\n\\vec{e_{I-1}}   - \\sum_{j=1}^{I-1}  \\frac{n_j \\hat \\alpha_j}{n_I}\\vec{e_I}\\\\\n&=& (\\hat \\mu + \\hat \\alpha_1) \\vec{e_1} + \\cdots +\n(\\hat \\mu  + \\hat \\alpha_{I-1}) \\vec{e_{I-1}} +\n(\\hat \\mu  - \\sum_{i=1}^{I-1} \\frac{n_i}{n_I}\\hat \\alpha_i) \\vec{e_I}.\n\\end{eqnarray*}\n\\]\n\\(\\sum \\alpha_i = 0\\) \\[\n\\begin{eqnarray*}\n&&\\bar{y_1} \\vec{e_1} + \\cdots + \\bar{y_I} \\vec{e_I} \\\\\n&=&\\hat \\mu \\mathbf{1} + \\hat \\alpha_1 \\vec{e_1} + \\cdots +\n\\hat \\alpha_{I-1} \\vec{e_{I-1}} - \\sum_{j=1}^{I-1} \\hat  \\alpha_j \\vec{e_I}\\\\\n&=& (\\hat \\mu + \\hat \\alpha_1) \\vec{e_1} + \\cdots +\n(\\hat \\mu  + \\hat \\alpha_{I-1})\\vec{e_{I-1}} +\n(\\hat \\mu - \\sum_{i=1}^{I-1} \\hat \\alpha_i) \\vec{e_I}.\n\\end{eqnarray*}\n\\]\n\nEn identifiant les différents termes, nous obtenons le résultat annoncé.\n\n\nExercice 4 (Estimateurs des MC et ANOVA à deux facteurs) Nous notons \\(\\vec{e}_{ijk}\\) le vecteur de \\(\\mathbb R^n\\) dont toutes les coordonnées sont nulles sauf celle indicée par \\(ijk\\) qui vaut 1. Sous les contraintes de type analyse par cellule, le modèle devient \\[\\begin{eqnarray*}\ny_{ijk} &=& \\gamma_{ij} + \\varepsilon_{ijk},\n\\end{eqnarray*}\\] et donc matriciellement \\[\\begin{eqnarray*}\nY= X \\beta +\\varepsilon \\quad \\quad X=(\\vec{e_{11}},\\vec{e_{12}},\\ldots,\\vec{e_{IJ}}),\n\\end{eqnarray*}\\] où le vecteur \\(\\vec{e}_{ij}= \\sum_{k} \\vec{e}_{ijk}\\). Les vecteurs colonnes de la matrice \\(X\\) sont orthogonaux entre eux. Le calcul matriciel \\((X'X)^{-1}X'Y\\) donne alors le résultat annoncé.\n\n\nExercice 5 (Estimateurs des MC et ANOVA à deux facteurs, suite) Nous notons \\(\\vec{e}_{ijk}\\) le vecteur de \\(\\mathbb R^n\\) dont toutes les coordonnées sont nulles sauf celle indicée par \\(ijk\\) qui vaut 1. Nous définissons ensuite les vecteurs suivants~: \\[\\begin{eqnarray*}\n\\vec{e}_{ij} = \\sum_{k} \\vec{e}_{ijk} \\quad\n\\vec{e}_{i.} = \\sum_{j} \\vec{e}_{ij}  \\quad\n\\vec{e}_{.j} = \\sum_{i} \\vec{e}_{ij}  \\quad\n\\vec{e} = \\sum_{i,j,k} \\vec{e}_{ijk}.\n\\end{eqnarray*}\\] Afin d’effectuer cet exercice, nous définissons les sous-espaces suivants~: \\[\\begin{eqnarray*}\nE_1&\\!\\!:=\\!\\!&\\{m \\vec{e},\\ m \\hbox{ quelconque} \\}\\\\\nE_2&\\!\\!:=\\!\\!&\\{\\sum_i a_i \\vec{e}_{i.},\\ \\sum_i a_i=0\\}\\\\\nE_3&\\!\\!:=\\!\\!&\\{\\sum_j b_j \\vec{e}_{.j},\\ \\sum_j b_j=0\\}\\\\\nE_4&\\!\\!:=\\!\\!&\\{\\sum_{ij} c_{ij} \\vec{e}_{ij},\n\\ \\forall j \\sum_{i} c_{ij}=0 \\hbox{ et } \\forall i \\sum_{j} c_{ij}=0\\}.\n\\end{eqnarray*}\\] Ces espaces \\(E_1\\), \\(E_2\\), \\(E_3\\) et \\(E_4\\) sont de dimension respective 1, \\(I-1\\), \\(J-1\\) et \\((I-1)(J-1)\\). Lorsque le plan est équilibré, tous ces sous-espaces sont orthogonaux. Nous avons la décomposition suivante~: \\[\\begin{eqnarray*}\nE = E_1 \\stackrel{\\perp}{\\oplus} E_2 \\stackrel{\\perp}{\\oplus} E_3\n\\stackrel{\\perp}{\\oplus} E_4.\n\\end{eqnarray*}\\]\nLa projection sur \\(E\\) peut se décomposer en une partie sur \\(E_1,\\cdots,E_4\\) et l’estimateur des MC est obtenu par projection de \\(Y\\) sur \\(E\\). Notons \\(P_{E^\\perp}\\), \\(P_{E},\\) \\(P_{E_1},\\) \\(P_{E_2},\\) \\(P_{E_3}\\) et \\(P_{E_4}\\) les projections orthogonales sur les sous-espaces \\(E^\\perp\\), \\(E\\), \\(E_1\\), \\(E_2\\), \\(E_3\\) et \\(E_4\\), nous avons alors \\[\\begin{eqnarray*}\nP_{E_1} Y &=& \\bar{y} \\mathbf{1} ,\n\\end{eqnarray*}\\] puis, en remarquant que projeter sur le sous-espace engendré par les colonnes de \\(A=[\\vec{e}_{1.},\\cdots,\\vec{e}_{I.}]\\) est identique à la projection sur \\(E_1 \\stackrel{\\perp}{\\oplus} E_2\\), nous avons alors avec \\(\\mathbf{1} = \\sum_i \\vec{e}_{i.}\\), \\[\\begin{eqnarray*}\nP_{A} Y = \\sum_i \\bar{y}_{i.} \\vec{e}_{i.} \\quad \\hbox{donc}\n\\quad P_{E_2} Y =\\sum_i (\\bar{y}_{i.} - \\bar{y})\\ \\vec{e}_{i.}.\\\\\n\\end{eqnarray*}\\] De la même façon, nous obtenons \\[\\begin{eqnarray*}\nP_{E_3}(Y)&=&\\sum_j (\\bar{y}_{.j} - \\bar{y})\\ \\vec{e}_{.j},\\\\\nP_{E_4}(Y)&=&\\sum_{ij} (\\bar{y}_{ij}-\\bar{y}_{i.}-\\bar{y}_{.j}+\\bar{y})\\  \\vec{e}_{i.},\\\\\nP_{E^\\perp}(Y)  &=&\\sum_{ijk} (y_{ijk}-\\bar{y}_{ij})\\ \\vec{e}_{ijk},\n\\end{eqnarray*}\\] où \\(\\vec{e}_{ijk}\\) est le vecteur dont toutes les coordonnées sont nulles sauf celle indicée par \\({ijk}\\) qui vaut 1. En identifiant terme à terme, nous retrouvons le résultat énoncé.\n\n\nExercice 6 (Tableau d’ANOVA à 2 facteurs équilibrés) Lorsque le plan est équilibré, nous avons démontré, que les sous-espaces \\(E_1\\), \\(E_2\\), \\(E_3\\) et \\(E_4\\) sont orthogonaux (cf. exercice précédent) deux à deux. Nous avons alors \\[\\begin{eqnarray*}\nY &=& P_{E_1}(Y) + P_{E_2}(Y) + P_{E_3}(Y) + P_{E_4}(Y) + P_{E^\\perp}(Y).\n\\end{eqnarray*}\\] Nous obtenons ensuite par le théorème de Pythagore \\[\\begin{eqnarray*}\n\\begin{array}{ccccccccccc}\n\\|Y - \\bar Y \\|^2 &=&  \\| P_{E_2}(Y)\\|^2 &+&\n\\|P_{E_3}(Y)\\|^2 &+& \\|P_{E_4}(Y)\\|^2 &+& \\|P_{E^\\perp}(Y)\\|^2\\\\\n\\mathop{\\mathrm{SCT}}&=& \\mathop{\\mathrm{SC}}_A &+& \\mathop{\\mathrm{SC}}_B &+& \\mathop{\\mathrm{SC}}_{AB} &+& \\mathop{\\mathrm{SCR}},\n\\end{array}\n\\end{eqnarray*}\\] où \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{SCT}}&=& \\sum_i \\sum_j \\sum k (y_{ijk} - \\bar y)^2\\\\\n\\mathop{\\mathrm{SC}}_A &=& Jr \\sum_i (y_{i..}-\\bar y)^2\\\\\n\\mathop{\\mathrm{SC}}_B &=& Ir \\sum_j (y_{.j.} - \\bar y)^2\\\\\n\\mathop{\\mathrm{SC}}_{AB} &=& r \\sum_i \\sum_j (y_{ij.} - y_{i..} - y_{.j.} +\\bar y)^2\\\\\n\\mathop{\\mathrm{SCR}}&=& \\sum_i \\sum_j \\sum_k (y_{ijk}- \\bar{y_{ij}})^2.\n\\end{eqnarray*}\\]\nAfin de bien visualiser les vecteurs voici un exemple avec \\(I=2\\), \\(J=3\\) et \\(r=2\\) en remplaçant les \\(0\\) par \\(.\\) :\n\\[\n\\begin{array}{*{12}c}\n      \\vec{e}&\\vec{e}_{1.}&\\vec{e}_{2.}&\n      \\vec{e}_{.1}&\\vec{e}_{.2}&\\vec{e}_{.3}&\n      \\vec{e_{11}}&\\vec{e}_{12}&\\vec{e}_{13}&\n      \\vec{e}_{21}&\\vec{e}_{22}&\\vec{e}_{23}&\\\\\n1&1&.&1&.&.&1&.&.&.&.&.& \\\\\n1&1&.&1&.&.&1&.&.&.&.&.& \\\\\n1&1&.&.&1&.&.&1&.&.&.&.& \\\\\n1&1&.&.&1&.&.&1&.&.&.&.& \\\\\n1&1&.&.&.&1&.&.&1&.&.&.& \\\\\n1&1&.&.&.&1&.&.&1&.&.&.& \\\\\n%%\n1&.&1&1&.&.&.&.&.&1&.&.& \\\\\n1&.&1&1&.&.&.&.&.&1&.&.& \\\\\n1&.&1&.&1&.&.&.&.&.&1&.& \\\\\n1&.&1&.&1&.&.&.&.&.&1&.& \\\\\n1&.&1&.&.&1&.&.&.&.&.&1& \\\\\n1&.&1&.&.&1&.&.&.&.&.&1& \\\\\n\\end{array}\n\\]\n\nEn écrivant les vecteurs dans le cadre général et en faisant la somme ci-dessous \\[\n\\vec{Y}=\\mu \\vec{e}+\\sum_{i} \\alpha_i \\vec{e}_{i.}+\\sum_{j} \\beta_j \\vec{e}_{.j}\n+\\sum_{ij} (\\alpha\\beta)_{ij} \\vec{e}_{ij} + \\vec{\\varepsilon},\n\\tag{1}\\] on a bien que la ligne \\(ijk\\) vaut \\[\ny_{ijk} = \\mu +\\alpha_i+\\beta_j+(\\alpha\\beta)_{ij}+ \\varepsilon_{ijk}.\n\\]\nMontrons que \\(E_1 \\perp E_2\\). Pour cela prenons deux vecteurs quelconques de \\(E_1\\) et $E_2 $, ils s’écrivent \\(m\\vec{e}\\) et \\(\\sum_{i=1}^I a_{i} \\vec{e}_{i.}\\) (avec \\(\\sum_{i=1}^I a_{i}=0\\)) et leur produit scalaire vaut \\[\n&lt;m\\vec{e};\\sum_{i=1}^I a_{i} \\vec{e}_{i.}&gt;=m\\sum_{i=1}^I a_{i}&lt;\\vec{e};\\vec{e}_{i.}&gt; = m I \\sum_{i=1}^I a_{i} =0\n\\] De même avec \\(E_1 \\perp E_3\\).\nMontrons que \\(E_1 \\perp E_4\\). Pour cela prenons deux vecteurs quelconques de \\(E_1\\) et \\(E_4\\), ils s’écrivent \\(m\\vec{e}\\) et \\(\\sum_{i=1}^I\\sum_{j=1}^J (ab)_{ij} \\vec{e}_{ij}\\) (avec pour tout \\(i\\) \\(\\sum_j (ab)_{ij}=0\\) et pour tout \\(j\\) \\(\\sum_i (ab)_{ij}=0\\)). Leur produit scalaire vaut \\[\\begin{align*}\n  &lt;m\\vec{e};\\sum_{i=1}^I\\sum_{j=1}^J (ab)_{ij}\\vec{e}_{ij}&gt;\n  &=m\\sum_{i=1}^I \\sum_{j=1}^J (ab)_{ij}&lt;\\vec{e};\\vec{e}_{ij}&gt; = m r \\sum_{i=1}^I \\sum_{j=1}^J (ab)_{ij} =0\n\\end{align*}\\]\nMontrons que \\(E_2 \\perp E_4\\). Pour cela prenons deux vecteurs quelconques de \\(E_2\\) et \\(E_4\\), ils s’écrivent \\(\\sum_{l=1}^I a_{l} \\vec{e}_{l}\\) (avec \\(\\sum_{l=1}^I a_{l}=0\\)) et \\(\\sum_{i=1}^I\\sum_{j=1}^J (ab)_{ij} \\vec{e}_{ij}\\) (avec \\(i\\) \\(\\sum_j (ab)_{ij}=0\\) et pour tout \\(j\\) \\(\\sum_i (ab)_{ij}=0\\). Leur produit scalaire vaut \\[\\begin{align*}\n  &lt;\\sum_{l=1}^I a_{l} \\vec{e}_{l.};\\sum_{i=1}^I\\sum_{j=1}^J (ab)_{ij}\\vec{e}_{ij}&gt;\n  &=\\sum_{l=1}^I a_{l}\\sum_{i=1}^I \\sum_{j=1}^J (ab)_{ij}&lt;\\vec{e}_{l.};\\vec{e}_{ij}&gt; \\\\\n  &=\\sum_{l=1}^I a_{l} r \\sum_{i=1}^I \\sum_{j=1}^J (ab)_{ij} =0\n\\end{align*}\\] De même avec \\(E_3 \\perp E_4\\).\nLa dimension de \\(E_{1}\\) vaut 1 (car \\(\\vec{e}\\) est non nul). Le sous-espace \\(E_{2}\\) est engendré par les \\(I\\) vecteurs non nuls et orthogonaux deux à deux \\(\\{\\vec{e}_{i.}\\}_{i=1}^{I}\\) donc le sous espace engendré est au moins de dimension \\(I\\). Cependant on ajoute une contrainte linéaire donc \\(dim(E_{2})=I-1\\). De même pour \\(E_{2}\\) dont la dimension est donc \\(J-1\\). Enfin sous-espace \\(E_{4}\\) est engendré par les \\(IJ\\) vecteurs non nuls et orthogonaux deux à deux \\(\\{\\vec{e}_{ij}\\}\\) (donc le sous espace engendré est de dimension \\(IJ\\)) mais auquel on ajoute plusieurs contraintes linéaires. Il faut donc compter le nombre de contrainte linéaires indépendantes.\nMontrons que les \\(I+J\\) contraintes $ i {j} (ab){ij}=0$ et \\(\\forall j \\sum_{i} (ab)_{ij}=0\\) ne sont pas indépendantes. En effet quand \\(I+J-1\\) contraintes sont vérifiées, la dernière restante l’est aussi. \\[\n  \\begin{array}{*{5}c}\n  (ab)_{11}&(ab)_{12}&\\ldots&(ab)_{1J-1}&(ab)_{1J}&=0\\\\\n  (ab)_{21}&(ab)_{22}&\\ldots&(ab)_{2J-1}&(ab)_{2J}&=0\\\\\n  \\vdots&\\vdots& &\\vdots&\\vdots&\\vdots\\\\\n  (ab)_{I1}&(ab)_{I2}&\\ldots&(ab)_{IJ-1}&(ab)_{IJ}&=0\\\\\n  =0&=0&\\ldots&=0&c=?&\n\\end{array}\n\\] Posons que \\(I+J-1\\) contraintes sont vérifiées~: \\(I\\) en ligne et \\(J-1\\) en colonnes (voir ci-dessus). En sommant toute la matrice on sait (somme en ligne) que cela vaut zéro et donc la somme en colonne vaut elle aussi 0 et donc la dernière somme \\(c\\) vaut 0 (voir ci-dessus). Nous avons donc que la dimension de \\(E_{4}\\) est \\(IJ-(I +J -1)=(I-1)(J-1)\\)\n\nCalculons \\(P_{1}Y\\) \\[\\begin{align*}\nP_{1}Y &= \\mathbf{1}(\\mathbf{1}' \\mathbf{1})^{-1} \\mathbf{1}'Y=\\mathbf{1}(IJr)^{-1} \\sum_{ijk}Y_{ijk} = \\frac{1}{IJr} Y_{...} \\mathbf{1}\\\\\n      &=\\bar Y_{...} \\vec{e}\n\\end{align*}\\]\nCalculons \\(P_{2}Y\\). On sait que \\(F_{2}=E_1 \\stackrel{\\perp}{\\bigoplus} E_2\\) et que \\(F_{2}\\) est de dimension \\(I\\). Il est donc engendré par les \\(I\\) vecteurs orthogonaux \\(\\{\\vec{e}_{i.}\\}_{i=1}^{I}\\) qui en forme une base. Du fait de la décomposition on a \\[\n\\begin{align}\nP_{F_{2}}Y &= P_{1}Y  + P_{2}Y\n\\end{align}\n\\tag{2}\\] Calculons maintenant directement la projection sur \\(F_{2}\\). Ce sous-espace est engendré par les \\(I\\) vecteurs orthogonaux \\(\\{\\vec{e}_{i.}\\}_{i=1}^{I}\\) en posant la matrice concaténant les (coordonnées des) vecteurs: \\[\\begin{align*}\nF_{2}&=(\\vec{e}_{1.}|\\vec{e}_{2}|\\cdots|\\vec{e}_{I.})\n\\end{align*}\\] on a le projecteur \\[\\begin{align*}\nP_{F_{2}}&=F_{2}(F_{2}'F_{2})^{-1}F'_{2}\n\\end{align*}\\] En effectuant le calcul matriciel on a \\[\\begin{align*}\n(F_{2}'F_{2})&=\\text{diag}(I, I, \\dotsc, I)\n\\end{align*}\\] et par calcul matriciel direct on trouve \\[\n\\begin{align}\nP_{F_{2}}Y&=F_{2}\\text{diag}(1/I, 1/I, \\dotsc, 1/I)F_{2}'Y\\nonumber\\\\\n&=F_{2}\n\\begin{pmatrix}\n\\bar Y_{1..}\\\\\n\\bar Y_{2..}\\\\\n\\vdots\\\\\n\\bar Y_{I..}\\\\\n\\end{pmatrix}\n= \\bar Y_{1..} \\vec{e}_{1.} + \\bar Y_{2..}\\vec{e}_{2.}\n+ \\dotsc \\bar Y_{I..}\\vec{e}_{I.}\n\\end{align}\n\\tag{3}\\] En utilisant les équations équation 2 et équation 3 on trouve \\[\\begin{align*}\nP_{2}Y&=\\bar Y_{1..} \\vec{e}_{1.} + \\bar Y_{2..}\\vec{e}_{2.}\n+ \\dotsc + \\bar Y_{I..}\\vec{e}_{I.} - \\bar Y_{...} \\vec{e}.\n\\end{align*}\\] Remarquons que \\(\\vec{e}=\\vec{e}_{1.} + \\dotsc +\\vec{e}_{I.}\\) et en remplaçant cela dans l’équation précédente nous avons \\[\n\\begin{align*}\nP_{2}Y&=(\\bar Y_{1..} -\\bar Y_{...})  \\vec{e}_{1.} + (\\bar Y_{2..} - \\bar Y_{...}) \\vec{e}_{2.}\n+ \\dotsc + (\\bar Y_{I..} -\\bar Y_{...}) \\vec{e}_{I.}.\n\\end{align*}\n\\]\nEn calquant ces calculs pour \\(E_{3}\\) on trouve \\[\\begin{align*}\nP_{3}Y&=(\\bar Y_{.1.}-\\bar Y_{...}) \\vec{e}_{.1} + (\\bar Y_{.2.}-\\bar Y_{...})\\vec{e}_{.2}\n+ \\dotsc +(\\bar Y_{.J.}-\\bar Y_{...})\\vec{e}_{.J}.\n\\end{align*}\\]\nEnfin pour \\(E_{4}\\), remarquons que \\(F_{4}=E=\\vect(\\vec{e}_{11}, \\dotsc, \\vec{e}_{IJ})\\). La projection sur \\(E\\) identifié à sa matrice \\((\\vec{e}_{11}| \\dotsc |\\vec{e}_{IJ})\\) peut être calculée de manière directe comme \\[\\begin{align}\nP_{E}Y&=E\\text{diag}(1/r, 1/r, \\cdots, 1/r)E'Y\\nonumber\\\\\n&=E\n\\begin{pmatrix}\n\\bar Y_{11.}\\\\\n\\bar Y_{21.}\\\\\n\\vdots\\\\\n\\bar Y_{IJ.}\\\\\n\\end{pmatrix}\n= \\bar Y_{11.} \\vec{e}_{11} + \\dotsc \\bar Y_{IJ.}\\vec{e}_{IJ}\\label{eq:pebis}\n\\end{align}\\] En se servant de la décomposition on a \\[\\begin{align}\nP_{E}Y&=P_{1}Y + P_{2}Y  + P_{3}Y  + P_{4}Y\n\\end{align}\\] Et en identifiant les deux calculs (avec \\(\\vec{e}_{i.}=\\vec{e}_{i1} + \\dotsc +\\vec{e}_{iJ}\\) et \\(\\vec{e}_{.j}=\\vec{e}_{1j} + \\dotsc +\\vec{e}_{Ij}\\) ) \\[\\begin{align*}\nP_{4}Y&= (\\bar Y_{11.} - \\bar Y_{1..} - \\bar Y_{.1.} + \\bar Y_{...})\\vec{e}_{11}+ \\dotsc + (\\bar Y_{IJ} - \\bar Y_{I..} - \\bar Y_{.J.} + \\bar Y_{...})\\vec{e}_{IJ}.\n\\end{align*}\\]\nLa dernière projection s’obtient comme \\[\\begin{align*}\nQY&=Y- P_{E}Y = Y -  \\bar Y_{11.} \\vec{e}_{11} + \\dotsc \\bar Y_{IJ.}\\vec{e}_{IJ}\n\\end{align*}\\]\n\nEn reprenant la décomposition en sous-espace orthogonaux suivante \\[\n\\begin{align}\n\\mathbb R^{n}&= E  \\stackrel{\\perp}{\\bigoplus} E^{\\perp} =E  \\stackrel{\\perp}{\\bigoplus} Q\\\\\n&= E_1 \\stackrel{\\perp}{\\bigoplus} E_2 \\stackrel{\\perp}{\\bigoplus} E_3 \\stackrel{\\perp}{\\bigoplus} E_4 \\stackrel{\\perp}{\\bigoplus} Q\n\\end{align}\n\\tag{4}\\] On a donc que \\[\n\\begin{align*}\nY &= P_{1}Y + P_{2}Y + P_{3}Y + P_{4}Y + P_{Q}Y.\n\\end{align*}\n\\tag{5}\\] En utilisant toutes les définitions de la question précédente on a \\[\n\\begin{split}\nY&=\\bar Y_{...} \\vec{e}\\\\\n& \\ \\  +  (\\bar Y_{1..} - \\bar Y_{...}) \\vec{e}_{1.} + \\dotsc +\\bar (Y_{I..}- \\bar Y_{...})\\vec{e}_{I.}\\\\\n& \\ \\    + (\\bar Y_{.1.}- \\bar Y_{...}) \\vec{e}_{.1} +  \\dotsc +\\bar (Y_{.J.}- \\bar Y_{...})\\vec{e}_{.J} \\\\\n& \\ \\  + (\\bar Y_{11.} - \\bar Y_{1..} - \\bar Y_{.1.} + \\bar Y_{...})\\vec{e}_{11}+ \\dotsc + (\\bar Y_{IJ} - \\bar Y_{I..} - \\bar Y_{.J.} + \\bar Y_{...})\\vec{e}_{IJ} \\\\\n& \\ \\ + P_{Q}Y.\n\\end{split}\n\\tag{6}\\] En utilisant l’équation 1 on identifie terme à terme et nous obtenons les paramètres du modèle : \\[\\begin{align*}\n\\hat \\mu&=\\bar Y_{...}\\\\\n\\hat \\alpha_{i}&=(\\bar Y_{i..} - \\bar Y_{...})\\\\\n\\hat \\beta_{j}&=(\\bar Y_{.j.} - \\bar Y_{...})\\\\\n(\\widehat{\\alpha\\beta})_{ij}&=(\\bar Y_{ij.} - \\bar Y_{i..} - \\bar Y_{.j.} + \\bar Y_{...})\\\\\n\\end{align*}\\]\nEn utilisant l’équation 5 et en se rappelant de l’orthogonalité (équation 4) on a \\[\n\\begin{align}\n\\|Y -\\bar Y_{...} \\vec{e}\\|^{2} &=\\| P_{1}Y + P_{2}Y + P_{3}Y + P_{4}Y + P_{Q}Y\\|^{2}\\nonumber\\\\\n&=\\|P_{1}Y\\|^{2} + \\|P_{2}Y\\|^{2} + \\|P_{3}Y\\|^{2} + \\|P_{4}Y\\|^{2} + \\|P_{Q}Y\\|^{2}\n\\end{align}\n\\tag{7}\\] et remplaçant les projections par leur expression (voir par exemple équation 6) et calculant les normes on a \\[\n\\begin{split}\n\\sum_{i=1}^I \\sum_{j=1}^J\\sum_{k=1}^r (Y_{ijk} - \\bar Y_{...})^{2}\n&= rJ\\sum_{i=1}^I(\\bar Y_{i..} - \\bar Y_{...})^{2}\n+ rI\\sum_{j=1}^J(\\bar Y_{.j.} - \\bar Y_{...})^{2}\\\\\n& \\ \\  + r\\sum_{i=1}^I\\sum_{j=1}^J\n(\\bar Y_{ij} - \\bar Y_{i..} - \\bar Y_{.j.} + \\bar Y_{...})^{2}\\\\\n& \\ \\  + \\sum_{i=1}^I\\sum_{j=1}^J\\sum_{k=1}^r\n(Y_{ijk} -  \\bar Y_{ij})^{2}.\n\\end{split}\n\\tag{8}\\] En multipliant par \\(1/n\\) l’équation ci-dessus nous obtenons la décomposition de la variance.\nLe vecteur \\(Y\\) grâce à \\({\\mathcal{H}}_{3}\\) est un vecteur gaussien de moyenne \\(\\vec{m}=\\mu +\\alpha_i+\\beta_j+(\\alpha\\beta)_{ij}\\) et de variance \\(\\sigma^{2}I_{n}\\). On sait que \\[\\begin{align*}\nP_{E}Y &= P_{1}Y + P_{2}Y + P_{3}Y + P_{4}Y\n\\end{align*}\\] Grâce au théorème de Cochran on a que \\[\n\\frac{\\|P_{i}Y -P_{i}\\vec{m} \\|^{2}}{\\sigma^{2}} \\sim \\chi^{2} (dim(E_{i}))\n\\] ou encore que \\(\\frac{\\|P_{i}Y\\|^{2}}{\\sigma^{2}}\\) suit un \\(\\chi^{2} (dim(E_{i}))\\) décentré de paramètre de décentrage \\(\\|P_{i}\\vec{m} \\|^{2}\\). Pour \\(Q=E^{\\perp}\\) on a que \\(P_{Q}\\vec{m}=0\\) et il n’y a pas de décentrage.\nReprenons l’équation 7 qui s’écrit aussi avec des sommes (équation 8) ou encore \\[\\begin{align*}\n\\mathop{\\mathrm{SCT}}\n&= \\mathop{\\mathrm{SCE}}_{a} + \\mathop{\\mathrm{SCE}}_{b} + \\mathop{\\mathrm{SCE}}_{ab}  + \\mathop{\\mathrm{SCR}}\n\\end{align*}\\] On a donc que \\[\\begin{align*}\n\\mathop{\\mathrm{SCE}}_{a}&=\\|P_{2}Y\\|^{2}\\sim \\sigma^{2}\\chi^{2} (I-1), \\|P_{2}\\vec{m} \\|^{2}),\\\\\n\\mathop{\\mathrm{SCE}}_{b}&=\\|P_{3}Y\\|^{2}\\sim \\sigma^{2}\\chi^{2} (J-1), \\|P_{3}\\vec{m} \\|^{2}),\\\\\n\\mathop{\\mathrm{SCE}}_{ab}&=\\|P_{4}Y\\|^{2}\\sim \\sigma^{2}\\chi^{2} ((I-1)(J-1)), \\|P_{4}\\vec{m} \\|^{2}),\\\\\n\\mathop{\\mathrm{SCE}}_{ab}&=\\|P_{Q}Y\\|^{2}\\sim \\sigma^{2}\\chi^{2} (n - IJ).\n\\end{align*}\\] Nous voyons donc que chaque terme est la norme carrée d’une projection du vecteur gaussien \\(Y\\) dans un sous-espace et que ces sous-espaces sont orthogonaux 2 à 2. Les vecteurs gaussiens projetés sont donc indépendants ainsi que leur norme au carré. Les lois de ces normes carrées sont donc à \\(\\sigma^{2}\\) près des \\(\\chi^{2}\\) décentrés (sauf pour \\(Q\\)) qui sont indépendants.\nNotons \\(\\mathop{\\mathrm{CME}}_{ab} = \\mathop{\\mathrm{SCE}}_{ab}/((I-1)(J-1))\\) et \\(\\mathop{\\mathrm{CMR}}= \\mathop{\\mathrm{SCR}}/(n-IJ)\\) nous avons donc \\[\\begin{align*}\n\\frac{\\mathop{\\mathrm{CME}}_{ab}}{\\mathop{\\mathrm{CMR}}}&=\\frac{\\frac{\\mathop{\\mathrm{SCE}}_{ab}}{\\sigma^{2}(I-1)(J-1)}}{\\frac{\\mathop{\\mathrm{SCR}}}{\\sigma^{2}(n-IJ)}}\n\\end{align*}\\] qui est le rapport de deux \\(\\chi^{2}\\) indépendants ramenés à leur degrés de liberté et dont le numérateur est décentré. Nous avons donc une loi de Fisher de paramètres \\((I-1)(J-1), n -IJ, \\|P_{4}\\vec{m} \\|^{2}\\). Sous \\({\\mathrm{H_0}}:\\) « il n’y a pas d’interaction » (ou \\(P_{4}\\vec{m}=0\\)) alors la loi se simplifie et le paramètre de décentrage inconnu (qui dépend de \\(\\vec{m}\\)) disparaît et la loi est \\(F((I-1)(J-1), n -IJ)\\).\n\n\n\n\n\n Retour au sommet",
    "crumbs": [
      "Corrections",
      "II Inférence",
      "7 Variables qualitatives : ANCOVA et ANOVA"
    ]
  },
  {
    "objectID": "correction/chap4.html",
    "href": "correction/chap4.html",
    "title": "4 Extension : non-inversibilité et (ou) erreurs non corrélées",
    "section": "",
    "text": "Exercice 1 (Questions de cours) B, B, B, A.\n\n\nExercice 2 (Corrélation multiple et hypothèse \\(\\mathcal{H}_1\\))  \n\nMontrons que la moyenne empirique de \\(X\\hat \\beta\\) vaut \\(\\bar Y\\). Le vecteur moyenne est obtenu en projetant sur \\(\\mathbf{1}_n\\). En effet, comme \\[\\begin{eqnarray*}\nP_{\\mathbf{1}}&=&\\mathbf{1}_n(\\mathbf{1}_n'\\mathbf{1}_n)^{-1}\\mathbf{1}_n'=\\frac{1}{n}\\mathbf{1}_n\\mathbf{1}_n',\n\\end{eqnarray*}\\] nous avons, pour une variable \\(Z=(Z_1,\\dotsc,Z_n)'\\), \\[\\begin{eqnarray*}\nP_{\\mathbf{1}}Z&=&=\\frac{1}{n}\\mathbf{1}_n \\mathbf{1}_n'Z= \\frac{1}{n}\\mathbf{1}_n\\sum_{i=1}^{n}{Z_i}=\\bar Z\\mathbf{1}_n.\n\\end{eqnarray*}\\] Comme \\(\\mathbf{1}_n\\in\\Im(X)\\), nous avons \\[\\begin{eqnarray*}\n\\bar Y&=&P_\\mathbf{1}Y=P_\\mathbf{1}P_XY=P_\\mathbf{1}X\\hat \\beta,\n\\end{eqnarray*}\\] c’est-à-dire que la moyenne empirique de \\(X\\hat \\beta\\) vaut \\(\\bar Y\\).\nLe coefficient de corrélation entre \\(\\hat Y\\) et \\(Y\\) élevé au carré s’écrit donc \\[\\begin{eqnarray*}\n\\rho^2(\\hat Y,Y)&=&\\frac{\\langle \\hat Y-\\bar Y,Y-\\bar Y\\rangle^2}{\\|\\hat Y-\\bar Y\\|^2\\|Y-\\bar Y\\|^2}\\\\\n&=&\\frac{\\langle \\hat Y-\\bar Y,Y-\\hat Y+\\hat Y-\\bar Y\\rangle^2}{\\|\\hat Y-\\bar Y\\|^2\\|Y-\\bar Y\\|^2}\\\\\n&=&\\Bigl\\{\\frac{\\langle \\hat Y-\\bar Y,Y-\\hat Y\\rangle}{\\|\\hat Y-\\bar Y\\|\\|Y-\\bar Y\\|}+\\frac{\\langle \\hat Y-\\bar Y,\\hat Y-\\bar Y\\rangle}{\\|\\hat Y-\\bar Y\\|\\|Y-\\bar Y\\|}\\Bigl\\}^2.\n\\end{eqnarray*}\\] Comme \\((Y-\\hat Y)\\in\\Im(X)^\\perp\\) et que \\((\\hat Y-\\bar Y)\\in\\Im(X)\\), nous avons \\(\\langle \\hat Y-\\bar Y,Y-\\hat Y\\rangle=0\\) et donc \\[\\begin{eqnarray*}\n\\rho^2(\\hat Y,Y)&=&\\frac{\\|\\hat Y-\\bar Y\\|^2 \\|\\hat Y-\\bar Y\\|^2}{\\|\\hat Y-\\bar Y\\|^2\\|Y-\\bar Y\\|^2}=\\mathop{\\mathrm{R^2}}2.\n\\end{eqnarray*}\\]\n\nEn effectuant le calcul nous trouvons que \\(Y-2X_1+2X_2=3\\eta\\).\nEn calculant les normes carrées, nous avons \\[\\begin{eqnarray*}\n\\|X_1\\|^2&=&1^2+1^2+1^2=3,\\\\\n\\|X_2\\|^2&=&1/2+1/2+2=3,\\\\\n\\|X_3\\|^2&=&3/2+3/2=3.\n\\end{eqnarray*}\\] En calculant les produits scalaires, nous avons \\[\\begin{eqnarray*}\n\\langle X_1,X_2\\rangle&=&1\\times 1/\\sqrt{2}+ 1\\times 1/\\sqrt{2} +1\\times (-\\sqrt{2})\n=\\sqrt{2}-\\sqrt{2}=0,\\\\\n\\langle X_1,\\eta\\rangle&=&\\sqrt{3}/\\sqrt{2}-\\sqrt{3}/\\sqrt{2}=0,\\\\\n\\langle X_2,\\eta\\rangle&=&1/\\sqrt{2}\\times\\sqrt{3}/\\sqrt{2}-1/\\sqrt{2}\\times\\sqrt{3}/\\sqrt{2}=0.\n\\end{eqnarray*}\\]\nLa représentation graphique est :\n\n\n\n\n\n\nNous avons ici \\(X_1\\in\\Im(X)\\), \\(X_2\\in\\Im(X)\\) et \\(\\eta\\in\\Im(X)^\\perp\\), ce qui permet de trouver \\(\\hat Y\\) : \\[\\begin{eqnarray*}\nP_XY&=&P_X(2X_1-2X_2+3\\eta)=2P_XX_1 -2P_XX_2+3P_X\\eta=\\\\\n&=&2X_1-2X_2=(2-\\sqrt{2},2-\\sqrt{2},2-2\\sqrt{2})'.\n\\end{eqnarray*}\\]\n\nPuisque \\(\\mathbf{1}\\) fait partie des variables explicatives, nous avons \\[\\begin{eqnarray*}\n\\rho(Y,\\hat Y)&=&\\frac{\\langle Y-\\bar Y,\\hat Y-\\bar Y\\rangle}{\\|\\hat Y-\\bar Y\\|\\|Y-\\bar Y\\|},\n\\end{eqnarray*}\\] ce qui est la définition du cosinus de l’angle entre \\(\\overrightarrow{\\bar YY}\\) et \\(\\overrightarrow{\\bar Y\\hat Y}\\).\nNotons par \\(Y_\\alpha\\) le vecteur \\(X\\alpha\\). Sa moyenne vaut \\(\\bar Y_\\alpha\\). Nous avons maintenant le cosinus de l’angle entre \\(\\overrightarrow{\\bar YY}\\) et \\(\\overrightarrow{\\bar Y_\\alpha Y_\\alpha}\\). Graphiquement, la moyenne de \\(Y_\\alpha\\) est la projection sur \\(X_1=\\mathbf{1}_3\\).\nLa représentation graphique nous permet de voir que l’angle entre \\(\\overrightarrow{\\bar YY}\\) et \\(\\overrightarrow{\\bar Y_\\alpha Y_\\alpha}\\) est le même que celui entre \\(\\overrightarrow{\\bar YY}\\) et \\(\\overrightarrow{\\bar Y\\hat Y}\\). L’angle est minimum (et le cosinus maximum) quand \\(\\alpha=\\hat\\beta\\) ou pour tout \\(\\alpha\\) tel que \\(\\overrightarrow{\\bar Y_\\alpha Y_\\alpha}=k\\overrightarrow{\\bar Y\\hat Y}\\) avec \\(k&gt;0\\).\nDu fait de l’orthogonalité entre \\(X_1\\) et \\(X_2\\), \\(\\overrightarrow{\\bar Y_\\alpha Y_\\alpha}\\) est toujours colinéaire à \\(\\overrightarrow{\\bar Y\\hat Y}\\), seul le signe change en fonction de l’orientation des vecteurs (même sens ou sens opposé).\n\nComme \\(\\rho(X_j;X_k)=1\\) alors \\(R(X_j;(\\mathbf{1},X_j))=1\\) et donc puisque la constante fait partie du modèle \\(R(X_j;X_{(j)})=1\\). L’hypothèse \\({\\mathcal{H}}_1\\) n’est donc pas vérifiée.\n\n\n\nExercice 3 (EQM de la régression Ridge)  \n\nLes démonstrations figurent en page 77 : \\[\\begin{eqnarray*}\nB(\\hat \\beta_{ridge}) &=& -\\kappa (X'X + \\kappa I)^{-1} \\beta,\\\\\nV(\\hat \\beta_{\\mathrm{ridge}})&=&\\sigma^2(X'X + \\kappa I)^{-1}X'X(X'X + \\kappa I)^{-1}\\\\\n\\mathop{\\mathrm{EQM}}(\\hat \\beta_{\\mathrm{ridge}})&=&(X'X + \\kappa I)^{-1}\\left[\\kappa^2\\beta \\beta'+\\sigma^2(X'X) \\right](X'X + \\kappa I)^{-1}.\n\\end{eqnarray*}\\]\nPuisque \\(X'X=P\\text{diag}(\\lambda_i) P'\\), nous avons \\[\\begin{eqnarray*}\n(X'X + \\kappa I)&=&P\\text{diag}(\\lambda_i) P'+ \\kappa PP'=P\\text{diag}(\\lambda_i+\\kappa)P'.\n\\end{eqnarray*}\\] En se rappelant que \\(P^{-1}=P'\\), son inverse vaut \\[\\begin{eqnarray*}\n(X'X + \\kappa I)^{-1}&=&P\\text{diag}(1/(\\lambda_i+\\kappa))P'.\n\\end{eqnarray*}\\] Nous avons donc \\[\\begin{equation*}\n\\begin{split}\n\\mathop{\\mathrm{EQM}}(\\hat \\beta_{\\mathrm{ridge}})&=P\\text{diag}(\\frac{1}{\\lambda_i+\\kappa})P'\\left[\\kappa^2\\beta \\beta'+\\sigma^2(X'X) \\right]P\\text{diag}(\\frac{1}{\\lambda_i+\\kappa})P'\\\\\n&=P\\text{diag}(\\frac{1}{\\lambda_i+\\kappa})\\left[\\kappa^2(P'\\beta\\beta'P)+\\sigma^2 I_p\\right]\\text{diag}(\\frac{1}{\\lambda_i+\\kappa})P'.\n\\end{split}\n\\end{equation*}\\] Nous en déduisons que sa trace vaut \\[\\begin{equation*}\n\\begin{split}\n\\text{tr}\\left\\{EQM(\\hat \\beta_{\\mathrm{ridge}})\\right\\}&=\\text{tr}\\left\\{\\text{diag}(\\frac{1}{\\lambda_i+\\kappa})\\left[\\kappa^2(P'\\beta\\beta'P)+\\sigma^2 I_p\\right]\\right.\\\\\n&\\quad \\left.\\text{diag}(\\frac{1}{\\lambda_i+\\kappa})P'P\\right\\},\n\\end{split}\n\\end{equation*}\\] et, comme \\(P'P=I_p\\), nous avons alors \\[\\begin{eqnarray*}\n\\text{tr}\\left\\{EQM(\\hat \\beta_{\\mathrm{ridge}})\\right\\}\n&=&\\text{tr}\\left\\{\\left[\\kappa^2(P'\\beta\\beta'P)+\\sigma^2 I_p\\right]\\text{diag}(\\frac{1}{(\\lambda_i+\\kappa)^2})\\right\\}.\n\\end{eqnarray*}\\] Le \\(i^e\\) élément de la diagonale de la matrice \\(P'\\beta\\beta'P\\) vaut \\([P'\\beta]_i^2\\). Celui de \\(\\left[\\kappa^2(P'\\beta\\beta'P)+\\sigma^2\n  I_p\\right]\\) vaut \\(\\kappa^2[P'\\beta]_i^2+\\sigma^2\\) et celui de \\[\\left[\\kappa^2(P'\\beta\\beta'P)+\\sigma^2\n  I_p\\right]\\text{diag}(\\frac{1}{(\\lambda_i+\\kappa)^2})\\] vaut donc \\[\\kappa^2[P'\\beta]_i^2+\\sigma^2/(\\lambda_i+\\kappa)^2.\\] On en déduit le résultat annoncé car la trace est la somme des éléments diagonaux d’une matrice.\nL’estimateur des MC est non biaisé et son \\(\\mathop{\\mathrm{EQM}}\\) vaut sa variance : \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{EQM}}(\\hat \\beta_{\\mathrm{MC}})&=&\\sigma^2(X'X)^{-1}.\n\\end{eqnarray*}\\] Nous avons alors \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{EQM}}(\\hat \\beta_{\\mathrm{MC}})\n\\!&=&\\!\\sigma^2(X'X \\!+\\! \\kappa I)^{-1}(X'X + \\kappa I)(X'X)^{-1}\\\\\n\\!&=&\\!\\sigma^2(X'X \\!+\\! \\kappa I)^{-1}(X'X(X'X)^{-1} + \\kappa I(X'X)^{-1})\\\\\n\\!&=&\\!\\sigma^2(X'X \\!+\\! \\kappa I)^{-1}(I\\!+\\!\\kappa (X'X)^{-1})(X'X \\!+\\! \\kappa I)(X'X \\!+\\! \\kappa I)^{-1}\\\\\n\\!&=&\\!\\sigma^2(X'X \\!+\\! \\kappa I)^{-1}(X'X\\!+\\!2 \\kappa I \\!+\\! \\kappa^2 (X'X)^{-1})(X'X \\!+\\! \\kappa I)^{-1}.\n\\end{eqnarray*}\\]\nLe calcul de \\(\\Delta=\\mathop{\\mathrm{EQM}}(\\hat \\beta_{\\mathrm{ridge}})-\n\\mathop{\\mathrm{EQM}}(\\hat \\beta_{\\mathrm{MC}})\\) est immédiat en utilisant l’expression précédente de \\(\\mathop{\\mathrm{EQM}}(\\hat \\beta_{\\mathrm{MC}})\\) et celle rappelée en question 1.\nEn utilisant le théorème proposé avec \\(A=(X'X + \\kappa I)^{-1}\\) et \\(B=(\\sigma^2(2I_p+\\kappa^2(X'X)^{-1})-\\kappa\\beta\\beta')\\) nous obtenons le résultat demandé. Cette condition dépend de \\(\\beta\\) qui est inconnu, mais aussi de \\(X\\), c’est-à-dire des mesures obtenues.\nIntéressons-nous à la matrice \\(\\gamma\\gamma'\\). Cette matrice est symétrique donc diagonalisable, de valeurs propres positives ou nulles. La somme de ses valeurs propres est égale à la trace de cette matrice \\[\\begin{eqnarray*}\n\\text{tr}(\\gamma\\gamma')=\\text{tr}(\\gamma'\\gamma)=\\gamma'\\gamma.\n\\end{eqnarray*}\\] Montrons que cette matrice n’a qu’une seule valeur propre non nulle \\(\\gamma'\\gamma\\). Pour cela, considérons le vecteur \\(\\gamma\\in\\mathbb R^p\\) et montrons qu’il est vecteur propre de \\(\\gamma\\gamma'\\) associé à la valeur propre \\(\\gamma'\\gamma\\)~: \\[\\begin{eqnarray*}\n(\\gamma\\gamma')\\gamma&=&\\gamma(\\gamma'\\gamma)=(\\gamma'\\gamma)\\gamma.\n\\end{eqnarray*}\\] Nous avons donc un vecteur propre de \\(\\gamma\\gamma'\\) qui est \\(\\gamma\\) associé à la valeur propre \\(\\gamma'\\gamma\\). De plus, nous savons que la somme des valeurs propres positives ou nulles de \\(\\gamma\\gamma'\\) vaut \\(\\gamma'\\gamma\\). Nous en déduisons que les \\(p-1\\) valeurs propres restantes sont toutes nulles. Nous pouvons donc dire que la matrice \\(\\gamma\\gamma'\\) se décompose comme \\[\\begin{eqnarray*}\n\\gamma\\gamma'&=&UDU',\n\\end{eqnarray*}\\] où \\(U\\) est la matrice orthogonale des vecteurs propres normés à l’unité de \\(\\gamma\\gamma'\\) et \\(D=\\text{diag}(\\gamma'\\gamma,0,\\dotsc,0)\\). Nous avons donc \\[\\begin{eqnarray*}\nI_p-\\gamma\\gamma'&=&UU' - UDU'=U(\\text{diag}(1-\\gamma'\\gamma,1,\\dotsc,1)U'.\n\\end{eqnarray*}\\] Les valeurs propres de \\(I_p-\\gamma\\gamma'\\) sont donc \\(1-\\gamma'\\gamma,1,\\dotsc,1\\), qui sont toutes positives ou nulles dès que \\(\\gamma'\\gamma\\le 1\\).\nUne condition pour que \\(\\sigma^2(2I_p-\\kappa\\beta\\beta')\\) soit semi-définie positive est que \\((\\kappa\\beta\\beta')\\le \\sigma^2\\) (cf. question précédente) et donc \\((\\sigma^2(2I_p+\\kappa^2(X'X)^{-1})-\\kappa\\beta\\beta')\\) est alors la somme de 2 matrices semi-définies positives donc semi-définie positive. Cela implique qu’il s’agit d’une condition suffisante pour que \\(\\Delta\\) soit semi-définie positive.\nNous venons de montrer 2 conditions, l’une nécessaire et suffisante, l’autre suffisante, afin que \\(\\Delta\\) soit semi-définie positive. Cette assertion signifie que, quelle que soit la combinaison linéaire du vecteur de paramètre (par exemple une coordonnée), l’estimateur ridge est meilleur que celui des MC au sens de l’EQM. Cela signifie aussi que, si une de ces conditions est vérifiée, globalement au sens de la trace de l’EQM, l’estimateur ridge est meilleur que celui des MC. Au niveau des conditions, cela permet de trouver la valeur optimale de \\(\\kappa\\). Malheureusement chacune des 2 conditions dépend de la valeur \\(\\beta\\) inconnue et donc n’est pas réellement utilisable en pratique. La condition suffisante procure une amélioration, dans le sens où elle ne dépend pas de \\(X\\) donc de l’expérience. Le prix à payer est bien sûr qu’il s’agit seulement d’une condition suffisante et donc plus restrictive.\n\n\n\nExercice 4 (Régression pondérée)  \n\nNous souhaitons minimiser \\[\\begin{eqnarray*}\n\\sum_{i=1}^n \\left(y_i-\\sum_{j=1}^p \\beta_j x_{ij}\\right)^2 p_i,\n\\end{eqnarray*}\\] où \\(p_i\\) est un réel positif. Nous pouvons écrire ce critère sous la forme suivante : \\[\\begin{eqnarray*}\n\\sum_{i=1}^n \\left(\\sqrt{p_i}y_i-\\sum_{j=1}^p \\beta_j\\sqrt{p_i} x_{ij}\\right)^2\n= \\sum_{i=1}^n \\left(y^\\star_i-\\sum_{j=1}^p \\beta_jx_{ij}^\\star\\right)^2,\n\\end{eqnarray*}\\] où \\(y^\\star_i=\\sqrt{p_i}y_i\\) et \\(x_{ij}^\\star = \\sqrt{p_i} x_{ij}\\).\nNotons \\(P^{1/2}\\) la matrice des poids qui vaut \\(P^{1/2}=\\text{diag}(\\sqrt{p_i})\\). Ce dernier critère est un critère des MC avec comme observations \\(Y^\\star\\) et \\(X^\\star\\) où \\(Y^\\star = P^{1/2} Y\\) et \\(X^\\star = P^{1/2} X\\). L’estimateur vaut alors \\[\\begin{eqnarray*}\n\\hat \\beta_{pond} &=& (X^{\\star\\prime}X^\\star)^{-1}X^{\\star\\prime}Y^\\star\\\\\n&=& (X'PX)^{-1}X'PY.\n\\end{eqnarray*}\\]\nLorsque nous avons la constante comme seule variable explicative, \\(X=\\mathbf{1}_n\\), et nous avons alors \\[\\begin{eqnarray*}\n\\hat \\beta_{pond} &=&\\frac{\\sum p_i y_i}{\\sum p_i}.\n\\end{eqnarray*}\\]\nLorsque les poids sont constants, nous retrouvons, non plus une moyenne pondérée, mais la moyenne usuelle.\n\n\n\nExercice 5 (Gauss-Markov) L’estimateur des MC s’écrit \\(\\hat \\beta_2 = \\sum_{i=1}^n p_i y_i,\\) avec \\(p_i=(x_i-\\bar x)/\\sum(x_i -\\bar x)^2\\). Considérons un autre estimateur \\(\\tilde{\\beta_2}\\) linéaire en \\(y_i\\) et sans biais, c’est-à-dire \\[\\tilde{\\beta_2} =\\sum_{i=1}^n \\lambda_i y_i.\\] Montrons que \\(\\sum \\lambda_i=0\\) et \\(\\sum \\lambda_i x_i=1\\). L’égalité \\(\\mathbf E(\\tilde{\\beta_2}) = \\beta_1 \\sum \\lambda_i +\n\\beta_2 \\sum \\lambda_i x_i +  \\sum \\lambda_i \\mathbf E(\\varepsilon_i)\\) est vraie pour tout \\(\\beta_2\\) et \\(\\tilde \\beta_2\\) est sans biais donc \\(\\mathbf E(\\tilde \\beta_2)=\\beta_2\\) pour tout \\(\\beta_2\\), c’est-à-dire que \\(\\sum \\lambda_i=0\\) et \\(\\sum \\lambda_i x_i=1\\).\nMontrons que \\(\\mathop{\\mathrm{V}}(\\tilde{\\beta_2}) \\geq \\mathop{\\mathrm{V}}(\\hat\n\\beta_2)\\). \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}(\\tilde{\\beta_2}) = \\mathop{\\mathrm{V}}(\\tilde{\\beta_2}- \\hat \\beta_2 + \\hat \\beta_2)\n=\\mathop{\\mathrm{V}}(\\tilde{\\beta_2}- \\hat \\beta_2)+\\mathop{\\mathrm{V}}(\\hat \\beta_2)+\n2\\mathop{\\mathrm{Cov}}(\\tilde{\\beta_2}- \\hat \\beta_2,\\hat \\beta_2).\n\\end{eqnarray*}\\] \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{Cov}}(\\tilde{\\beta_2}- \\hat \\beta_2,\\hat \\beta_2)\n\\!=\\!\\mathop{\\mathrm{Cov}}(\\tilde{\\beta_2},\\hat \\beta_2) -\\mathop{\\mathrm{V}}(-\\hat \\beta_2)\n\\!=\\!\\frac{\\sigma^2\\sum \\lambda_i(x_i-\\bar x)}{\\sum (x_i-\\bar x)^2} -\n\\frac{\\sigma^2}{\\sum (x_i-\\bar x)^2}\n\\!=\\!0,\n\\end{eqnarray*}\\] et donc \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}(\\tilde{\\beta_2}) =\n\\mathop{\\mathrm{V}}(\\tilde{\\beta_2}- \\hat \\beta_2)+\\mathop{\\mathrm{V}}(\\hat \\beta_2).\n\\end{eqnarray*}\\] Une variance est toujours positive et donc \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}(\\tilde{\\beta_2}) \\geq \\mathop{\\mathrm{V}}(\\hat \\beta_2).\n\\end{eqnarray*}\\] Le résultat est démontré. On obtiendrait la même chose pour \\(\\hat \\beta_1\\).\n\n\n\n\n Retour au sommet",
    "crumbs": [
      "Corrections",
      "I Introduction au modèle linéaire",
      "4 Extension : non-inversibilité et (ou) erreurs non corrélées"
    ]
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Résumé",
    "section": "",
    "text": "Le livre se décompose en quatre parties, chacune constituée de deux à cinq chapitres.\n\nLa première pose les fondamentaux du problème de régression et montre, à travers quelques exemples, comment on peut l’aborder à l’aide d’un modèle linéaire simple d’abord, puis multiple. Les problèmes d’estimation ainsi que la géométrie associée à la méthode des moindres carrés sont proposés dans les deux premiers chapitres de cette partie. Le troisième chapitre propose les principaux diagnostics qui permettent de s’assurer de la validité du modèle tandis que le dernier présente quelques stratégies à envisager lorsque les hypothèses classiques du modèle linéaire ne sont pas vérifiées. Les quatrième et cinquième chapitre abordent des extensions du modèle linéaire comme la corrélation des erreurs et les régressions polynomiales et splines.\nLa seconde partie aborde la partie inférentielle. Il s’agit d’une des parties les plus techniques et calculatoires de l’ouvrage. Cette partie permet, entre autres, d’exposer précisément les procédures de tests et de construction d’intervalles de confiance dans le modèle linéaire. Elle décrit également les spécificités engendrées par l’utilisation de variables qualitatives dans ce modèle.\nLa troisième partie est consacrée à un problème désormais courant en régression : la réduction de la dimension. En effet, face à l’augmentation conséquente des données, nous sommes de plus en plus confrontés à des problèmes où le nombre de variables est (très) grand. Les techniques standards appliquées à ce type de données se révèlent souvent peu performantes et il est nécessaire de trouver des alternatives. Nous présentons tout d’abord les techniques classiques de choix de variables qui consistent à se donner un critère de performance et à rechercher à l’aide de procédures exhaustives ou pas à pas le sous-groupe de variables qui optimise le critère donné. Nous présentons ensuite les approches régularisées de type Ridge-Lasso qui consistent à trouver les estimateurs qui optimisent le critère des moindres carrés pénalisés par une fonction de la norme des paramètres. Le troisième chapitre propose de faire la régression non pas sur les variables initiales mais sur des combinaisons linéaires de celles-ci. Nous insistons sur la régression sur composantes principales (PCR) et la régression Partial Least Square (PLS). A ce stade, nous disposons de plusieurs algorithmes qui répondent à un même problème de régression. Il devient important de se donner une méthode qui permette d’en choisir un automatiquement (on ne laisse pas l’utilisateur décider, ce sont les données qui doivent choisir). Nous proposons un protocole basé sur la minimisation de risques empiriques calculés par des algorithmes de type validation croisée qui permet de choisir l’algorithme le plus approprié pour un problème donné.\nDans la quatrième partie, nous présentons le modèle linéaire généralisé. Cette partie généralise les modèles initiaux, qui permettaient de traiter uniquement le cas d’une variable à expliquer continue, à des variables à expliquer binaire (régression logistique) ou de comptage (régression de Poisson). Nous insistons uniquement sur les spécificités associées à ces types de variables, la plupart des concepts étudiés précédemment s’adaptent directement à ces cas nouveaux.\n\n\n\n\n Retour au sommet"
  },
  {
    "objectID": "donnees.html",
    "href": "donnees.html",
    "title": "Données",
    "section": "",
    "text": "Les données utilisées dans le livre sont ci-dessous. On peut aussi récupérer tous les fichiers en cliquant ici.\n\nad_data.txt\n\nartere.txt\ncourbe_lasso.csv\ndonnees_dd.csv\ndd_ex_ech_des1.csv\ndd_exo3_1.csv\ndd_exo3_2.csv\ndd_exo3_3.csv\nechan_lasso.csv\neucalyptus.txt\n\nlasso_exo8_courbe.csv\nlasso_exo8_echan.csv\nlogit_donnees.csv\nlogit_ex6.csv\nlogit_ridge_lasso.csv\nozone_complet.txt\nozone_long.txt\nozone_simple.txt\nozone_transf.txt\nozone.txt\npanne.txt\npoissonData.csv\npoissonData3.csv\nregulglm_exo3.csv\nSAh.csv\ntpbisrespartiel.dta\ntprespartiel.dta\n\n\n\n\n Retour au sommet"
  },
  {
    "objectID": "codes/chap4.html",
    "href": "codes/chap4.html",
    "title": "4 Extensions : non iversibilité et (ou) erreurs corrélées",
    "section": "",
    "text": "Retour au sommet",
    "crumbs": [
      "Codes",
      "I Introduction au modèle linéaire",
      "4 Extensions : non iversibilité et (ou) erreurs corrélées"
    ]
  },
  {
    "objectID": "codes/codes.html",
    "href": "codes/codes.html",
    "title": "Codes Pythons",
    "section": "",
    "text": "I Introduction au modèle linéaire\n\n\n\n\n\n\n\n\n[object Object],[object Object],[object Object],[object Object],[object Object]\n\n\n\n\n\n\n\nII Inférence\n\n\n\n\n\n\n\n\n[object Object],[object Object]\n\n\n\n\n\n\n\nIII Réduction de dimension\n\n\n\n\n\n\n\n\n[object Object],[object Object],[object Object],[object Object]\n\n\n\n\n\n\n\nIV Le modèle linéaire généralisé\n\n\n\n\n\n\n\n\n[object Object],[object Object],[object Object],[object Object],[object Object]\n\n\n\n\n\n\nAucun article correspondant\n\n Retour au sommet"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Présentation",
    "section": "",
    "text": "Les auteurs\n\nPierre-André Cornillon\nNicolas Hengartner\nÉric Matzner-Løber\nLaurent Rouvière\n\n\n\nDescriptif\n\n4e de couverture\nAvant-propos\nSommaire détaillé\n\n\n\nBoutique en ligne\nPar ici\n\n\n\n\nDescriptif\nCette première édition de régression avec python est une adaptation de la version régression avec R. On trouvera notamment sur ce site :\n\nles jeux de données ;\nles lignes de code ;\nles corrections des exercices.\n\n\n\n\n\n\n Retour au sommet"
  },
  {
    "objectID": "correction/chap1.html",
    "href": "correction/chap1.html",
    "title": "1 La régression linéaire simple",
    "section": "",
    "text": "Exercice 1 (Questions de cours) B, A, B, A.\n\n\nExercice 2 (Biais des estimateurs) Les \\(\\hat \\beta_j\\) sont fonctions de \\(Y\\) (aléatoire), ce sont donc des variables aléatoires. Une autre façon d’écrire \\(\\hat \\beta_2\\) en fonction de \\(\\beta_2\\) consiste à remplacer \\(y_i\\) par sa valeur soit \\[\\begin{eqnarray*}\n\\hat \\beta_2 &=&\\frac{\\sum (x_i - \\bar x) y_i}{\\sum(x_i-\\bar x)^2}\n=\\frac{\\beta_1\\sum (x_i - \\bar x)+\\beta_2\\sum x_i(x_i - \\bar x)+\n\\sum (x_i - \\bar x) \\varepsilon_i }{\\sum(x_i-\\bar x)^2} \\\\\n&=& \\beta_2 + \\frac{\\sum (x_i - \\bar x) \\varepsilon_i}{\\sum(x_i-\\bar x)^2}.\n\\end{eqnarray*}\\] Par hypothèse \\(\\mathbf E(\\varepsilon_i)=0\\), les autres termes ne sont pas aléatoires, le résultat est démontré.\nLe résultat est identique pour \\(\\hat \\beta_1\\) car \\(\\mathbf E(\\hat \\beta_1) =\n\\mathbf E(\\bar y) -\\bar x \\mathbf E(\\hat \\beta_2)= \\beta_1 +\\bar x \\beta_2 - \\bar\nx \\beta_2=\\beta_1\\), le résultat est démontré.\n\n\nExercice 3 (Variance des estimateurs) Nous avons \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}(\\hat \\beta_2)\n&=&\n\\mathop{\\mathrm{V}}\\left(\\beta_2+\\frac{\\sum(x_i-\\bar x)\\varepsilon_i}\n{\\sum(x_i-\\bar x)^2}\\right).\n\\end{eqnarray*}\\] Or \\(\\beta_2\\) est inconnu mais pas aléatoire et les \\(x_i\\) ne sont pas aléatoires donc \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}(\\hat \\beta_2)\n&=&\\mathop{\\mathrm{V}}\\left(\\frac{\\sum(x_i-\\bar x)\\varepsilon_i}{\\sum(x_i-\\bar x)^2}\\right)\n=\\frac{\\mathop{\\mathrm{V}}\\left(\\sum(x_i-\\bar x)\\varepsilon_i\\right)}{\\left[\\sum(x_i-\\bar x)^2\\right]^2}\\\\\n&=&\\frac{\\sum_{i,j}(x_i-\\bar x)(x_j-\\bar x)\\mathop{\\mathrm{Cov}}(\\varepsilon_i,\\varepsilon_j)}{\\left[\\sum(x_i-\\bar x)^2\\right]^2}.\n\\end{eqnarray*}\\] Or \\(\\mathop{\\mathrm{Cov}}(\\varepsilon_i,\\varepsilon_j)=\\delta_{ij}\\sigma^2\\) donc \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}(\\hat \\beta_2)\n&=&\\frac{\\sum_i(x_i-\\bar x)^2\\sigma^2}{\\left[\\sum_i(x_i-\\bar x)^2\\right]^2}\n=\\frac{\\sigma^2}{\\sum(x_i-\\bar x)^2}.\n\\end{eqnarray*}\\] Plus les mesures \\(x_i\\) sont dispersées autour de leur moyenne, plus \\(\\mathop{\\mathrm{V}}(\\hat \\beta_2)\\) est faible et plus l’estimation est précise. Bien sûr, plus \\(\\sigma^2\\) est faible, c’est-à-dire plus les \\(y_i\\) sont proches de la droite inconnue, plus l’estimation est précise.\nPuisque \\(\\hat \\beta_1=\\bar y - \\hat \\beta_2 \\bar x\\), nous avons \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}(\\hat \\beta_1)\n&=&\n\\mathop{\\mathrm{V}}\\left(\\bar y-\\hat \\beta_2 \\bar x \\right)=\\mathop{\\mathrm{V}}\\left(\\bar y\\right)+V(\\bar x \\hat \\beta_2)-2\\mathop{\\mathrm{Cov}}(\\bar y,\\hat \\beta_2 \\bar x)\\\\\n&=&\\mathop{\\mathrm{V}}\\left(\\frac{\\sum y_i}{n}\\right)+\\bar x^2\\frac{\\sigma^2}{\\sum(x_i-\\bar x)^2}-2 \\bar x\\mathop{\\mathrm{Cov}}(\\bar y,\\hat \\beta_2)\\\\\n&=&\\frac{\\sigma^2}{n}+\\bar x^2\\frac{\\sigma^2}{\\sum(x_i-\\bar x)^2}-2 \\bar x\\sum_i\\mathop{\\mathrm{Cov}}(\\bar y,\\hat \\beta_2).\n\\end{eqnarray*}\\]\nCalculons \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{Cov}}(\\bar y, \\hat \\beta_2)\n&=& \\frac{1}{n}\n\\mathop{\\mathrm{Cov}}\\left(\\sum_{i}\\left(\\beta_1+\\beta_2 x_i+\\varepsilon_i\\right),\\frac{\\sum_j(x_j-\\bar x)\\varepsilon_j}\n{\\sum_j(x_j-\\bar x)^2}\\right)\\\\\n&=&\\frac{1}{n}\\sum_{i}\\mathop{\\mathrm{Cov}}\\left(\\varepsilon_i,\\frac{\\sum_j(x_j-\\bar x)\\varepsilon_j}\n{\\sum_j(x_j-\\bar x)^2}\\right)\\\\\n&=&\\frac{1}{\\sum_j(x_j-\\bar x)^2}\n\\sum_{i}\\frac{1}{n}\\mathop{\\mathrm{Cov}}\\left(\\varepsilon_i,\\sum_j(x_j-\\bar x)\\varepsilon_j\\right)\\\\\n&=&\\frac{\\sigma^2 \\frac{1}{n}\\sum_{i}(x_i-\\bar x)}{\\sum_j(x_j-\\bar x)^2}=0.\n\\end{eqnarray*}\\] Nous avons donc \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}(\\hat \\beta_1)\n&=& \\frac{\\sigma^2}{n}+\\bar x^2\\frac{\\sigma^2}{\\sum(x_i-\\bar x)^2}\n=\\frac{\\sigma^2 \\sum x_i^2}{n\\sum(x_i-\\bar x)^2}.\n\\end{eqnarray*}\\] Là encore, plus \\(\\sigma^2\\) est faible, c’est-à-dire plus les \\(y_i\\) sont proches de la droite inconnue, plus l’estimation est précise. Plus les valeurs \\(x_i\\) sont dispersées autour de leur moyenne, plus la variance de l’estimateur sera faible. De même, une faible moyenne \\(\\bar x\\) en valeur absolue contribue à bien estimer \\(\\beta_1\\).\n\n\nExercice 4 (Covariance de \\(\\hat\\beta_1\\) et \\(\\hat\\beta_2\\)) Nous avons \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{Cov}}(\\hat \\beta_1,\\hat \\beta_2) &=&\\mathop{\\mathrm{Cov}}(\\bar y-\\hat \\beta_2 \\bar x,\\hat \\beta_2)\n= \\mathop{\\mathrm{Cov}}(\\bar y,\\hat \\beta_2)- \\bar x \\mathop{\\mathrm{V}}(\\hat \\beta_2)=\n-\\frac{\\sigma^2 \\bar x}{\\sum(x_i-\\bar x)^2}.\n\\end{eqnarray*}\\] La covariance entre \\(\\beta_1\\) et \\(\\beta_2\\) est négative. L’équation \\(\\bar y=\\hat \\beta_1+\\hat \\beta_2\\bar x\\) indique que la droite des MC passe par le centre de gravité du nuage \\((\\bar x, \\bar y)\\). Supposons \\(\\bar x\\) positif, nous voyons bien que, si nous augmentons la pente, l’ordonnée à l’origine va diminuer et vice versa. Nous retrouvons donc le signe négatif pour la covariance entre \\(\\hat \\beta_1\\) et \\(\\hat\n\\beta_2\\).\n\n\nExercice 5 (Théorème de Gauss-Markov) L’estimateur des MC s’écrit \\(\\hat \\beta_2 = \\sum_{i=1}^n p_i y_i,\\) avec \\(p_i=(x_i-\\bar x)/\\sum(x_i -\\bar x)^2\\).\nConsidérons un autre estimateur \\(\\tilde{\\beta_2}\\) linéaire en \\(y_i\\) et sans biais, c’est-à-dire\n\\[\\tilde{\\beta_2} =\\sum_{i=1}^n \\lambda_i y_i.\\]\nMontrons que \\(\\sum \\lambda_i=0\\) et \\(\\sum \\lambda_i x_i=1\\). L’égalité \\(\\mathbf E(\\tilde{\\beta_2}) = \\beta_1 \\sum \\lambda_i +\n\\beta_2 \\sum \\lambda_i x_i +  \\sum \\lambda_i \\mathbf E(\\varepsilon_i)\\) est vraie pour tout \\(\\beta_2\\) et \\(\\tilde \\beta_2\\) est sans biais donc \\(\\mathbf E(\\tilde \\beta_2)=\\beta_2\\) pour tout \\(\\beta_2\\), c’est-à-dire que \\(\\sum \\lambda_i=0\\) et \\(\\sum \\lambda_i x_i=1\\).\nMontrons que \\(\\mathop{\\mathrm{V}}(\\tilde{\\beta_2}) \\geq \\mathop{\\mathrm{V}}(\\hat\n\\beta_2)\\). \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}(\\tilde{\\beta_2}) = \\mathop{\\mathrm{V}}(\\tilde{\\beta_2}- \\hat \\beta_2 + \\hat \\beta_2)\n=\\mathop{\\mathrm{V}}(\\tilde{\\beta_2}- \\hat \\beta_2)+\\mathop{\\mathrm{V}}(\\hat \\beta_2)+\n2\\mathop{\\mathrm{Cov}}(\\tilde{\\beta_2}- \\hat \\beta_2,\\hat \\beta_2).\n\\end{eqnarray*}\\] \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{Cov}}(\\tilde{\\beta_2}- \\hat \\beta_2,\\hat \\beta_2)\n\\!=\\!\\mathop{\\mathrm{Cov}}(\\tilde{\\beta_2},\\hat \\beta_2) -\\mathop{\\mathrm{V}}(-\\hat \\beta_2)\n\\!=\\!\\frac{\\sigma^2\\sum \\lambda_i(x_i-\\bar x)}{\\sum (x_i-\\bar x)^2} -\n\\frac{\\sigma^2}{\\sum (x_i-\\bar x)^2}\n\\!=\\!0,\n\\end{eqnarray*}\\] et donc \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}(\\tilde{\\beta_2}) =\n\\mathop{\\mathrm{V}}(\\tilde{\\beta_2}- \\hat \\beta_2)+\\mathop{\\mathrm{V}}(\\hat \\beta_2).\n\\end{eqnarray*}\\] Une variance est toujours positive et donc \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}(\\tilde{\\beta_2}) \\geq \\mathop{\\mathrm{V}}(\\hat \\beta_2).\n\\end{eqnarray*}\\] Le résultat est démontré. On obtiendrait la même chose pour \\(\\hat \\beta_1\\).\n\n\nExercice 6 (Somme des résidus) Il suffit de remplacer les résidus par leur définition et de remplacer \\(\\hat \\beta_1\\) par son expression \\[\\begin{eqnarray*}\n\\sum_i \\hat \\varepsilon_i\n= \\sum_i (y_i - \\bar y + \\hat \\beta_2 \\bar x - \\hat \\beta_2 x_i)\n= \\sum_i (y_i-\\bar y) - \\hat \\beta_2 \\sum_i (x_i - \\bar x)= 0.\n\\end{eqnarray*}\\]\n\n\nExercice 7 (Estimateur de la variance du bruit) Récrivons les résidus en constatant que \\(\\hat \\beta_1= \\bar y- \\hat \\beta_2 \\bar x\\) et \\(\\beta_1=\\bar y - \\beta_2 \\bar x - \\bar \\varepsilon\\), \\[\\begin{eqnarray*}\n\\hat \\varepsilon_i&=& \\beta_1 + \\beta_2 x_i +\\varepsilon_i - \\hat \\beta_1 - \\hat \\beta_2x_i\\\\\n&=& \\bar{y} - \\beta_2 \\bar{x} - \\bar{\\varepsilon} + \\beta_2 x_i +\\varepsilon_i -\\bar{y} + \\hat \\beta_2 \\bar{x} - \\hat \\beta_2x_i\\\\\n&=& (\\beta_2-\\hat \\beta_2)(x_i -\\bar{x}) + (\\varepsilon_i - \\bar{\\varepsilon}).\n\\end{eqnarray*}\\] En développant et en nous servant de l’écriture de \\(\\hat \\beta_2\\) donnée dans la solution de l’exercice 2, nous avons \\[\\begin{eqnarray*}\n\\sum \\hat \\varepsilon_i^2& \\!=\\!& (\\beta_2-\\hat \\beta_2)^2 \\!\\sum  \\!(x_i \\!-\\!\\bar{x})^2\n\\!+\\!\\sum \\!(\\varepsilon_i \\!-\\! \\bar{\\varepsilon})^2\\!+\\!2 \\!(\\beta_2\\!-\\!\\hat \\beta_2)\n\\!\\sum \\!(x_i \\!-\\!\\bar{x})(\\varepsilon_i \\!- \\!\\bar{\\varepsilon})\\\\\n&=& (\\beta_2-\\hat \\beta_2)^2 \\sum  (x_i -\\bar{x})^2\n+\\sum (\\varepsilon_i - \\bar{\\varepsilon})^2 - 2 (\\beta_2-\\hat \\beta_2)^2 \\sum  (x_i -\\bar{x})^2.\n\\end{eqnarray*}\\] Prenons en l’espérance \\[\\begin{eqnarray*}\n\\mathbf E\\left( \\sum \\hat{\\varepsilon_i}^2\\right)= \\mathbf E\\left(\\sum (\\varepsilon_i - \\bar{\\varepsilon})^2\\right) -\n\\sum  (x_i -\\bar{x})^2 \\mathop{\\mathrm{V}}(\\hat \\beta_2)\n= (n-2) \\sigma^2.\n\\end{eqnarray*}\\]\n\n\nExercice 8 (Prévision) Calculons la variance \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}\\left(\\hat y^p_{n+1}\\right)\n&=&\\mathop{\\mathrm{V}}\\left(\\hat \\beta_1 + \\hat \\beta_2x_{n+1}\\right)\n=\\mathop{\\mathrm{V}}(\\hat \\beta_1)+x_{n+1}^2 \\mathop{\\mathrm{V}}(\\hat \\beta_2)\n+2 x_{n+1} \\mathop{\\mathrm{Cov}}\\left(\\hat \\beta_1,\\hat \\beta_2\\right)\\\\\n&=& \\frac{\\sigma^2}{\\sum(x_i-\\bar x)^2}\\left(\n\\frac{\\sum x_i^2}{n}+x_{n+1}^2-2 x_{n+1}\\bar x \\right)\\\\\n&=& \\frac{\\sigma^2}{\\sum(x_i-\\bar x)^2}\\left(\n\\frac{\\sum (x_i-\\bar x)^2}{n}+\\bar x^2 + x_{n+1}^2-2 x_{n+1}\\bar x \\right)\\\\\n&=& \\sigma^2\\left(\\frac{1}{n}+\n\\frac{(x_{n+1}-\\bar x)^2}{\\sum (x_i-\\bar x)^2}\\right).\n\\end{eqnarray*}\\] Plus la valeur à prévoir s’éloigne du centre de gravité, plus la valeur prévue sera variable (i.e. de variance élevée).\nVariance de l’erreur de prévision\\ Nous obtenons la variance de l’erreur de prévision en nous servant du fait que \\(y_{n+1}\\) est fonction de \\(\\varepsilon_{n+1}\\) seulement, alors que \\(\\hat y^p_{n+1}\\) est fonction des autres \\(\\varepsilon_i\\), \\(i=1,\\cdots,n\\). Les deux quantités ne sont pas corrélées. Nous avons alors \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}(\\hat \\varepsilon_{n+1}^p) \\!=\\! \\mathop{\\mathrm{V}}\\left(y_{n+1} \\!-\\! \\hat y_{n+1}^p\\right)\n\\!=\\! \\mathop{\\mathrm{V}}(y_{n+1})\\!+\\!\\mathop{\\mathrm{V}}(\\hat y_{n+1}^p)\\!=\\! \\sigma^2\\left(1+\\frac{1}{n}\n\\!+\\!\\frac{(x_{n+1}-\\bar x)^2}{\\sum (x_i-\\bar x)^2}\\right).\n\\end{eqnarray*}\\]\n\n\nExercice 9 (\\(R^2\\) et coefficient de corrélation) Le coefficient \\(\\mathop{\\mathrm{R^2}}\\) s’écrit \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{R^2}}&=&%\\frac{\\|\\hat{Y} -\\bar{y}\\1\\|^2}{\\|Y-\\bar{y}\\1\\|^2}=\n\\frac{\\sum_{i=1}^{n}{\\left(\\hat \\beta_1 + \\hat \\beta_2 x_i - \\bar{y}\\right)^2}}{\n\\sum_{i=1}^{n}{\\left(y_i-\\bar{y}\\right)^2}}=\n\\frac{\\sum_{i=1}^{n}{\\left( \\bar{y}-\\hat \\beta_2 \\bar{x}+ \\hat \\beta_2 x_i - \\bar{y}\\right)^2}}{\n{\\sum_{i=1}^{n}(y_i-\\bar{y})^2}}\\\\\n&=&\n\\frac{\\hat \\beta_2^2\\sum_{i=1}^{n}{\\left( x_i - \\bar{x}\\right)^2}}{\n\\sum_{i=1}^{n}\\left(y_i-\\bar{y}\\right)^2}\n=\\frac{\n\\left[\\sum_{i=1}^{n}{( x_i - \\bar{x})(y_i-\\bar{y})}\\right]^2\n\\sum_{i=1}^{n}{( x_i - \\bar{x})^2}}{\n\\left[\\sum_{i=1}^{n}{( x_i - \\bar{x})^2}\\right]^2\n\\sum_{i=1}^{n}(y_i-\\bar{y})^2}\\\\\n&=&\\frac{\\left[\n\\sum_{i=1}^{n}{( x_i - \\bar{x})(y_i-\\bar{y})}\\right]^2}{\n\\sum_{i=1}^{n}{( x_i - \\bar{x})^2}\\sum_{i=1}^{n}{(y_i-\\bar{y})^2}}=\\rho^2(X,Y).\n\\end{eqnarray*}\\]\n\n\nExercice 10 (Les arbres) Le calcul donne \\[\\begin{eqnarray*}\n\\hat \\beta_1 =\\frac{6.26}{28.29}=0.22 \\quad \\quad\n\\hat \\beta_0 = 18.34-0.22 \\times 34.9=10.662.\n\\end{eqnarray*}\\] Nous nous servons de la propriété \\(\\sum_{i=1}^n \\hat \\varepsilon_i=0\\) pour obtenir \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{R^2}}2 &=& \\frac{\\sum_{i=1}^{20} (\\hat y_i - \\bar y)^2}\n{\\sum_{i=1}^{20}(y_i - \\bar y)^2}=\\frac{\\sum_{i=1}^{20}\n(\\hat \\beta_1 x_i - \\hat \\beta_1 \\bar x)^2}\n{\\sum_{i=1}^{20}(y_i - \\bar y)^2}=0.22^2 \\times \\frac{28.29}{2.85}=0.48.\n\\end{eqnarray*}\\] Les statistiques de test valent 5.59 pour \\(\\beta_0\\) et 4.11 pour \\(\\beta_1\\). Elles sont à comparer à un fractile de la loi de Student admettant 18 ddl, soit 2.1. Nous rejetons dans les deux cas l’hypothèse de nullité du coefficient. Nous avons modélisé la hauteur par une fonction affine de la circonférence, il semblerait évident que la droite passe par l’origine (un arbre admettant un diamètre proche de zéro doit être petit), or nous rejetons l’hypothèse \\(\\beta_0=0\\). Les données mesurées indiquent des arbres dont la circonférence varie de 26 à 43 cm, les estimations des paramètres du modèle sont valides pour des données proches de \\([26;43]\\).\n\n\nExercice 11 (Modèle quadratique) Les modèles sont \\[\\begin{eqnarray*}\n\\mathtt{O3} &=& \\beta_1 + \\beta_2 \\mathtt{T12} +\\varepsilon \\quad\n\\hbox{modèle classique,}\\\\\n\\mathtt{O3} &=& \\gamma_1 + \\gamma_2 \\mathtt{T12}^2 +\\varepsilon \\quad\n\\hbox{modèle demandé}.\n\\end{eqnarray*}\\] L’estimation des paramètres donne \\[\\begin{eqnarray*}\n\\widehat{\\mathtt{O3}} &=& 31.41 + 2.7 \\ \\mathtt{T12} \\quad\\quad \\mathop{\\mathrm{R^2}}2=0.28 \\quad\n\\hbox{modèle classique,}\\\\\n\\widehat{\\mathtt{O3}} &=& 53.74 + 0.075 \\ \\mathtt{T12}^2 \\quad \\mathop{\\mathrm{R^2}}2=0.35 \\quad\n\\hbox{modèle demandé}.\n\\end{eqnarray*}\\] Les deux modèles ont le même nombre de paramètres, nous préférons le modèle quadratique car le \\(\\mathop{\\mathrm{R^2}}\\) est plus élevé.\n\n\n\n\n Retour au sommet",
    "crumbs": [
      "Corrections",
      "I Introduction au modèle linéaire",
      "1 La régression linéaire simple"
    ]
  },
  {
    "objectID": "correction/chap6.html",
    "href": "correction/chap6.html",
    "title": "6 Inférence dans le modèle gaussien",
    "section": "",
    "text": "Exercice 1 (Questions de cours) A, C, A, B, B.\n\n\nExercice 2 (Théorème 6.1) L’IC (i) découle de la propriété (i) de la proposition 5.3. La propriété (ii) donnant un IC pour \\(\\sigma^2\\) découle de la loi de \\(\\hat \\sigma^2\\). Enfin, la propriété (iii) est une conséquence de la loi obtenue propriété (ii) de la proposition 5.3.\n\n\nExercice 3 (Test et \\(R^2\\)) En utilisant l’orthogonalité des sous-espaces (figure 5.3 page 99) et le théorème de Pythagore, nous avons \\[\\begin{eqnarray*}\n\\|\\hat Y_0-\\hat Y\\|^2 &=& \\|\\hat \\varepsilon_0\\|^2- \\| \\hat \\varepsilon\\|^2.\n\\end{eqnarray*}\\] Nous pouvons le démontrer de la manière suivante : \\[\\begin{eqnarray*}\n\\|\\hat Y_0-\\hat Y\\|^2 &=& \\|\\hat Y_0-Y+Y-\\hat Y\\|^2\\\\\n&=& \\|\\hat \\varepsilon_0\\|^2+ \\| \\hat \\varepsilon\\|^2\n+2\\langle \\hat Y_0-Y,Y-\\hat Y\\rangle\\\\\n&=& \\|\\hat \\varepsilon_0\\|^2+ \\| \\hat \\varepsilon\\|^2\n-2\\langle Y-\\hat Y_0,Y-\\hat Y\\rangle\\\\\n&=& \\|\\hat \\varepsilon_0\\|^2+ \\| \\hat \\varepsilon\\|^2\n-2\\langle P_{X_0^\\perp}Y,P_{X^\\perp}Y\\rangle\\\\\n&=& \\|\\hat \\varepsilon_0\\|^2+ \\| \\hat \\varepsilon\\|^2\n-2\\langle (P_{X^\\perp}+P_{X})P_{X_0^\\perp}Y,P_{X^\\perp}Y\\rangle.\n\\end{eqnarray*}\\] Or \\(\\Im(X_0) \\subset \\Im(X)\\), nous avons donc \\(P_{X^\\perp}P_{X_0^\\perp}=P_{X^\\perp}\\). De plus, \\(\\hat \\varepsilon=P_{X^\\perp}Y\\), cela donne \\[\\begin{eqnarray*}\n\\langle (P_{X^\\perp}+P_{X})P_{X_0^\\perp}Y,P_{X^\\perp}Y\\rangle &=&\n\\langle P_{X^\\perp}Y,P_{X^\\perp}Y\\rangle\n+\\langle P_{X}P_{X_0^\\perp}Y,P_{X^\\perp}Y\\rangle \\\\\n&=& \\|\\hat \\varepsilon\\|^2 + 0.\n\\end{eqnarray*}\\] Le résultat est démontré, revenons à la statistique de test. Introduisons les différentes écritures du \\(\\mathop{\\mathrm{R^2}}\\) \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{R^2}}2 = \\frac{\\|\\hat Y - \\bar Y\\|^2}{\\|Y - \\bar Y\\|^2}=1 -\n\\frac{\\|\\hat \\varepsilon\\|^2}{\\|Y - \\bar Y\\|^2}.\n\\end{eqnarray*}\\] La statistique de test vaut \\[\\begin{eqnarray*}\nF&=&\\frac{\\|\\hat \\varepsilon_0\\|^2- \\| \\hat \\varepsilon\\|^2}\n{\\| Y-\\hat Y\\|^2}\\frac{n-p}{p-p_0}\\\\\n&=&\\frac{\\|\\hat \\varepsilon_0\\|^2/\\|Y-\\bar Y\\|^2-\n\\| \\hat \\varepsilon\\|^2/\\|Y-\\bar Y\\|^2}\n{\\| Y-\\hat Y\\|^2/\\|Y-\\bar Y\\|^2}\\frac{n-p}{p-p_0},\n\\end{eqnarray*}\\] nous obtenons \\[\\begin{eqnarray*}\nF&=&\\frac{\\mathop{\\mathrm{R^2}}-\\mathop{\\mathrm{R^2_0}}}{1-\\mathop{\\mathrm{R^2}}}\\frac{n-p}{p-p_0},\n\\end{eqnarray*}\\] soit le résultat annoncé. Cette dernière quantité est toujours positive car \\(\\mathop{\\mathrm{R^2_0}}\\leq \\mathop{\\mathrm{R^2}}\\) et nous avons là un moyen de tester des modèles emboîtés via le coefficient de détermination.\n\n\nExercice 4 (Ozone)  \n\nLes résultats sont dans l’ordre : 45, 24.663, 5.886, 1.567, 0.581, -4.547.\nLa statistique de test de nullité du paramètre se trouve dans la colonne t, nous conservons \\({\\mathrm{H_0}}\\) pour les paramètres associés à T12 et T15, et la rejetons pour les autres.\nLa statistique de test de nullité simultanée des paramètres autres que la constante vaut 24.463. Nous rejetons \\({\\mathrm{H_0}}\\).\nNous connaissons \\[\\begin{align*}\n\\hat y^{p}_{n+1} &= x'_{n+1}\\hat \\beta,\\\\\nx'_{n+1}&=(1, 10, 20, 0, 0, 1)\\\\\n\\hat \\beta&=(62, -4, 5, -1.5, -0.5, 0.8)'\n\\end{align*}\\] et donc la prévision est \\(\\hat y^{p}_{n+1} =122.8\\). Pour l’intervalle de confiance il nous faut \\(\\hat\\sigma=16\\) mais aussi la matrice \\(X'X\\) (donc toutes les données) ce que nous n’avons pas ici. On ne peut donc faire d’intervalle de confiance.\nNous sommes en présence de modèles emboîtés, nous pouvons appliquer la formule adaptée (voir l’exercice précédent) : \\[\\begin{eqnarray*}\nF&=& \\frac{\\mathop{\\mathrm{R^2}}-\\mathop{\\mathrm{R^2_0}}}{1-\\mathop{\\mathrm{R^2}}}\\frac{n-p}{p-p_0}\\\\\n&=& \\frac{0.684-0.634}{1-0.684}\\frac{45}{2}= 3.56.\n\\end{eqnarray*}\\] La quantile vaut 3.20, nous rejetons donc \\({\\mathrm{H_0}}\\) et privilégions le plus grand modèle.\n\n\n\nExercice 5 (Équivalence du test T et du test F) Récrivons la statistique de test \\(F\\), en se rappelant que \\(X_0\\) est la matrice \\(X\\) privée de sa \\(j^e\\) colonne, celle correspondant au coefficient que l’on teste : \\[\\begin{eqnarray*}\nF&=&\\frac{\\|X\\hat \\beta-P_{X_0}X\\hat \\beta\\|^2}{\\hat \\sigma^2}\n  =\\frac{\\|X_j\\hat \\beta_j-\\hat \\beta_jP_{X_0}X_j\\|^2}{\\hat \\sigma^2}\n=\\frac{\\hat\\beta_j^2}{\\hat \\sigma^2}X_j'(I-P_{X_0})X_j.\n\\end{eqnarray*}\\] Récrivons maintenant le carré de la statistique \\(T\\) en explicitant \\(\\hat\n\\sigma^2_{\\hat \\beta_j}\\) : \\[\\begin{eqnarray*}\nT^2&=&\\frac{\\hat \\beta_j^2}{\\hat \\sigma^2 [(X'X)^{-1}]_{jj}},\n\\end{eqnarray*}\\] où \\([(X'X)^{-1}]_{jj}\\) est le \\(j^e\\) élément diagonal de la matrice \\((X'X)^{-1}\\). Afin de calculer ce terme, nous utilisons la formule permettant d’obtenir l’inverse d’une matrice bloc, formule donnée en annexe A.2. Pour appliquer facilement cette formule, en changeant l’ordre des variables, la matrice \\(X\\) devient \\((X_0|X_j)\\) et \\(X'X\\) s’écrit alors \\[\\begin{eqnarray*}\nX'X&=&\\left(\n\\begin{array}{c|c}\nX'_0X_0&X'_0X_j\\\\\\hline\nX'_jX_0&X'_jX_j\n\\end{array}\\right).\n\\end{eqnarray*}\\] Son inverse, en utilisant la formule d’inverse de matrice bloc, est \\[\\begin{eqnarray*}\n[(X'X)^{-1}]_{jj}&=&\\left(X'_jX_j-X'_jX_0(X'_0X_0)^{-1}X'_0X_j\\right)^{-1}\n=\\left(X_j'(I-P_{X_0})X_j\\right)^{-1}.\n\\end{eqnarray*}\\] Nous avons donc \\(T^2=F\\). Au niveau des lois, l’égalité est aussi valable et nous avons que le carré d’un Student à \\((n-p)\\) ddl est une loi de Fisher à \\((1,n-p)\\) ddl. Bien entendu, le quantile \\((1-\\alpha)\\) d’une loi de Fisher correspond au quantile \\(1-\\alpha/2\\) d’une loi de Student. La loi \\(\\mathcal{T}\\) est symétrique autour de 0 et donc, lorsqu’elle est élevée au carré, les valeurs plus faibles que \\(t_{n-p}(\\alpha/2)\\), qui ont une probabilité sous \\({\\mathrm{H_0}}\\) de \\(\\alpha/2\\) d’apparaître, et celles plus fortes que \\(t_{n-p}(1-\\alpha/2)\\), qui ont une probabilité sous \\({\\mathrm{H_0}}\\) de \\(\\alpha/2\\) d’apparaître, deviennent toutes plus grandes que \\(t^2_{n-p}(1-\\alpha/2)\\). La probabilité que ces valeurs dépassent ce seuil sous \\({\\mathrm{H_0}}\\) est de \\(\\alpha\\) et correspond donc bien par définition à \\(f_{1,n-p}(1-\\alpha)\\).\n\n\nExercice 6 (Équivalence du test F et du test de VM) Nous avons noté la vraisemblance en début du chapitre par \\[\\begin{eqnarray*}\n\\mathcal{L}(Y,\\beta,\\sigma^2) &=& \\prod_{i=1}^n f_{Y}(y_i)\n= \\left(\\frac{1}{2\\pi\\sigma^2}\\right)^{n/2}\\exp{\\left[-\\frac{1}{2 \\sigma^2}\n\\sum_{i=1}^n \\left(y_i- \\sum_{j=1}^p\\beta_jx_{ij}\\right)^2\\right]}\\\\\n&=& \\left(\\frac{1}{2\\pi\\sigma^2}\\right)^{n/2}\\exp{\\left[-\\frac{1}{2 \\sigma^2}\n\\|Y-X\\beta\\|^2\\right]}.\n\\end{eqnarray*}\\] Cette vraisemblance est maximale lorsque \\(\\hat \\beta\\) est l’estimateur des MC et que \\(\\hat \\sigma^2 = \\|Y-X\\hat \\beta\\|^2/n\\). Nous avons alors \\[\\begin{eqnarray*}\n\\max_{\\beta,\\sigma^2} \\mathcal{L}(Y,\\beta,\\sigma^2)&=&\n\\left(\\frac{n}{2\\pi\\|Y-X\\hat \\beta\\|^2 }\\right)^{n/2}\\exp{\\left(-\\frac{n}{2}\\right)}\\\\\n&=&\\left(\\frac{n}{2\\pi \\mathop{\\mathrm{SCR}}}\\right)^{n/2}\\exp{\\left(-\\frac{n}{2}\\right)}\n=\\mathcal{L}(Y,\\hat \\beta,\\hat \\sigma^2),\n\\end{eqnarray*}\\] où \\(\\mathop{\\mathrm{SCR}}=\\|Y-X\\hat \\beta\\|^2\\).\nSous l’hypothèse \\({\\mathrm{H_0}}\\) nous obtenons de façon évidente le résultat suivant : \\[\\begin{eqnarray*}\n\\max_{\\beta,\\sigma^2} \\mathcal{L}_0(Y,\\beta_0,\\sigma^2)\n=\\left(\\frac{n}{2\\pi \\mathop{\\mathrm{SCR}}_0}\\right)^{n/2}\\exp{\\left(-\\frac{n}{2}\\right)}\n=\\mathcal{L}_0(Y,\\hat \\beta_0,\\hat \\sigma^2_0),\n\\end{eqnarray*}\\] où \\(\\mathop{\\mathrm{SCR}}_0\\) correspond à la somme des carrés résiduels sous \\({\\mathrm{H_0}}\\), c’est-à-dire \\(\\mathop{\\mathrm{SCR}}_0=\\|Y-X_0\\hat \\beta_0\\|^2\\). On définit le test du rapport de vraisemblance maximale (VM) par la région critique suivante : \\[\\begin{eqnarray*}\n\\mathcal{D}_\\alpha = \\left\\{\nY \\in \\mathbb R^n : \\lambda=\\frac{\\mathcal{L}_0(Y,\\hat \\beta_0,\\hat \\sigma^2)}\n{\\mathcal{L}(Y,\\hat \\beta,\\hat \\sigma^2)} &lt; \\lambda_0\n\\right\\}.\n\\end{eqnarray*}\\] La statistique du rapport de vraisemblance maximale vaut ici \\[\\begin{eqnarray*}\n\\lambda = \\left(\\frac{\\mathop{\\mathrm{SCR}}}{\\mathop{\\mathrm{SCR}}_0}\\right)^{n/2} =\n\\left(\\frac{\\mathop{\\mathrm{SCR}}_0}{\\mathop{\\mathrm{SCR}}}\\right)^{-n/2}.\n\\end{eqnarray*}\\] Le test du rapport de VM rejette \\({\\mathrm{H_0}}\\) lorsque la statistique \\(\\lambda\\) est inférieure à une valeur \\(\\lambda_0\\) définie de façon à avoir le niveau du test égal à \\(\\alpha\\). Le problème qui reste à étudier est de connaître la distribution (au moins sous \\({\\mathrm{H_0}}\\)) de \\(\\lambda\\). Définissons, pour \\(\\lambda\\) positif, la fonction bijective \\(g\\) suivante : \\[\\begin{eqnarray*}\ng(\\lambda) = \\lambda^{-2/n}-1.\n\\end{eqnarray*}\\] La fonction \\(g\\) est décroissante (sa dérivée est toujours négative), donc \\(\\lambda&lt;\\lambda_0\\) si et seulement si \\(g(\\lambda)&gt;g(\\lambda_0)\\). Cette fonction \\(g\\) va nous permettre de nous ramener à des statistiques dont la loi est connue. Nous avons alors \\[\\begin{eqnarray*}\ng(\\lambda)&&gt;&g(\\lambda_0)\\\\\n\\frac{\\mathop{\\mathrm{SCR}}_0-\\mathop{\\mathrm{SCR}}}{\\mathop{\\mathrm{SCR}}}&&gt;&g(\\lambda_0)\\\\\n\\frac{n-p}{p-p_0}\\frac{\\mathop{\\mathrm{SCR}}_0-\\mathop{\\mathrm{SCR}}}{\\mathop{\\mathrm{SCR}}}&&gt;&f_0\n\\end{eqnarray*}\\] où \\(f_0\\) est déterminée par \\[\\begin{eqnarray*}\nP_{{\\mathrm{H_0}}}\\left(\\frac{n-p}{p-p_0}\\frac{\\mathop{\\mathrm{SCR}}_0-\\mathop{\\mathrm{SCR}}}{\\mathop{\\mathrm{SCR}}}&gt;f_0\n\\right)=\\alpha,\n\\end{eqnarray*}\\] avec la loi de cette statistique qui est une loi \\(\\mathcal{F}_{p-p_0,n-p}\\) (cf.~section précédente). Le test du rapport de VM est donc équivalent au test qui rejette \\({\\mathrm{H_0}}\\) lorsque la statistique \\[\\begin{eqnarray*}\nF=\\frac{n-p}{p-p_0}\\frac{\\mathop{\\mathrm{SCR}}_0-\\mathop{\\mathrm{SCR}}}{\\mathop{\\mathrm{SCR}}}\n\\end{eqnarray*}\\] est supérieure à \\(f_0\\), où \\(f_0\\) est la valeur du fractile \\(\\alpha\\) de la loi de Fisher à \\((p-p_0,n-p)\\) degrés de liberté.\n\n\nExercice 7 (Test de Fisher pour une hypothèse linéaire quelconque) Nous pouvons toujours traduire l’hypothèse \\({\\mathrm{H_0}}\\) : \\(R\\beta=r\\) en terme de sous-espace de \\(\\mathcal M_X\\). Lorsque \\(r=0\\), nous avons un sous-espace vectoriel de \\(\\mathcal M_X\\) et lorsque \\(r\\neq 0\\) nous avons un sous-espace affine de \\(\\mathcal M_X\\). Dans les deux cas, nous noterons ce sous-espace \\(\\mathcal M_0\\) et \\(\\mathcal M_0 \\subset \\mathcal M_X\\). Cependant nous ne pourrons plus le visualiser facilement comme nous l’avons fait précédemment avec \\(\\mathcal M_{X_0}\\) où nous avions enlevé des colonnes à la matrice \\(X\\). Nous allons décomposer l’espace \\(\\mathcal M_X\\) en deux sous-espaces orthogonaux \\[\\begin{eqnarray*}\n\\mathcal M_X = \\mathcal M_0 \\stackrel{\\perp}{\\oplus} ( \\mathcal M_0^\\perp \\cap \\mathcal M_X ).\n\\end{eqnarray*}\\] Sous \\({\\mathrm{H_0}}\\), l’estimation des moindres carrés donne \\(\\hat Y_0\\) projection orthogonale de \\(Y\\) sur \\(\\mathcal M_0\\) et nous appliquons la même démarche pour construire la statistique de test. La démonstration est donc la même que celle du théorème 5.2. C’est-à-dire que nous regardons si \\(\\hat Y_0\\) est proche de \\(\\hat Y\\) et nous avons donc \\[\\begin{eqnarray*}\nF&=&\\frac{\\|\\hat Y -\\hat Y_0\\|^2/\\dim(\\mathcal M_0^{\\perp}  \\cap \\mathcal M_X)}{\\|Y - \\hat Y\\|^2/\n\\dim(\\mathcal M_{X^{\\perp}})}\\\\\n&=&\\frac{n-p}{q} \\frac{\\|Y-\\hat Y_0\\|^2-\\|Y-\\hat Y\\|^2}{ \\|Y-\\hat Y\\|^2}\\\\\n&=& \\frac{n-p}{q}\\frac{\\mathop{\\mathrm{SCR}}_0-\\mathop{\\mathrm{SCR}}}{\\mathop{\\mathrm{SCR}}}\\sim \\mathcal{F}_{q,n-p}.\n\\end{eqnarray*}\\] Le problème du test réside dans le calcul de \\(\\hat Y_0\\). Dans la partie précédente, il était facile de calculer \\(\\hat Y_0\\) car nous avions la forme explicite du projecteur sur \\(\\mathcal M_0\\). Une première façon de procéder revient à trouver la forme du projecteur sur \\(\\mathcal M_0\\). Une autre façon de faire est de récrire le problème de minimisation sous la contrainte \\(R\\beta=r\\). Ces deux manières d’opérer sont présentées en détail dans la correction de l’exercice 2.13. Dans tous les cas l’estimateur des MC contraints par \\(R\\beta=r\\) est défini par \\[\\begin{eqnarray*}\n\\hat \\beta_0&=&\\hat \\beta + (X'X)^{-1}R'[R(X'X)^{-1}R']^{-1}(r-R\\hat \\beta).\n\\end{eqnarray*}\\]\n\n\nExercice 8 (Généralisation de la régression ridge) Soit la fonction à minimiser \\[\\begin{align*}\n    R(\\beta)&=\\|Y-X\\beta\\|^2 -\\sum_{j=1}^{p}\\delta_j(\\beta_j^2) \\\\\n    &= (Y-X\\beta)'(Y-X\\beta) - \\beta' \\Delta \\beta\n  \\end{align*}\\] avec \\(\\delta_{1}, \\dotsc, \\delta_{p}\\) des réels positifs ou nuls.\nSachant que \\(\\frac{\\partial \\beta' A\\beta}{\\partial \\beta}=2A\\beta\\) (avec \\(A\\) symétrique) et que \\(\\frac{\\partial X\\beta}{\\partial \\beta}=X'\\) nous avons la dérivée partielle suivante \\[\\begin{align*}\n    \\frac{\\partial R}{\\partial \\beta}&=-2X'(Y-X\\beta)  + 2\\Delta \\beta\n  \\end{align*}\\] En annulant cette dérivée nous avons \\[\\begin{align*}\n    -2X'(Y-X\\hat\\beta_{\\mathrm{RG}}) + 2\\Delta \\hat\\beta_{\\mathrm{RG}}&=0\\\\\n         (X'X + \\Delta) \\hat\\beta_{\\mathrm{RG}} &=  X'Y\n  \\end{align*}\\] donc en prémultipliant par \\((X'X-\\Delta)^{-1}\\) nous obtenons \\[\\begin{align*}\n    \\hat\\beta_{\\mathrm{RG}}=(X'X-\\Delta)^{-1}X'Y.\n  \\end{align*}\\] En régression multiple le nombre de paramètres est \\(p=\\text{tr}(P_{X})\\) avec \\(P_{X}\\) la matrice de l’endomorphisme qui permet d’obtenir \\(\\hat Y\\) à partir de \\(Y\\). Dans cette régression ridge, nous avons que \\[\\begin{align*}\n    \\hat Y_{\\mathrm{RG}}&=X\\hat\\beta_{\\mathrm{RG}}=X(X'X-\\Delta)^{-1}X'Y\n  \\end{align*}\\] donc la matrice de l’endomorphisme est ici \\(X(X'X-\\Delta)^{-1}X'\\) et le nombre équivalent de paramètres est \\(\\text{tr}(X(X'X-\\Delta)^{-1}X')\\).\n\n\nExercice 9 (IC pour la régression ridge)  \n\nLoi de \\(\\hat \\beta\\) : \\(N(\\beta, \\sigma^{2}(X'X)^{-1})\\) grâce au modèle et à \\({\\mathcal{H}}_3\\).\nLoi de $_{}() $. Comme \\(\\hat \\beta_{\\mathrm{ridge}}(\\tilde\\kappa)= (X'X-\\tilde\\kappa I)^{-1}X'Y\\) avec \\(A=(X'X-\\tilde\\kappa I)^{-1}X'\\) qui est une matrice fixe. Avec \\({\\mathcal{H}}_{3}\\) et le modèle de régression multiple on a que \\(Y\\sim N(X\\beta, \\sigma^{2}I)\\).\nPuisque \\(Y\\) est un vecteur gaussien, il en est de même de \\(\\hat \\beta_{\\mathrm{ridge}}(\\tilde\\kappa)=AY\\). Calculons son espérance \\[\\begin{align*}\n\\mathbf E(\\hat \\beta_{\\mathrm{ridge}}(\\tilde\\kappa))&=\\mathbf E(AY)=A\\mathbf E(Y)=AX\\beta\\\\\n&=(X'X-\\tilde\\kappa I)^{-1}X'X\\beta\n\\end{align*}\\] et sa variance \\[\\begin{align*}\n\\mathop{\\mathrm{V}}(\\hat \\beta_{\\mathrm{ridge}}(\\tilde\\kappa))&=\\mathop{\\mathrm{V}}(AY)=A\\mathop{\\mathrm{V}}(Y)A'=A\\sigma^{2}I A' = \\sigma^{2} A A'\\\\\n&=\\sigma^{2}(X'X-\\tilde\\kappa I)^{-1}X'X(X'X-\\tilde\\kappa I)^{-1}.\n\\end{align*}\\]\nCalculons le produit scalaire de \\(Y-\\hat Y_{\\mathrm{ridge}}\\) et \\(\\hat Y_{MC}:\\) \\[\\begin{align*}\n&lt;Y-\\hat Y_{\\mathrm{ridge}};\\hat Y_{MC}&gt;&=&lt;Y-\\hat Y_{MC} + \\hat Y_{MC} -\\hat Y_{\\mathrm{ridge}};\\hat Y_{MC}&gt;  \\\\\n& =  &lt;Y-\\hat Y_{MC}; \\hat Y_{MC}&gt; + &lt;   \\hat Y_{MC} -\\hat Y_{\\mathrm{ridge}} ;  \\hat Y_{MC}&gt;\\\\\n&= 0 + &lt;   \\hat Y_{MC} -\\hat Y_{\\mathrm{ridge}} ;  \\hat Y_{MC}&gt;\n\\end{align*}\\] Or \\(\\hat Y_{\\mathrm{ridge}} = X\\beta_{\\mathrm{ridge}}(\\tilde\\kappa)\\) donc il appartient au sous espace vectoriel \\(\\Im(X)\\), de même que \\(\\hat Y_{MC}=P_{X}Y\\). Sauf si \\(\\tilde\\kappa=0\\) on a que \\(\\hat Y_{\\mathrm{ridge}}\\neq \\hat Y_{MC}\\) donc \\(\\hat Y_{MC} -\\hat Y_{\\mathrm{ridge}}\\) est un vecteur non nul de \\(\\Im(X)\\) et donc son produit scalaire avec \\(\\hat Y_{MC}\\in \\Im(X)\\) est non nul.\nIl faut pouvoir démontrer l’indépendance de \\(\\hat\\sigma_{\\mathrm{ridge}}\\) et \\(\\hat \\beta_{\\mathrm{ridge}}\\). Pour le théorème 5.1, on montre l’indépendance entre\\(\\hat \\beta\\) et \\(\\hat \\sigma\\) en considérant les 2 vecteurs \\(\\hat\\beta\\) et \\(\\hat \\varepsilon=(Y-\\hat Y)\\). Comme nous pouvons écrire \\(\\hat \\beta=(X'X)^{-1}X'P_XY\\), \\(\\hat \\beta\\) est donc une fonction fixe (dépendante uniquement des \\(X\\)) de \\(P_XY\\). De plus, \\(\\hat \\varepsilon=P_{X^\\perp}Y\\) est orthogonal à \\(P_XY\\). Ces 2 vecteurs suivent des lois normales et sont donc indépendants. Il en résulte que \\(\\hat \\beta\\) et \\(Y-\\hat Y\\) sont indépendants et de même pour \\(\\hat \\beta\\) et \\(\\hat \\sigma\\).\nIci, \\(\\hat\\sigma_{\\mathrm{ridge}}\\) est une fonction de \\(Y-\\hat Y_{\\mathrm{ridge}}\\). Le vecteur \\(\\hat\\beta_{\\mathrm{ridge}}=(X'X+\\tilde\\kappa I_p)^{-1}X'Y=(X'X+\\tilde\\kappa I_p)^{-1}X'P_XY\\) est une fonction fixe (\\(\\tilde \\kappa\\) est considéré comme fixé) de \\(P_XY\\). Par contre, \\(P_XY\\) n’est pas orthogonal à \\((Y-\\hat Y_{\\mathrm{ridge}})\\), comme nous l’avons montré, nous ne pouvons donc montrer l’indépendance de \\(\\hat\\beta_{\\mathrm{ridge}}\\) et \\(\\hat\\sigma_{\\mathrm{ridge}}\\).\nUne autre idée serait d’utiliser \\(\\hat\\sigma\\) mais en général si l’on utilise la régression ridge c’est que l’on se doute que \\(\\hat Y\\) n’est pas un bon estimateur de \\(X\\beta\\) et donc \\(\\hat\\sigma\\) qui est une fonction de \\(Y-\\hat Y\\) risque de ne pas être un bon estimateur de \\(\\sigma\\). L’estimateur \\(\\hat\\sigma\\) peut même être nul, ce qui pratiquement peut arriver quand \\(p&gt;n\\).\nEn général quand \\(X\\) est fixe pour un bootstrap en régression on estime \\(\\hat \\beta\\) puis on déduit les \\(\\{\\hat \\epsilon_{i}\\}\\). De cet ensemble sont tirés de manière équiprobable avec remise \\(n\\) résidus \\(\\{\\hat \\epsilon_{i}^{*}\\}\\). Ces nouveaux résidus sont additionnés à \\(X\\beta\\) pour faire un nouveau vecteur \\(Y^{*}\\) et avoir un échantillon bootstap \\(Y^{*}, X\\).\nIci l’estimation de \\(\\hat \\beta\\) sera mauvaise (et c’est pour cela que l’on utilise la régression ridge) et plutôt que d’estimer de mauvais résidus nous allons retirer avec remise parmi les \\(Y_{i}, X_{i.}\\) ce qui est la procédure adaptée au \\(X\\) aléatoire mais ici nous avons peu de choix\nEntrées : \\(\\tilde \\kappa\\) fixé, \\(\\alpha\\) fixé, \\(B\\) choisi.  Sorties : IC, au niveau \\(\\alpha\\), coordonnée par coordonnée de \\(\\beta\\).\n\nEstimer \\(\\beta_{\\mathrm{ridge}}(\\tilde \\kappa)\\) .\nEn déduire \\(\\hat \\varepsilon_{\\mathrm{ridge}}=Y-X\\hat \\beta_{\\mathrm{ridge}}\\).\nPour \\(k=1\\) à \\(B\\)\n\ntirer avec remise \\(n\\) résidus estimés parmi les \\(n\\) coordonnées de \\(\\hat \\varepsilon_{\\mathrm{ridge}}\\) ;\non note ces résidus (réunis dans 1 vecteur) \\(\\hat \\varepsilon_{\\mathrm{ridge}}^{(k)}\\) ;\nconstruire 1 échantillon \\(Y^{(k)}=X\\beta_{\\mathrm{ridge}}(\\tilde \\kappa)+\\hat \\varepsilon_{\\mathrm{ridge}}^{(k)}\\) ;\n\\(\\tilde \\kappa^{(k)} \\leftarrow \\tilde \\kappa\\) ;\nestimer le vecteur de paramètre \\(\\beta_{\\mathrm{ridge}}^{(k)}(\\tilde \\kappa^{(k)})=(X'X+\\tilde\\kappa^{(k)} I_p)^{-1}X'Y^{(k)}\\) ;\n\nPour \\(j=1\\) à \\(p\\)\n\ncalculer les quantiles empiriques de niveau \\(\\alpha/2\\) et \\(1-\\alpha/2\\) pour la coordonnée \\(j\\), sur tous les vecteurs \\(\\{\\beta_{\\mathrm{ridge}}^{(k)}(\\tilde \\kappa)\\}\\) ;\n\n\nL’algorithme est presque le même. Cependant comme \\(\\tilde \\kappa\\) n’est pas fixé, pour estimer \\(\\beta_{\\mathrm{ridge}}(\\tilde \\kappa)\\) il faut déterminer \\(\\tilde \\kappa\\) par une méthode choisie. Ensuite, à chaque estimation de \\(\\beta_{\\mathrm{ridge}}^{(k)}(\\tilde \\kappa^{(k)})\\), il est nécessaire au préalable de déterminer \\(\\tilde \\kappa^{(k)}\\) par la même méthode que celle utilisée pour déterminer \\(\\tilde \\kappa\\).\n\n\n\n\n\n Retour au sommet",
    "crumbs": [
      "Corrections",
      "II Inférence",
      "6 Inférence dans le modèle gaussien"
    ]
  },
  {
    "objectID": "correction/chap8.html",
    "href": "correction/chap8.html",
    "title": "8 Choix de variables",
    "section": "",
    "text": "Exercice 1 (Questions de cours) A, C, B en général. Un cas particulier de la dernière question est le suivant : si les variables sélectionnées \\(\\xi\\) engendrent un sous-espace orthogonal au sous-espace engendré par les variables non sélectionnées \\(\\bar\\xi\\), alors C est la bonne réponse.\n\n\nExercice 2 (Analyse du biais) La preuve des deux premiers points s’effectue comme l’exemple de la section 7.2.1. Nous ne détaillerons que le premier point. Supposons que \\(|\\xi|\\) soit plus petit que \\(p\\), le “vrai” nombre de variables entrant dans le modèle. Nous avons pour estimateur de \\(\\beta\\) \\[\n\\hat \\beta_{\\xi} = (X_{\\xi}'X_{\\xi})^{-1}X_{\\xi}'Y = P_{X_{\\xi}}Y.\n\\] Le vrai modèle étant obtenu avec \\(p\\) variables, \\(\\mathbf E(Y)=X_p \\beta\\). Nous avons alors \\[\n\\begin{eqnarray*}\n\\mathbf E(\\hat \\beta_{\\xi})&=&P_{X_{\\xi}}X_p \\beta\\\\\n&=& P_{X_{\\xi}}X_{\\xi} \\beta_{\\xi} +P_{X_{\\xi}}X_{\\bar \\xi} \\beta_{\\bar \\xi}.\n\\end{eqnarray*}\n\\] Cette dernière quantité n’est pas nulle sauf si \\(\\Im(X_{\\xi}) \\perp \\Im(X_{\\bar \\xi})\\). Comme \\(\\hat \\beta_{\\xi}\\) est en général biaisé, il en est de même pour la valeur prévue \\(\\hat y_{\\xi}\\) dont l’espérance ne vaudra pas \\(X\\beta\\).\n\n\nExercice 3 (Variance des estimateurs) L’estimateur obtenu avec les \\(|\\xi|\\) variables est noté \\(\\hat \\beta_{\\xi}\\) et l’estimateur obtenu dans le modèle complet \\(\\hat \\beta\\). Ces vecteurs ne sont pas de même taille, le premier est de longueur \\(|\\xi|\\), le second de longueur \\(p\\). Nous comparons les \\(|\\xi|\\) composantes communes, c’est-à-dire que nous comparons \\(\\hat \\beta_{\\xi}\\) et \\([\\hat \\beta]_{\\xi}\\). Partitionnons la matrice \\(X\\) en \\(X_{\\xi}\\) et \\(X_{\\bar \\xi}\\). Nous avons alors \\[\n\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}(\\hat \\beta) &=& \\sigma^2 \\left(\n\\begin{array}{cc}\nX'_{\\xi}X_{\\xi} &X'_{\\xi}X_{\\bar \\xi}\\\\\nX'_{\\bar \\xi}X_{\\xi} &X'_{\\bar \\xi}X_{\\bar \\xi}\n\\end{array}\n\\right)^{-1}.\n\\end{eqnarray*}\n\\] En utilisant la formule d’inverse par bloc, donnée en annexe A, nous obtenons \\[\n\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}([\\hat \\beta]_{\\xi}) &=& \\sigma^2 \\left[X_{\\xi}'X_{\\xi}-X_{\\xi}'X_{\\bar \\xi}(X_{\\bar \\xi}'\nX_{\\bar \\xi})^{-1}X_{\\bar \\xi}'X_{\\xi}\\right]^{-1},\n\\end{eqnarray*}\n\\] alors que la variance de \\(\\hat \\beta_{\\xi}\\) vaut \\[\n\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}(\\hat \\beta_{\\xi}) &=& \\sigma^2 \\left[X_{\\xi}'X_{\\xi}\\right]^{-1}.\n\\end{eqnarray*}\n\\] Nous devons comparer \\(\\mathop{\\mathrm{V}}([\\hat \\beta]_{\\xi})\\) et \\(\\mathop{\\mathrm{V}}(\\hat \\beta_{\\xi})\\). Nous avons \\[\n\\begin{eqnarray*}\nX_{\\xi}'X_{\\xi}-X_{\\xi}'X_{\\bar \\xi}(X_{\\bar \\xi}'\nX_{\\bar \\xi})^{-1}X_{\\bar \\xi}'X_{\\xi}=X_{\\xi}'(I-P_{X_{\\bar \\xi}})X_{\\xi}\n=X_{\\xi}'P_{X_{\\bar \\xi}^\\perp}X_{\\xi}.\n\\end{eqnarray*}\n\\] La matrice \\(P_{X_{\\bar \\xi}^\\perp}\\) est la matrice d’un projecteur, alors elle est semi-définie positive (SDP) (cf. annexe A), donc \\(X_{\\xi}'P_{X_{\\bar \\xi}^\\perp}X_{\\xi}\\) est également SDP. La matrice \\(X_{\\xi}'P_{X_{\\bar \\xi}^\\perp}X_{\\xi}-X_{\\xi}'P_{X_{\\bar \\xi}^\\perp}X_{\\xi}\\) est définie positive (DP) puisque c’est \\(\\mathop{\\mathrm{V}}([\\hat \\beta]_{\\xi})/ \\sigma^2\\). Utilisons le changement de notation suivant : \\[\n\\begin{eqnarray*}\nA=X_{\\xi}'X_{\\xi}-X_{\\xi}'P_{X_{\\bar \\xi}}X_{\\xi} \\quad \\hbox{et} \\quad\nB=X_{\\xi}'P_{X_{\\bar \\xi}}X_{\\xi}.\n\\end{eqnarray*}\n\\] La matrice \\(A\\) est DP et la matrice \\(B\\) SDP. La propriété donnée en annexe A indique que \\(A^{-1}-(A+B)^{-1}\\) est SDP, or \\[\n\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}([\\hat \\beta]_{\\xi})-\\mathop{\\mathrm{V}}(\\hat \\beta_{\\xi}) = \\sigma^2 (A^{-1}-(A+B)^{-1}).\n\\end{eqnarray*}\n\\] Donc la quantité \\(\\mathop{\\mathrm{V}}([\\hat \\beta]_{\\xi})-\\mathop{\\mathrm{V}}(\\hat \\beta_{\\xi})\\) est SDP. Le résultat est démontré. L’estimation, en terme de variance, de \\(\\xi\\) composantes est plus précise que les mêmes \\(\\xi\\) composantes extraites d’une estimation obtenue avec \\(p\\) composantes.\nLa variance des valeurs ajustées dépend de la variance de \\(\\hat \\beta\\), le point 2 de la proposition se démontre de façon similaire.\nRemarque : nous venons de comparer deux estimateurs de même taille via leur matrice de variance. Pour cela, nous montrons que la différence de ces deux matrices est une matrice SDP. Que pouvons-nous dire alors sur la variance de chacune des coordonnées ? Plus précisément, pour simplifier les notations, notons le premier estimateur (de taille \\(p\\)) \\(\\tilde \\beta\\) de variance \\(\\mathop{\\mathrm{V}}(\\tilde \\beta)\\) et le second estimateur \\(\\hat \\beta\\) de variance \\(\\mathop{\\mathrm{V}}(\\hat \\beta)\\). Si \\(\\mathop{\\mathrm{V}}(\\tilde \\beta)-\\mathop{\\mathrm{V}}(\\hat \\beta)\\) est SDP, pouvons-nous dire que \\(\\mathop{\\mathrm{V}}(\\tilde \\beta_i)-\\mathop{\\mathrm{V}}(\\hat \\beta_i)\\) est un nombre positif pour \\(i\\) variant de \\(1\\) à \\(p\\) ? Considérons par exemple le vecteur \\(u_1'=(1,0,\\dotsc,0)\\) de \\(\\mathbb R^p\\). Nous avons alors \\[\nu_1' \\hat \\beta = \\hat \\beta_1 \\quad \\hbox{et}\n\\quad u_1' \\tilde \\beta = \\tilde \\beta_1.\n\\] Comme \\(\\mathop{\\mathrm{V}}(\\tilde \\beta)-\\mathop{\\mathrm{V}}(\\hat \\beta)\\) est SDP, nous avons pour tout vecteur \\(u\\) de \\(\\mathbb R^p\\) que \\(u'(\\mathop{\\mathrm{V}}(\\tilde \\beta)-\\mathop{\\mathrm{V}}(\\hat \\beta))u\\geq 0\\), c’est donc vrai en particulier pour \\(u_1\\). Nous avons donc \\[\n\\begin{eqnarray*}\nu_1'(\\mathop{\\mathrm{V}}(\\tilde \\beta)-\\mathop{\\mathrm{V}}(\\hat \\beta))u_1&\\geq& 0\\\\\nu_1'\\mathop{\\mathrm{V}}(\\tilde \\beta)u_1-u_1'\\mathop{\\mathrm{V}}(\\hat \\beta)u_1&\\geq& 0\\\\\n\\mathop{\\mathrm{V}}(u_1'\\tilde \\beta)-\\mathop{\\mathrm{V}}(u_1'\\hat \\beta)&\\geq& 0\\\\\n\\mathop{\\mathrm{V}}(\\tilde \\beta_1) &\\geq & \\mathop{\\mathrm{V}}(\\hat \\beta_1).\n\\end{eqnarray*}\n\\] Nous pouvons retrouver ce résultat pour les autres coordonnées des vecteurs estimés ou encore pour des combinaisons linéaires quelconques de ces coordonnées.\n\n\nExercice 4 (Choix de variables) Tous les modèles possibles ont été étudiés, la recherche est donc exhaustive. En prenant comme critère l’AIC ou le BIC, le modèle retenu est le modèle M134. Comme prévu, le \\(\\mathop{\\mathrm{R^2}}\\) indique le modèle conservant toutes les variables. Cependant le \\(\\mathop{\\mathrm{R^2}}\\) peut être utilisé pour tester des modèles emboîtés. Dans ce cas, le modèle retenu est également le M134.\nPour une procédure avec test nous devons démarrer d’un modèle. Démarrons par exemple du modèle avec uniquement la constante. Nous ajoutons une variable après l’autre. Nous ajoutons celle avec la statistique de test la plus élevée et cette valeur doit être plus grande que 2.3 (sinon aucune variable n’est ajoutée et le modèle courant est conservé). Dans les modèles à une variable, c’est le modèle M1 qui est choisi (statistique la plus élevée de 41.9 et supérieure à 2.3). Ensuite nous ajoutons à M1 une variable (2 ou 3 ou 4) et la meilleure est la 4 mais la statistique est de 0.9 (&lt; 2.3) on n’ajoute pas de variable et on choisit M1.\nDémarrons du modèle complet 1234, on enlève la moins significative (la valeur de la statistique de test la plus faible en dehors de la constante et qui doit être plus faible que 2.3). Ici nous enlevons la variable 2 et nous avons donc le modèle M134. Dans ce modèle la variable la moins significative est la 3 mais sa statistique est plus grande que 2.3, on conserve donc le modèle M134.\n\n\nExercice 5 (Utilisation du \\(R^2\\)) Plaçons nous dans le cas pratique où la constante fait partie des modèles, elle est donc dans une des colonnes de \\(Z\\) (par exemple la première). Le \\(\\mathop{\\mathrm{R^2}}\\) est par définition \\[\n\\begin{align*}\n\\mathop{\\mathrm{R^2}}(Z)&=\\frac{\\|P_{Z}Y - P_{\\mathbf{1}}Y\\|^{2}}{\\|Y - P_{\\mathbf{1}}Y\\|^{2}}\\\\\n\\mathop{\\mathrm{R^2}}(X)&=\\frac{\\|P_{X}Y - P_{\\mathbf{1}}Y\\|^{2}}{\\|Y - P_{\\mathbf{1}}Y\\|^{2}}\n\\end{align*}\n\\] Comme \\(\\Im(Z)\\subset \\Im(X)\\) on peut décomposer en deux \\(\\Im(X)\\): la partie \\(\\Im(Z)\\) puis le reste ce qui se note \\[\n\\Im(X)=\\Im(Z) \\stackrel{\\perp}{\\oplus} (\\Im(X)\\cap \\Im(Z)^{\\perp})\n\\] Au niveau des projecteurs on a donc \\[\nP_{X}=P_{Z} + P_{X\\cap Z^{\\perp}}\n\\] ce qui permet d’écrire le \\(\\mathop{\\mathrm{R^2}}\\) du modèle avec \\(X\\) comme suit puis par pythagore (\\((P_{Z}Y - P_{\\mathbf{1}}Y)\\in \\Im(Z)\\) et \\(P_{X\\cap Z^{\\perp}}Y\\in \\Im(Z)^{\\perp}\\)) \\[\n\\begin{align*}\n\\mathop{\\mathrm{R^2}}(X)&=\\frac{\\|P_{Z}Y + P_{X\\cap Z^{\\perp}}Y - P_{\\mathbf{1}}Y\\|^{2}}{\\|Y - P_{\\mathbf{1}}Y\\|^{2}} \\\\\n&= \\frac{\\|(P_{Z}Y - P_{\\mathbf{1}}Y) + P_{X\\cap Z^{\\perp}}Y \\|^{2}}{\\|Y - P_{\\mathbf{1}}Y\\|^{2}}\\\\\n&=\\frac{\\|(P_{Z}Y - P_{\\mathbf{1}}Y) \\|^{2}}{\\|Y - P_{\\mathbf{1}}Y\\|^{2}} + \\frac{\\|P_{X\\cap Z^{\\perp}}Y \\|^{2}}{\\|Y - P_{\\mathbf{1}}Y\\|^{2}}= \\mathop{\\mathrm{R^2}}2(Z) + \\frac{\\|P_{X\\cap Z^{\\perp}}Y \\|^{2}}{\\|Y - P_{\\mathbf{1}}Y\\|^{2}}\n\\end{align*}\n\\] Comme la seconde partie est positive ou nulle on a que \\(\\mathop{\\mathrm{R^2}}(X)\\) est au moins aussi grand que \\(\\mathop{\\mathrm{R^2}}(Z)\\). Il y a égalité dans le cas rare où \\(Y\\perp (\\Im(X)\\cap \\Im(Z)^{\\perp})\\) c’est à dire que l’on a ajouté des variables qui ne sont pas entièrement reliées aux variables de \\(Z\\) mais dont la partie non redondante avec celle de \\(Z\\) (qui existe car le rang de \\(X\\) est \\(p\\)) a une corrélation empirique avec \\(Y\\) de 0 exactement. Comme le \\(\\mathop{\\mathrm{R^2}}\\) augmente en ajoutant des variables le modèle sélectionné sera celui avec toutes les variables.\nDonc si nous cherchons à expliquer la concentration d’ozone à Rennes et si nous avons en même temps la consommation de nouille au Viêt Nam les mêmes jours, le \\(\\mathop{\\mathrm{R^2}}\\) nous conduira à sélectionner aussi la consommation de nouille au Viet-Nam comme variable explicative, variable dont on sent le peu d’influence sur la concentration d’ozone.\n\n\nExercice 6 (Cas orthogonal)  \n\nLes variables sont orthogonales donc on a \\(X'X=I_{p}\\) et l’estimateur des MCO s’écrit \\[\n\\hat \\beta=(X'X)^{-1}X'Y=X'Y\n\\] En remplaçant \\(Y\\) par le modèle (\\(Y=X\\beta + \\varepsilon\\)) on a \\[\n\\hat \\beta=X'X\\beta + X'\\varepsilon = \\beta + X'\\varepsilon.\n\\]\nLa somme des résidus vaut ici \\[\n\\begin{align*}\n\\mathop{\\mathrm{SCR}}&=\\|Y-X\\hat\\beta\\|^{2}=(Y-X\\hat\\beta)'(Y-X\\hat\\beta)= Y'Y - 2Y'X\\hat\\beta + \\hat\\beta'X'X\\hat\\beta\\\\\n&= \\sum_{i=1}^n y_{i}^2- 2Y'X\\hat\\beta + \\hat\\beta'X'X\\hat\\beta.\n\\end{align*}\n\\] Évaluons le second terme du membre de droite: comme \\(X\\hat\\beta=P_{X}Y\\) et en utilisant le fait que \\(Y=P_{X}Y + P_{X^{\\perp}}Y\\) on obtient \\[\nY'X\\hat\\beta= (P_{X}Y + P_{X^{\\perp}}Y)'P_{X}Y= Y' P_{X}' P_{X}Y = \\hat\\beta'X'X\\hat \\beta\n\\] car \\(P_{X}Y\\) et \\(P_{X^{\\perp}}Y\\) sont orthogonaux et leur produit scalaire \\(Y'P_{X^{\\perp}}'P_{X}Y\\) vaut 0. On a donc \\[\n\\mathop{\\mathrm{SCR}}=\\sum_{i=1}^n y_{i}^2 - \\hat\\beta'X'X\\hat \\beta\n\\] Comme ici la matrice \\(X\\) est orthogonale on a \\(X'X=I_{p}\\) et l’expression devient \\[\n\\mathop{\\mathrm{SCR}}=\\sum_{i=1}^n y_{i}^2 - \\sum_{j=1}^p \\hat \\beta_{j}\n\\tag{1}\\] La somme du carré des résidus est d’autant plus faible que les coefficients estimés sont grands en valeur absolue.\nPrenons un modèle \\(\\xi\\) qui regroupe les \\(k\\) premières variables (ce qui est pratique pour les notations). Comme la matrice \\(X\\) est orthogonale on a au niveau des sous espaces que toutes les variables engendrent des sous-espaces vectoriels orthogonaux que l’on peut regrouper: \\[\n\\begin{align*}\n\\Im(X) &= \\Im(X_{1})\\stackrel{\\perp}{\\oplus} \\Im(X_{2})\\stackrel{\\perp}{\\oplus} \\cdots \\stackrel{\\perp}{\\oplus}\\Im(X_{p})\\\\\n&= \\Im(X_{\\xi})\\stackrel{\\perp}{\\oplus}\\Im(X_{\\bar\\xi})\n\\end{align*}\n\\] Ce qui donne au niveau des projecteurs \\[\nP_{X}= P_{X_{\\xi}} + P_{X_{\\bar\\xi}}\n\\] Écrivons par ailleurs l’ajustement: \\[\n\\begin{align*}\nP_{X}Y&= X\\hat \\beta= X_{1}\\hat \\beta_{1} + X_{2}\\hat \\beta_{2} + \\cdots +X_{p}\\hat \\beta_{p}\\\\\nP_{X_{\\xi}}Y + P_{X_{\\bar\\xi}}Y&= X_{1}\\hat \\beta_{1} + X_{2}\\hat \\beta_{2} + \\cdots +X_{p}\\hat \\beta_{p}\n\\end{align*}\n\\] Si je projette sur \\(\\Im(X_{\\xi})\\) l’équation ci-dessus cela nous donne (\\(\\Im(X_{\\bar\\xi})\\subset \\Im(X_{\\xi})^{\\perp}\\)) \\[\nP_{X_{\\xi}}Y= X_{1}\\hat \\beta_{1}  + \\cdots +X_{k}\\hat \\beta_{k}\n\\] Or nous savons que les MCO dans le modèle \\(\\xi\\) donne l’ajustement \\(P_{X_{\\xi}}Y=X_{\\xi}\\hat\\beta_{\\xi}\\) qui est unique, ce qui donne en identifiant les deux écritures que \\[\nX_{1}\\hat \\beta_{1}  + \\cdots +X_{k}\\hat \\beta_{k}= X_{\\xi}\n\\] Comme une base de \\(\\Im(X_{\\bar\\xi})\\) est donnée par les colonnes orthonormées de \\(X_{\\xi}\\) on en déduit que les coefficients dans la base sont uniques d’où \\[\n[\\hat \\beta]_{\\xi}= \\hat\\beta_{\\xi}.\n\\]\nLes critères AIC et BIC valent pour le modèle \\(\\xi\\) \\[\n-2\\mathcal{L} + 2 |\\xi + 1| f(n)\n\\] et quand on compare le modèle \\(\\xi\\) et le modèle \\(\\{\\xi, l\\}\\) on compare donc \\[\n-2\\mathcal{L}(\\xi) + 2 |\\xi + 1| f(n) \\ \\ \\mathrm{et} \\ \\ -2\\mathcal{L}(\\{\\xi, l\\}) + 2 |\\xi + 2| f(n)\n\\] Comme \\(\\mathcal{L}(\\xi)\\) vaut \\(-{n}/{2}\\log\\sigma^{2} -{n}/{2}-{1}/{2\\sigma^{2}}.\\mathop{\\mathrm{SCR}}(\\xi)\\) lorsque l’on compare on peut éliminer les termes identiques et il reste donc \\[\n\\frac{1}{\\sigma^{2}}\\mathop{\\mathrm{SCR}}(\\xi) \\ \\ \\mathrm{et} \\ \\  \\frac{1}{\\sigma^{2}}\\mathop{\\mathrm{SCR}}(\\{\\xi, l\\}) + 2  f(n)\n\\] Effectuons la différence: \\[\n\\Delta =  \\frac{1}{\\sigma^{2}}(\\mathop{\\mathrm{SCR}}(\\{\\xi, l\\}) - \\mathop{\\mathrm{SCR}}(\\xi)) + 2  f(n)\n\\] et en utilisant équation 1 et la question 3 on obtient \\[\n\\begin{align*}\n\\Delta &= \\frac{1}{\\sigma^{2}}(\\sum_{i=1}^n y_{i}^2 - \\sum_{\\xi} \\hat \\beta_{j}^2 - \\hat \\beta_{l}^2 - \\sum_{i=1}^n y_{i}^2 + \\sum_{\\xi} \\hat \\beta_{j}^2)+ 2  f(n)\\\\\n&= -\\frac{\\hat \\beta_{l}^2}{\\sigma^{2}} + 2f(n)\n\\end{align*}\n\\] Quand \\(\\Delta\\) est négatif l’AIC (ou le BIC) du modèle \\(\\{\\xi, l\\}\\) est plus faible que l’AIC (ou le BIC) que celui du modèle \\(\\xi\\), c’est à dire que l’AIC (ou le BIC) du modèle \\(\\{\\xi, l\\}\\) est meilleur: on ajoute la variable \\(l\\). Cela donne donc \\[\n\\begin{align*}\n\\Delta & &lt; 0\\\\\n-\\frac{\\hat \\beta_{l}^2}{\\sigma^{2}} + 2f(n) & &lt; 0\\\\\n\\frac{\\hat \\beta_{l}^2}{\\sigma^{2}}&&gt; 2f(n)\n\\end{align*}\n\\]\nEn écrivant que \\[\nN=\\frac{\\mathop{\\mathrm{SCR}}(\\xi) - \\mathop{\\mathrm{SCR}}(\\{\\xi, l\\})}{\\sigma^{2}}\n\\] nous voyons qu’il faut évaluer la différence des SCR. En utilisant équation 1 et la question 3 on a \\[\n\\begin{align*}\n\\mathop{\\mathrm{SCR}}(\\xi) - \\mathop{\\mathrm{SCR}}(\\{\\xi, l\\})&=\\sum_{i=1}^n y_{i}^2  -  \\sum_{\\xi} \\hat \\beta_{j}^2 - \\sum_{i=1}^n y_{i}^2 + \\sum_{\\xi} \\hat \\beta_{j}^2 + \\hat \\beta_{l}^2  \\\\\n&= \\hat \\beta_{l}^2\n\\end{align*}\n\\] Nous en déduisons la valeur de \\(N\\) (la dernière égalité découle du calcul de \\(\\hat \\beta\\) en question 1) \\[\nN=\\frac{\\hat\\beta_{l}^{2}}{\\sigma^{2}}= \\frac{(\\beta_{l} + [X' \\varepsilon]_{l})^{2}}{\\sigma^{2}}\n\\] Le dernier membre nous indique que la partie aléatoire est \\([X' \\varepsilon]_{l}\\). D’après \\({\\mathcal{H}}_{3}\\) on a que \\(X' \\varepsilon\\) est gaussien d’espérance \\(\\mathbf E(X'\\varepsilon)=X'\\mathbf E(\\varepsilon)=0\\) et de variance \\(\\mathop{\\mathrm{V}}(X'\\varepsilon)=X'\\mathop{\\mathrm{V}}(\\varepsilon)X=\\sigma^{2}I_{p}\\). Sa coordonnée \\(l\\) notée \\([X' \\varepsilon]_{l}\\) est donc une loi normale \\(N(0, \\sigma^{2})\\) et on en déduit que \\[\n\\frac{\\hat \\beta_{l}}{\\sigma} \\sim N(\\frac{\\beta_{l}}{\\sigma}, 1).\n\\]\nLe carré de cette loi normale (noté \\(N\\)) est donc un \\(\\chi^{2}(1)\\) décentré de paramètre de décentrage \\({\\beta_{l}^{2}}/{\\sigma^{2}}\\). On connait donc la loi de la statistique de test \\(N\\) (un \\(\\chi^{2}(1)\\) décentré de paramètre de décentrage \\(\\beta_{l}^{2}\\)). Quand \\({\\mathrm{H_0}}\\) est vrai, c’est-à-dire le modèle \\(\\xi\\) est valide, il n’y a pas la variable \\(l\\) dans le modèle, donc \\(\\beta_{l}=0\\) et le paramètre de décentrage vaut \\(0\\). Le seuil de rejet basé sur la statistique \\(N\\) est son quantile de niveau 0.95 sous \\({\\mathrm{H_0}}\\)’ c’est à dire celui d’un \\(\\chi^{2}(1)\\) à 0.95 :\n\nfrom scipy.stats import chi2\nprint(chi2.ppf(0.95, df=1))\n\n3.841458820694124\n\n\nPar définition le \\(\\mathop{\\mathrm{R^2_a}}\\) vaut \\[\n\\mathop{\\mathrm{R^2_a}}(\\xi) =1-\\frac{n-1}{\\mathop{\\mathrm{SCT}}}\\frac{\\mathop{\\mathrm{SCR}}(\\xi)}{n-|\\xi|}\n\\] Calculons la différence des \\(\\mathop{\\mathrm{R^2_a}}\\) \\[\n\\Delta\\mathop{\\mathrm{R^2_a}}= \\frac{n-1}{\\mathop{\\mathrm{SCT}}}(\\frac{\\mathop{\\mathrm{SCR}}(\\xi)}{n-|\\xi|} - \\frac{\\mathop{\\mathrm{SCR}}(\\{\\xi, l\\})}{n-|\\xi| -1})\n\\] Cette différence est positive (on ajoute la variable \\(l\\)) si \\[\n\\begin{align*}\n\\frac{\\mathop{\\mathrm{SCR}}(\\xi)}{n-|\\xi|} - \\frac{\\mathop{\\mathrm{SCR}}(\\{\\xi, l\\})}{n-|\\xi| -1}&&gt;0\\\\\n\\frac{\\mathop{\\mathrm{SCR}}(\\xi)}{n-|\\xi|}- \\frac{\\mathop{\\mathrm{SCR}}(\\xi) -\\hat\\beta^{2}_{l} }{n-|\\xi| -1}&&gt;0\\\\\n\\frac{\\mathop{\\mathrm{SCR}}(\\xi)}{n-|\\xi|} - \\frac{\\mathop{\\mathrm{SCR}}(\\xi)}{n-|\\xi|}.\\frac{n-|\\xi|}{n-|\\xi|-1}+ \\frac{\\hat\\beta^{2}_{l}}{n-|\\xi| -1} &&gt;0\\\\\n\\frac{\\mathop{\\mathrm{SCR}}(\\xi)}{n-|\\xi|}\\frac{-1}{n-|\\xi| -1}+ \\frac{\\hat\\beta^{2}_{l}}{n-|\\xi| -1}&&gt;0\n\\end{align*}\n\\] En utilisant l’approximation on a donc approximativement \\[\n\\begin{align*}\n   \\frac{1}{n-|\\xi| -1}(\\hat\\beta^{2}_{l}- \\sigma^{2})  &&gt;0\\\\\n  \\frac{\\sigma^{2}}{n-|\\xi| -1} (\\frac{\\hat\\beta^{2}_{l}}{\\sigma^{2}}- 1)&&gt;0\\\\\n\\end{align*}\n\\] Le premier terme étant positif on retrouve que \\[\n\\begin{align*}\n\\frac{\\hat\\beta^{2}_{l}}{\\sigma^{2}}- 1&&gt;0\\\\\n\\frac{\\hat\\beta^{2}_{l}}{\\sigma^{2}}&&gt;1.\n\\end{align*}\n\\]\nComme \\[\n\\mathop{\\mathrm{C_p}}(\\xi)=\\frac{\\mathop{\\mathrm{SCR}}(\\xi)}{\\sigma^2}-n+2|\\xi|.\n\\] on fait là encore la différence des \\(\\mathop{\\mathrm{C_p}}\\) \\[\n\\begin{align*}\n\\Delta\\mathop{\\mathrm{C_p}}&= \\frac{\\mathop{\\mathrm{SCR}}(\\{\\xi, l\\})-\\mathop{\\mathrm{SCR}}(\\xi)}{\\sigma^2}+2 =\n\\frac{\\mathop{\\mathrm{SCR}}(\\xi) -\\hat\\beta^{2}_{l} -\\mathop{\\mathrm{SCR}}(\\xi)}{\\sigma^2} +2\\\\\n&=-\\frac{\\hat\\beta^{2}_{l}}{\\sigma^2} +2\n\\end{align*}\n\\] Cette différence est négative (on ajoute la variable \\(l\\)) si \\[\n-\\frac{\\hat\\beta^{2}_{l}}{\\sigma^2} +2&lt;0 \\quad\\Longleftrightarrow\\quad\n\\frac{\\hat\\beta^{2}_{l}}{\\sigma^2}&gt;2.\n\\]\nD’après la question 3 nous n’avons besoin d’estimer qu’une fois le modèle complet et les coordonnées donnent les estimateurs dans les modèles restreints.\nAlgorithme :\n\nEstimer \\(\\beta\\) dans le modèle complet \\(\\hat \\beta = X'Y\\) .\nDéduire \\(\\hat\\sigma^{2}\\) dans le modèle complet \\(\\hat\\sigma^{2}=\\|Y - X\\hat \\beta\\|^{2}/(n-p)\\) .\nOrdonner les coordonnées dans l’ordre décroissant: \\[\n\\hat \\beta_{{(1)}} \\geq \\hat \\beta_{{(2)}}\\geq \\cdots \\geq \\hat \\beta_{{(p)}}.\n\\] Les colonnes correspondantes seront notées \\((k)\\).\nPour \\(k=1\\) à \\(p\\)\n\nSi \\(\\frac{\\hat\\beta^{2}_{(k)}}{\\hat\\sigma^2}\\) &gt; Seuil alors ajout la variable \\((k)\\)\nSinon Sortie de la boucle, plus de variable à ajouter\n\nLe seuil vaut \\(2f(n)\\) pour l’AIC et le BIC, il vaut 3.84 pour un test, 1 pour le \\(\\mathop{\\mathrm{R^2_a}}\\) et 2 pour le \\(\\mathop{\\mathrm{C_p}}\\).\n\n\n\n\n\n\n Retour au sommet",
    "crumbs": [
      "Corrections",
      "III Réduction de dimension",
      "8 Choix de variables"
    ]
  },
  {
    "objectID": "codes/chap2.html",
    "href": "codes/chap2.html",
    "title": "2 La régression linéaire multiple",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n\nLa concentration en ozone\n\nozone = pd.read_csv(\"../donnees/ozone.txt\", header=0, sep=\";\")\n\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.scatter(ozone.T12, ozone.Vx,ozone.O3)\nax.set_xlabel('T12') ; ax.set_ylabel('Vx') ; ax.set_zlabel('O3')\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nreg = smf.ols('O3 ~ T12+Vx', data=ozone).fit()\nreg.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nO3\nR-squared:\n0.525\n\n\nModel:\nOLS\nAdj. R-squared:\n0.505\n\n\nMethod:\nLeast Squares\nF-statistic:\n25.96\n\n\nDate:\nFri, 31 Jan 2025\nProb (F-statistic):\n2.54e-08\n\n\nTime:\n17:30:04\nLog-Likelihood:\n-210.53\n\n\nNo. Observations:\n50\nAIC:\n427.1\n\n\nDf Residuals:\n47\nBIC:\n432.8\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n35.4530\n10.745\n3.300\n0.002\n13.838\n57.068\n\n\nT12\n2.5380\n0.515\n4.927\n0.000\n1.502\n3.574\n\n\nVx\n0.8736\n0.177\n4.931\n0.000\n0.517\n1.230\n\n\n\n\n\n\n\n\nOmnibus:\n0.280\nDurbin-Watson:\n1.678\n\n\nProb(Omnibus):\n0.869\nJarque-Bera (JB):\n0.331\n\n\nSkew:\n0.165\nProb(JB):\n0.848\n\n\nKurtosis:\n2.777\nCond. No.\n94.4\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\nLa hauteur des eucalyptus\n\neucalyptus = pd.read_csv(\"../donnees/eucalyptus.txt\",header=0,sep=\";\")\nfig = plt.figure()\nplt.plot(eucalyptus.circ, eucalyptus.ht, '+k')\nplt.ylabel('ht') ; plt.xlabel('circ')\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nreg = smf.ols('ht ~ circ+np.sqrt(circ)', data=eucalyptus).fit()\nreg.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nht\nR-squared:\n0.792\n\n\nModel:\nOLS\nAdj. R-squared:\n0.792\n\n\nMethod:\nLeast Squares\nF-statistic:\n2718.\n\n\nDate:\nFri, 31 Jan 2025\nProb (F-statistic):\n0.00\n\n\nTime:\n17:30:04\nLog-Likelihood:\n-2208.5\n\n\nNo. Observations:\n1429\nAIC:\n4423.\n\n\nDf Residuals:\n1426\nBIC:\n4439.\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n-24.3520\n2.614\n-9.314\n0.000\n-29.481\n-19.223\n\n\ncirc\n-0.4829\n0.058\n-8.336\n0.000\n-0.597\n-0.369\n\n\nnp.sqrt(circ)\n9.9869\n0.780\n12.798\n0.000\n8.456\n11.518\n\n\n\n\n\n\n\n\nOmnibus:\n3.015\nDurbin-Watson:\n0.947\n\n\nProb(Omnibus):\n0.221\nJarque-Bera (JB):\n2.897\n\n\nSkew:\n-0.097\nProb(JB):\n0.235\n\n\nKurtosis:\n3.103\nCond. No.\n4.41e+03\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 4.41e+03. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\n\ngrille = pd.DataFrame({'circ' : np.linspace(eucalyptus.circ.min(), \\\n                            eucalyptus.circ.max(),100)})\ncalculprev = reg.get_prediction(grille)\nprev = calculprev.predicted_mean\n\nfig = plt.figure()\nplt.plot(eucalyptus.circ, eucalyptus.ht, '+k')\nplt.ylabel('ht') ; plt.xlabel('circ')\nplt.plot(grille.circ, prev, '-', lw=1)\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\n Retour au sommet",
    "crumbs": [
      "Codes",
      "I Introduction au modèle linéaire",
      "2 La régression linéaire multiple"
    ]
  },
  {
    "objectID": "codes/chap8.html",
    "href": "codes/chap8.html",
    "title": "8 Choix de variables",
    "section": "",
    "text": "import pandas as pd; import numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\n\nimport sys\nsys.path.append('../modules')  # Ajoute le répertoire 'modules' au chemin de recherche des modules\nimport choixolsstats\n\n\nIntroduction\n\n\nNotations\n\n\nChoix incorrects des variables\n\n\nCritères classiques de choix de modèles\n\n\nProcédure de sélection\n\n\nExemple : la concentration en ozone\n\nozone = pd.read_csv(\"../donnees/ozone.txt\", header = 0, sep = \";\")\nozone[\"vent\"] = ozone[\"vent\"].astype(\"category\")\n\n\nupper = \"O3 ~ T12 + T15 + Ne12 + N12 + S12 + E12 + W12 + Vx + O3v\"\nmustbe = \"1\"\nmodcomplet = smf.ols(upper + \"+\" + mustbe, data=ozone).fit()\nmodsel = choixolsstats.bestols(ozone, upper=upper, mustbe=mustbe)\nmodsel['Cp'] = modsel['SSR']/modcomplet.mse_resid - modcomplet.nobs + 2 *  modsel['nb_var']\n\n\nmodsel.sort_values(by=[\"BIC\",\"nb_var\"]).iloc[:4,[1,3]]\n\n\n\n\n\n\n\n\nvar_added\nBIC\n\n\n\n\n366\n(T15, O3v, Ne12, Vx)\n411.974567\n\n\n381\n(T12, O3v, Ne12, Vx)\n412.085048\n\n\n443\n(T15, O3v, Ne12)\n412.146450\n\n\n499\n(O3v, Ne12)\n412.310100\n\n\n\n\n\n\n\n\nmodsel.sort_values(by=[\"AIC\",\"nb_var\"]).iloc[:4,[1,2]]\n\n\n\n\n\n\n\n\nvar_added\nAIC\n\n\n\n\n366\n(T15, O3v, Ne12, Vx)\n402.414452\n\n\n381\n(T12, O3v, Ne12, Vx)\n402.524933\n\n\n219\n(S12, T15, O3v, Ne12, Vx)\n403.238253\n\n\n234\n(S12, T12, O3v, Ne12, Vx)\n403.563498\n\n\n\n\n\n\n\n\nmodsel.sort_values(by=[\"Cp\",\"nb_var\"]).iloc[:5,[1,7]]\n\n\n\n\n\n\n\n\nvar_added\nCp\n\n\n\n\n366\n(T15, O3v, Ne12, Vx)\n2.947034\n\n\n381\n(T12, O3v, Ne12, Vx)\n3.042036\n\n\n219\n(S12, T15, O3v, Ne12, Vx)\n3.948540\n\n\n234\n(S12, T12, O3v, Ne12, Vx)\n4.222300\n\n\n354\n(T15, E12, O3v, Ne12)\n4.264315\n\n\n\n\n\n\n\n\nmodsel.sort_values(by=[\"R2adj\",\"nb_var\"],ascending=False). \\\n  iloc[:4,[1,6]]\n\n\n\n\n\n\n\n\nvar_added\nR2adj\n\n\n\n\n115\n(S12, T15, W12, O3v, Ne12, Vx)\n0.709059\n\n\n219\n(S12, T15, O3v, Ne12, Vx)\n0.708613\n\n\n366\n(T15, O3v, Ne12, Vx)\n0.708307\n\n\n23\n(N12, S12, T15, W12, O3v, Ne12, Vx)\n0.707954\n\n\n\n\n\n\n\n\nmodsel.sort_values(by=[\"R2\",\"nb_var\"], ascending=False). \\\n    iloc[:4,[1,5]]\n\n\n\n\n\n\n\n\nvar_added\nR2\n\n\n\n\n0\n(N12, S12, T15, E12, W12, T12, O3v, Ne12, Vx)\n0.750501\n\n\n4\n(N12, S12, T15, E12, W12, O3v, Ne12, Vx)\n0.750189\n\n\n6\n(N12, S12, T15, W12, T12, O3v, Ne12, Vx)\n0.750040\n\n\n23\n(N12, S12, T15, W12, O3v, Ne12, Vx)\n0.749675\n\n\n\n\n\n\n\n\nselec = modsel.sort_values(by=[\"BIC\",\"nb_var\"]).iloc[:1,[1,3]]\nformule = \"O3 ~ 1 +\" + \"+\".join(selec.iloc[0,0])\nmodsel = smf.ols(formule, data=ozone).fit()\n\n\n\n\n\n Retour au sommet",
    "crumbs": [
      "Codes",
      "III Réduction de dimension",
      "8 Choix de variables"
    ]
  },
  {
    "objectID": "correction/chap3.html",
    "href": "correction/chap3.html",
    "title": "3 Validation du modèle",
    "section": "",
    "text": "Exercice 1 (Questions de cours) C si \\(\\mathbf{1}\\) fait partie des variables ou si \\(\\mathbf{1} \\in \\Im(X)\\), A, C, C, A.\n\n\nExercice 2 (Propriétés d’une matrice de projection) La trace d’un projecteur vaut la dimension de l’espace sur lequel s’effectue la projection, donc \\(\\text{tr}(P_X)=p\\). Le second point découle de la propriété \\(P^2=P\\).\nLes matrices \\(P_X\\) et \\(P_XP_X\\) sont égales, nous avons que \\((P_X)_{ii}\\) vaut \\((P_XP_X)_{ii}\\). Cela s’écrit \\[\\begin{eqnarray*}\nh_{ii} &=&  \\sum_{k=1}^n h_{ik} h_{ki}\\\\\n&=& h_{ii}^2 + \\sum_{k=1, k \\neq i}^n h_{ik}^2\\\\\nh_{ii}(1-h_{ii}) &=& \\sum_{k=1, k \\neq i}^n h_{ik}^2.\n\\end{eqnarray*}\\] La dernière quantité de droite de l’égalité est positive et donc le troisième point est démontré. En nous servant de cet écriture les deux derniers points sont aussi démontrés.\nNous pouvons écrire \\[\\begin{eqnarray*}\nh_{ii}(1-h_{ii}) &=& h_{ij}^2 + \\sum_{k=1, k \\neq i ,j }^n h_{ik}^2.\n\\end{eqnarray*}\\] La quantité de gauche est maximum lorsque \\(h_{ii}=0.5\\) et vaut alors \\(0.25\\). Le quatrième point est démontré.\n\n\nExercice 3 (Lemme d’invertion matricielle) Commençons par effectuer les calculs en notant que la quantité \\(u'M^{-1}v\\) est un scalaire que nous noterons \\(k\\). Nous avons \\[\\begin{eqnarray*}\n\\left(M+uv'\\right)\\left(M^{-1}-\\frac{M^{-1}uv'M^{-1}}{1+u'M^{-1}v}\\right)\n&=&MM^{-1}-\\frac{MM^{-1}uv'M^{-1}}{1+k}+uv'M^{-1}\n-\\frac{uv'M^{-1}uv'M^{-1}}{1+k}\\\\\n&=&I+\\frac{-uv'M^{-1}+uv'M^{-1}+kuv'M^{-1}-ukv'M^{-1}}{1+k}.\n\\end{eqnarray*}\\] Le résultat est démontré.\n\n\nExercice 4 (Résidus studentisés)  \n\nIl suffit d’utiliser la définition du produit matriciel et de la somme matricielle et d’identifier les 2 membres des égalités.\nEn utilisant maintenant l’égalité de l’exercice précédent sur les inverses, avec \\(u=-x_i\\) et \\(v=x_i'\\), nous avons \\[\\begin{eqnarray*}\n(X'_{(i)}X_{(i)})^{-1}=(X'X-x_{i}x_{i}')^{-1}=(X'X)^{-1}+\n\\frac{(X'X)^{-1}x_{i}x_{i}'(X'X)^{-1}}\n{1-x_{i}'(X'X)^{-1}x_{i}}.%\\label{eq:hiieta}\n\\end{eqnarray*}\\] La définition de \\(h_{ii}=x_{i}'(X'X)^{-1}x_{i}\\) donne le résultat.\nCalculons la prévision où \\(\\hat \\beta_{(i)}\\) est l’estimateur de \\(\\beta\\) obtenu sans la \\(i^e\\) observation\n\n\\[\\begin{eqnarray*}\n\\hat y_i^p\n= x_{i}'\\hat \\beta_{(i)}\n&=& x_{i}' (X'_{(i)}X_{(i)})^{-1}X'_{(i)}Y_{(i)}\\\\\n&=& x_{i}'\\left[(X'X)^{-1}\n+ \\frac{(X'X)^{-1}x_{i}x_{i}'(X'X)^{-1}}{1-h_{ii}}\n\\right]\\left(X'Y-x_{i}'y_i\\right)\\\\\n&=& x_{i}' \\hat \\beta + \\frac{h_{ii}}{1-h_{ii}}x_{i}' \\hat \\beta\n- h_{ii}y_i -\\frac{h_{ii}^2}{1-h_{ii}}y_i\\\\\n&=& \\frac{1}{1-h_{ii}}\\hat y_i - \\frac{h_{ii}}{1-h_{ii}}y_i.\n\\end{eqnarray*}\\]\n\nCe dernier résultat donne \\[\\begin{eqnarray*}\n\\hat \\varepsilon_i = (1-h_{ii})(y_i-\\hat y^p_i).\n\\end{eqnarray*}\\] Nous avons alors \\[\\begin{eqnarray*}\nt^*_i &=& \\frac{\\hat \\varepsilon_i}{\\hat \\sigma_{(i)}\\sqrt{1-h_{ii}}}\\\\\n&=&\\frac{\\sqrt{(1-h_{ii})}(y_i - \\hat y_i^p)}{\\hat \\sigma_{(i)}}.\n\\end{eqnarray*}\\] Pour terminer, remarquons qu’en multipliant l’égalité de la question 3 à gauche par \\(x_{i}'\\) et à droite par \\(x_{i}\\) \\[\\begin{eqnarray*}\nx_{i}'(X'_{(i)}X_{(i)})^{-1}x_{i}\n&=& h_{ii}+ \\frac{h_{ii}^2}{1-h_{ii}}.\\\\\n1+x_{i}'(X'_{(i)}X_{(i)})^{-1}x_{i}\n&=& 1 +\\frac{h_{ii}}{1-h_{ii}}=\\frac{h_{ii}}{1-h_{ii}}.\n\\end{eqnarray*}\\]\nUtilisons l’expression \\[\\begin{eqnarray*}\nt^*_i=\\frac{y_i-\\hat y_i^p }\n{\\hat \\sigma_{(i)}\\sqrt{1+x_{i}'(X'_{(i)}X_{(i)})^{-1}x_{i}}}.\n\\end{eqnarray*}\\] Nous pouvons alors appliquer la preuve de la proposition 5.4 page 97, en constatant que la \\(i^e\\) observation est une nouvelle observation. Nous avons donc \\(n-1\\) observations pour estimer les paramètres, cela donne donc un Student à \\(n-1-p\\) paramètres.\n\n\n\nExercice 5 (Distance de Cook)  \n\nNous reprenons une partie des calculs de l’exercice précédent : \\[\\begin{eqnarray*}\n\\hat \\beta_{(i)} &=& (X'_{(i)}X_{(i)})^{-1}X'_{(i)}Y_{(i)}\\\\\n&=& (X'X)^{-1}[X'Y-x_{i}y_i]+\\frac{1}{1-h_{ii}}\n(X'X)^{-1}x_{i}x_{i}'(X'X)^{-1}[X'Y-x_{i}y_i]\\\\\n&=& \\hat \\beta - (X'X)^{-1}x_{i}y_i + \\frac{1}{1-h_{ii}}\n(X'X)^{-1}x_{i}x_{i}'\\hat \\beta - \\frac{h_{ii}}{1-h_{ii}}\n(X'X)^{-1}x_{i}y_i,\n\\end{eqnarray*}\\] d’où le résultat.\nPour obtenir la seconde écriture de la distance de Cook, nous écrivons d’abord que \\[\\begin{eqnarray*}\n\\hat \\beta_{(i)} - \\hat \\beta = \\frac{- \\hat \\varepsilon_i}{1-h_{ii}}\n(X'X)^{-1}x_{i}.\n\\end{eqnarray*}\\] Puis nous développons \\[\\begin{eqnarray*}\nC_i &=& \\frac{1}{p \\hat \\sigma^2}(\\hat \\beta_{[i]}-\\hat \\beta)'\nX'X(\\hat \\beta_{(i)}-\\hat \\beta)\\\\\n&=& \\frac{1}{p \\hat \\sigma^2} \\left(\\frac{- \\hat \\varepsilon_i}{1-h_{ii}}\\right)^2 x_{i}' (X'X)^{-1}(X'X)(X'X)^{-1}x_{i}.\n\\end{eqnarray*}\\] Le résultat est démontré.\n\n\n\nExercice 6 (Régression partielle) Nous avons le modèle suivant : \\[\\begin{eqnarray*}\nP_{X_{\\bar{j}}^\\perp} Y&=&\\beta_jP_{X_{\\bar{j}}^\\perp} X_j + \\eta.\n\\end{eqnarray*}\\] L’estimateur des moindres carrés \\(\\tilde\\beta_j\\) issu de ce modèle vaut \\[\\begin{eqnarray*}\n\\tilde \\beta_j = (X'_j P_{X_{\\bar{j}}^\\perp} X_j)^{-1}\nX'_j P_{X_{\\bar{j}}^\\perp} Y.\n\\end{eqnarray*}\\] La projection de \\(Y\\) sur \\(\\Im(X_{\\bar{j}})\\) (i.e. la prévision par le modèle sans la variable \\(X_j\\)) peut s’écrire comme la projection \\(Y\\) sur \\(\\Im(X)\\) qui est ensuite projetée sur \\(\\Im(X_{\\bar{j}})\\), puisque \\(\\Im(X_{\\bar{j}})\\subset \\Im(X)\\). Ceci s’écrit \\[\\begin{eqnarray*}\nP_{X_{\\bar{j}}}Y&=&P_{X_{\\bar{j}}}P_{X}Y=P_{X_{\\bar{j}}}X\\hat{\\beta}\n=P_{X_{\\bar{j}}}(X_{\\bar{j}}\\hat\\beta_{\\bar{j}}+\\hat\\beta_jX_j)\n=X_{\\bar{j}}\\hat\\beta_{\\bar{j}}+\\hat\\beta_jP_{X_{\\bar{j}}}X_j,\n\\end{eqnarray*}\\] et donc \\[\\begin{eqnarray*}\nX_{\\bar{j}}\\hat\\beta_{\\bar{j}} = P_{X_{\\bar{j}}} Y -\n\\hat\\beta_jP_{X_{\\bar{j}}}X_j.\n\\end{eqnarray*}\\] Récrivons les résidus \\[\\begin{eqnarray*}\n\\hat{\\varepsilon}&=&P_{X^\\perp} Y=Y-X\\hat\\beta\n=Y-X_{\\bar{j}}\\hat\\beta_{\\bar{j}}-\\hat\\beta_jX_j\\nonumber\\\\\n&=&Y-P_{X_{\\bar{j}}}Y + \\hat\\beta_jP_{X_{\\bar{j}}}X_j  -\\hat\\beta_j X_j\\nonumber\\\\\n&=&(I-P_{X_{\\bar{j}}})Y - \\hat\\beta_j(I-P_{X_{\\bar{j}}})X_j\\nonumber\\\\\n&=&P_{X_{\\bar{j}}^\\perp} Y-\\hat\\beta_jP_{X_{\\bar{j}}^\\perp} X_j.%\\label{eq:origine:residpartiel}\n\\end{eqnarray*}\\] En réordonnant cette dernière égalité, nous pouvons écrire \\[\\begin{eqnarray}\nP_{X_{\\bar{j}}^\\perp} Y&=&\\hat\\beta_jP_{X_{\\bar{j}}^\\perp} X_j+\\hat{\\varepsilon}.\\nonumber\n\\end{eqnarray}\\] Nous avons alors \\[\\begin{eqnarray*}\n\\tilde\\beta_j &=& (X'_j P_{X_{\\bar{j}}^\\perp} X_j)^{-1}\nX'_j P_{X_{\\bar{j}}^\\perp} Y\\\\\n&=& (X'_j P_{X_{\\bar{j}}^\\perp} X_j)^{-1}\nX'_j(\\hat\\beta_jP_{X_{\\bar{j}}^\\perp} X_j+\\hat{\\varepsilon})\\\\\n&=& \\hat\\beta_j +(X'_j P_{X_{\\bar{j}}^\\perp} X_j)^{-1} X'_j\\hat{\\varepsilon}).\n\\end{eqnarray*}\\] Le produit scalaire \\(X'_j\\hat{\\varepsilon} = \\langle X_j,\\hat{\\varepsilon} \\rangle\\) est nul car les deux vecteurs appartiennent à des sous-espaces orthogonaux, d’où le résultat.\n\n\nExercice 7 (TP : Résidus partiels)  \n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom patsy import dmatrices\n\n\nImportation\n\ndon = pd.read_csv(\"../donnees/tprespartiel.dta\", sep=\";\")\n\nEstimation du modèle\n\nX = don[['X1', 'X2', 'X3', 'X4']]\nX = sm.add_constant(X)\nY = don['Y']\nmod = sm.OLS(Y, X).fit()\n\nAnalyse des résidus partiels. Commençons par visualiser les résidus partiels :\n\nfig = sm.graphics.plot_ccpr_grid(mod)\nplt.show()\n\n\n\n\n\n\n\n\nLes 3 premières variables montrent des tendances linéaires (ou aucune pour la troisième) alors que la troisième semble montrer plutôt une tendance quadratique.\nRefaisons le modèle avec X5 :\n\ndon['X5'] = don['X4'] ** 2\nX2 = don[['X1', 'X2', 'X3', 'X5']]\nX2 = sm.add_constant(X2)\nmod2 = sm.OLS(Y, X2).fit()\nfig2 = sm.graphics.plot_ccpr_grid(mod2)\nplt.show()\n\n\n\n\n\n\n\n\net nous constatons que les résidus partiels sont tous à tendance linéaire. Les 2 modèles ayant le même nombre de variables nous pouvons les comparer via leur \\(\\mathop{\\mathrm{R^2}}\\) qui valent\n\nprint(mod.rsquared)\nprint(mod2.rsquared)\n\n0.9860422309885765\n0.9966109930897685\n\n\nLa seconde modélisation est la meilleure tant pour la qualité globale que pour l’analyse des résidus.\nAvec le second jeu de données\n\ndonbis = pd.read_csv(\"../donnees/tpbisrespartiel.dta\", sep=\";\")\nX = donbis[['X1', 'X2', 'X3', 'X4']]\nX = sm.add_constant(X)\nY = donbis['Y']\nmod3 = sm.OLS(Y, X).fit()\nfig = sm.graphics.plot_ccpr_grid(mod3)\nplt.show()\n\n\n\n\n\n\n\n\nNous voyons clairement une sinusoïde de type \\(\\sin(-2\\pi X_4)\\) sur le dernier graphique. Changeons X4\n\ndonbis['X5'] = np.sin(-2*np.pi*donbis['X4'])\nX2 = donbis[['X1', 'X2', 'X3', 'X5']]\nX2 = sm.add_constant(X2)\nmod4 = sm.OLS(Y, X2).fit()\nfig2 = sm.graphics.plot_ccpr_grid(mod4)\nplt.show()\n\n\n\n\n\n\n\n\n\nprint(mod3.rsquared)\nprint(mod4.rsquared)\n\n0.8106256841560806\n0.9984664330652744\n\n\n\n\n\n\n\n Retour au sommet",
    "crumbs": [
      "Corrections",
      "I Introduction au modèle linéaire",
      "3 Validation du modèle"
    ]
  },
  {
    "objectID": "codes/chap6.html",
    "href": "codes/chap6.html",
    "title": "Inférence dans le modèle gaussien",
    "section": "",
    "text": "import numpy as np; import pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\n\n\nozone = pd.read_csv(\"../donnees/ozone.txt\", header=0, sep=\";\")\nmod3 = smf.ols(\"O3 ~ T12+Vx+Ne12\", data=ozone).fit()\nprint(mod3.conf_int(alpha=0.05))\n\n                   0           1\nIntercept  57.158415  111.936250\nT12         0.313811    2.316281\nVx          0.149186    0.823705\nNe12       -6.960609   -2.826137\n\n\n\nmod3.summary()\nICparams=mod3.conf_int(alpha=0.05)\nmod3.params\n\nstats.t.ppf(0.95, mod3.df_resid)\n\nICparams=mod3.conf_int(alpha=0.025)\n\n\ndef ellipse(m, ij, alpha=0.05, npoints=500):\n    import numpy as np\n    from scipy.stats import f\n    hatSigma = m.cov_params().iloc[ij,ij]\n    valpr,vectpr = np.linalg.eig(hatSigma)\n    hatSigmademi = vectpr @ np.diag(np.sqrt(valpr))\n    theta = np.linspace(0, 2 * np.pi, npoints)\n    rho = (2 * f.isf(alpha, 2, m.nobs - 2))**0.5\n    XX = np.array([rho * np.cos(theta), rho * np.sin(theta)])\n    ZZ = np.add((hatSigmademi @ XX).T, m.params.iloc[ij].to_numpy())\n    return ZZ\n\nfig = plt.figure()\nfig, axs = plt.subplots(3, 2,figsize=(8, 8))\nic = 0\nil = 0\nfor i in range(0,3):\n    for j in range(i+1,4):\n        coord = ellipse(mod3, [i, j])\n        axs[il,ic].fill(coord[:,0], coord[:,1], fc='lightgrey', ec='k', lw=1)\n        axs[il,ic].plot(mod3.params.iloc[i], mod3.params.iloc[j], \"+\")\n        r = matplotlib.patches.Rectangle(ICparams.iloc[[i, j], 0], ICparams.diff(axis=1).iloc[i, 1], ICparams.diff(axis=1).iloc[j, 1], fill=False)\n        axs[il,ic].add_artist(r)\n        if ic==0:\n            ic = 1\n        else:\n            ic = 0\n            il += 1\n\n\n            \nfig.tight_layout()            \n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n#print(mod3.scale*mod3.df_resid/scipy.stats.chi2.ppf(0.975,mod3.df_resid))\n#print(mod3.scale*mod3.df_resid/scipy.stats.chi2.ppf(0.025,mod3.df_resid))\n\n\nExemple 1 : la concentration en ozone\n\nmod3 = smf.ols(\"O3 ~ T12 + Vx + Ne12\",data=ozone).fit()\nmod3.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nO3\nR-squared:\n0.682\n\n\nModel:\nOLS\nAdj. R-squared:\n0.661\n\n\nMethod:\nLeast Squares\nF-statistic:\n32.87\n\n\nDate:\nFri, 31 Jan 2025\nProb (F-statistic):\n1.66e-11\n\n\nTime:\n15:41:08\nLog-Likelihood:\n-200.50\n\n\nNo. Observations:\n50\nAIC:\n409.0\n\n\nDf Residuals:\n46\nBIC:\n416.7\n\n\nDf Model:\n3\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n84.5473\n13.607\n6.214\n0.000\n57.158\n111.936\n\n\nT12\n1.3150\n0.497\n2.644\n0.011\n0.314\n2.316\n\n\nVx\n0.4864\n0.168\n2.903\n0.006\n0.149\n0.824\n\n\nNe12\n-4.8934\n1.027\n-4.765\n0.000\n-6.961\n-2.826\n\n\n\n\n\n\n\n\nOmnibus:\n0.211\nDurbin-Watson:\n1.758\n\n\nProb(Omnibus):\n0.900\nJarque-Bera (JB):\n0.411\n\n\nSkew:\n-0.050\nProb(JB):\n0.814\n\n\nKurtosis:\n2.567\nCond. No.\n148.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nmod3.conf_int(alpha=0.05)\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\nIntercept\n57.158415\n111.936250\n\n\nT12\n0.313811\n2.316281\n\n\nVx\n0.149186\n0.823705\n\n\nNe12\n-6.960609\n-2.826137\n\n\n\n\n\n\n\n\nmod2 = smf.ols(\"O3 ~ T12 + Vx\",data=ozone).fit()\nround(sm.stats.anova_lm(mod2,mod3),3)\n\n\n\n\n\n\n\n\ndf_resid\nssr\ndf_diff\nss_diff\nF\nPr(&gt;F)\n\n\n\n\n0\n47.0\n13299.399\n0.0\nNaN\nNaN\nNaN\n\n\n1\n46.0\n8904.622\n1.0\n4394.777\n22.703\n0.0\n\n\n\n\n\n\n\n\n\nExemple 2 : la hauteur des eucalyptus\n\neucalyptus = pd.read_csv(\"../donnees/eucalyptus.txt\",header=0,sep=\";\")\nregS = smf.ols('ht~circ', data=eucalyptus).fit()\nregM = smf.ols('ht~circ +  np.sqrt(circ)', data=eucalyptus).fit()\nround(sm.stats.anova_lm(regS,regM),3)\n\n\n\n\n\n\n\n\ndf_resid\nssr\ndf_diff\nss_diff\nF\nPr(&gt;F)\n\n\n\n\n0\n1427.0\n2052.084\n0.0\nNaN\nNaN\nNaN\n\n\n1\n1426.0\n1840.656\n1.0\n211.428\n163.798\n0.0\n\n\n\n\n\n\n\n\nregM.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nht\nR-squared:\n0.792\n\n\nModel:\nOLS\nAdj. R-squared:\n0.792\n\n\nMethod:\nLeast Squares\nF-statistic:\n2718.\n\n\nDate:\nFri, 31 Jan 2025\nProb (F-statistic):\n0.00\n\n\nTime:\n15:41:08\nLog-Likelihood:\n-2208.5\n\n\nNo. Observations:\n1429\nAIC:\n4423.\n\n\nDf Residuals:\n1426\nBIC:\n4439.\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n-24.3520\n2.614\n-9.314\n0.000\n-29.481\n-19.223\n\n\ncirc\n-0.4829\n0.058\n-8.336\n0.000\n-0.597\n-0.369\n\n\nnp.sqrt(circ)\n9.9869\n0.780\n12.798\n0.000\n8.456\n11.518\n\n\n\n\n\n\n\n\nOmnibus:\n3.015\nDurbin-Watson:\n0.947\n\n\nProb(Omnibus):\n0.221\nJarque-Bera (JB):\n2.897\n\n\nSkew:\n-0.097\nProb(JB):\n0.235\n\n\nKurtosis:\n3.103\nCond. No.\n4.41e+03\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 4.41e+03. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\n\ngrille = pd.DataFrame({'circ' : np.linspace(eucalyptus[\"circ\"].min(),eucalyptus[\"circ\"].max(),100)})\ncalculprev = regM.get_prediction(grille)\nprev = calculprev.predicted_mean\nICdte = calculprev.conf_int(obs=False, alpha=0.05)\nICpre = calculprev.conf_int(obs=True, alpha=0.05)\nfig = plt.figure()\nplt.plot(eucalyptus[\"circ\"], eucalyptus[\"ht\"], '+k')\nplt.ylabel('ht') ; plt.xlabel('circ')\nplt.plot(grille['circ'],  prev, '-', label=\"E(Y)\", lw=1)\nlesic, = plt.plot(grille['circ'], ICdte[:,0], 'r--', label=r\"IC $\\mathbb{E}(Y)$\",lw=1)\nplt.plot(grille['circ'], ICdte[:, 1], 'r--', lw=1)\nlesic2, = plt.plot(grille['circ'], ICpre[:, 0], 'g:', label=r\"IC $Y$\",lw=1)\nplt.plot(grille['circ'], ICpre[:, 1],'g:', lw=1)\nplt.legend(handles=[lesic, lesic2], loc='lower right')\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\n\nIntervlle de confiance : boostrap\n\nmod3 = smf.ols(\"O3 ~ 1 + T12 + Vx + Ne12\", data = ozone).fit()\n\nmod3.params\nmod3.scale\nresidus3 = mod3.resid\nychap = mod3.fittedvalues\nn = residus3.shape[0]\n\nB = 1000\nCOEFF = np.zeros((B, 4))\nrng = np.random.default_rng(seed=1234)\nozoneetoile = ozone[[\"O3\", \"T12\" , \"Vx\",  \"Ne12\"]].copy()\nfor  b in range(B):\n    resetoile = residus3[rng.integers(n, size=n)]\n    O3etoile = np.add(ychap.values ,resetoile.values)\n    ozoneetoile.loc[:,\"O3\"] = O3etoile\n    regboot = smf.ols(\"O3 ~ 1+ T12 + Vx + Ne12\", data=ozoneetoile).fit()\n    COEFF[b] = regboot.params.values\n\n    \nprint(np.quantile(COEFF,[0.025, 0.975],axis=0))\n\nfig = plt.figure()\nn, bins, patches = plt.hist(COEFF[:,1], 30, density=True, facecolor='g', alpha=0.75)\nplt.xlabel(r'$\\hat \\beta_{2}^*$')\nfig.tight_layout()\n\n[[ 58.65115402   0.44272441   0.18933792  -6.84767267]\n [108.56181699   2.31524345   0.80828089  -2.7953567 ]]\n\n\n\n\n\n\n\n\n\n\n\n\n\n Retour au sommet",
    "crumbs": [
      "Codes",
      "II Inférence",
      "Inférence dans le modèle gaussien"
    ]
  },
  {
    "objectID": "codes/chap10.html",
    "href": "codes/chap10.html",
    "title": "10 Régression sur composantes : PCR et PLS",
    "section": "",
    "text": "import pandas as pd; import numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.formula.api as smf\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.cross_decomposition import PLSRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV, KFold\nfrom sklearn.metrics import mean_squared_error\nimport sys\nsys.path.append('../modules')\nfrom ols_step_sk import LinearRegressionSelectionFeatureIC\n\n\nRégression sur composantes principales (PCR)\n\nozone = pd.read_csv(\"../donnees/ozone.txt\", header = 0, sep = \";\", index_col=0).iloc[:,:10]\nX = ozone.iloc[:,1:10]\ny = ozone.iloc[:,:1]\nreg = smf.ols('O3 ~ T12 + T15 + Ne12 + N12 + S12 + E12 + W12 + Vx + O3v', data=ozone).fit()\nreg.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nO3\nR-squared:\n0.751\n\n\nModel:\nOLS\nAdj. R-squared:\n0.694\n\n\nMethod:\nLeast Squares\nF-statistic:\n13.37\n\n\nDate:\nFri, 31 Jan 2025\nProb (F-statistic):\n1.51e-09\n\n\nTime:\n17:30:32\nLog-Likelihood:\n-194.43\n\n\nNo. Observations:\n50\nAIC:\n408.9\n\n\nDf Residuals:\n40\nBIC:\n428.0\n\n\nDf Model:\n9\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n54.7278\n17.279\n3.167\n0.003\n19.806\n89.650\n\n\nT12\n-0.3518\n1.573\n-0.224\n0.824\n-3.531\n2.827\n\n\nT15\n1.4972\n1.538\n0.974\n0.336\n-1.611\n4.605\n\n\nNe12\n-4.1922\n1.064\n-3.941\n0.000\n-6.342\n-2.042\n\n\nN12\n1.2755\n1.363\n0.936\n0.355\n-1.480\n4.031\n\n\nS12\n3.1711\n1.911\n1.660\n0.105\n-0.691\n7.033\n\n\nE12\n0.5277\n1.943\n0.272\n0.787\n-3.399\n4.454\n\n\nW12\n2.4749\n2.072\n1.194\n0.239\n-1.713\n6.663\n\n\nVx\n0.6077\n0.486\n1.251\n0.218\n-0.374\n1.589\n\n\nO3v\n0.2454\n0.096\n2.543\n0.015\n0.050\n0.440\n\n\n\n\n\n\n\n\nOmnibus:\n0.252\nDurbin-Watson:\n1.887\n\n\nProb(Omnibus):\n0.881\nJarque-Bera (JB):\n0.364\n\n\nSkew:\n0.153\nProb(JB):\n0.833\n\n\nKurtosis:\n2.716\nCond. No.\n859.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nmod_lin9v = LinearRegression()\nmod_lin9v.fit(X,y)\nnp.round(mod_lin9v.coef_,2)\n\narray([[-0.35,  1.5 , -4.19,  1.28,  3.17,  0.53,  2.47,  0.61,  0.25]])\n\n\n\nreg_bic = LinearRegressionSelectionFeatureIC(crit=\"bic\")\nreg_bic.fit(X, y)\nX.columns[reg_bic.selected_features_]\n\nIndex(['T15', 'Ne12', 'Vx', 'O3v'], dtype='object')\n\n\n\ncr = StandardScaler()\nacp = PCA(n_components=9)\npipe_acp = Pipeline(steps=[(\"cr\", cr), (\"acp\", acp)])\npipe_acp.fit(X)\nXortho = pipe_acp.fit_transform(X)\nreg_bic.fit(Xortho, y)\nvarsel = reg_bic.selected_features_\nvarsel\n\n[0, 4]\n\n\n\nstep_cr = pipe_acp.named_steps[\"cr\"]\nstdX = step_cr.scale_\nmeanX = step_cr.mean_\ncoef_pcr =reg_bic.coef_\nstep_acp = pipe_acp.named_steps[\"acp\"]\nP = step_acp.components_[reg_bic.selected_features_,]\nbetafinpcr = P.T.dot(coef_pcr.T)[:,0]/stdX\nintercept = y.mean(axis=0) - meanX.dot(betafinpcr.T)\nnp.round(np.append(intercept, betafinpcr),2)\n\narray([ 5.279e+01,  4.100e-01,  4.700e-01, -2.540e+00, -8.700e-01,\n        2.000e-02,  1.060e+00, -8.500e-01,  2.300e-01,  3.400e-01])\n\n\n\ncr = StandardScaler()\nacp = PCA()\nreg = LinearRegression()\npipe_pcr = Pipeline(steps=[(\"cr\", cr), (\"acp\", acp), (\"reg\", reg)])\nparam_grid_pcr = { \"acp__n_components\" : list(range(1,10))}\nkf = KFold(n_splits=4, shuffle=True, random_state=0)\ncv_pcr = GridSearchCV(pipe_pcr, param_grid_pcr, cv=kf, scoring = 'neg_mean_squared_error', n_jobs=3).fit(X, y)\n\n\nimport matplotlib.cm as cm\ncolors = cm.Set1(range(1))\ncolors2 = colors\ncolors2[:,3] = colors2[:,3] -0.7\nfig, ax1 = plt.subplots(1, 1)\nax1.errorbar(list(range(1,10)), -cv_pcr.cv_results_[\"mean_test_score\"],\n             cv_pcr.cv_results_[\"std_test_score\"]/kf.n_splits**0.5 , capsize=10,  c=colors[0], ecolor=colors2[0])\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nbestpcr =  cv_pcr.best_estimator_\ncv_pcr.best_params_\nstep_reg = bestpcr.named_steps[\"reg\"]\ncoef_pcr = step_reg.coef_\nstep_cr =  bestpcr.named_steps[\"cr\"]\nstdX = step_cr.scale_\nmeanX = step_cr.mean_\nstep_acp = bestpcr.named_steps[\"acp\"]\nP = step_acp.components_\nbetafinpcr = P.T.dot(coef_pcr.T)[:,0]/stdX\nintercept = y.mean(axis=0) - meanX.dot(betafinpcr.T)\nnp.round(np.append(intercept, betafinpcr),2)\n\narray([58.33,  0.68,  0.44, -4.45,  1.24,  2.11,  1.84,  0.6 ,  0.15,\n        0.25])\n\n\n\n\nRégression aux moindres carrés partiels (PLS)\n\nregpls = PLSRegression()\nparam_grid_pls = { \"n_components\" : list(range(1,10))}\nkf = KFold(n_splits=4, shuffle=True, random_state=0)\ncv_pls = GridSearchCV(regpls, param_grid_pls, cv=kf, scoring = 'neg_mean_squared_error').fit(X, y)\n\n\ncolors = cm.Set1(range(1))\ncolors2 = colors\ncolors2[:,3] = colors2[:,3] -0.6\nfig, ax1 = plt.subplots(1, 1)\nax1.errorbar(list(range(1,10)), -cv_pls.cv_results_[\"mean_test_score\"],  cv_pls.cv_results_[\"std_test_score\"]/kf.n_splits**0.5 ,  capsize=10, c=colors[0], ecolor=colors2[0])\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nbestpls =  cv_pls.best_estimator_\ncv_pls.best_params_\nbestpls.fit(X,y)\nbestpls.coef_\n\narray([[ 0.41610174,  0.53217084, -4.4244714 ,  0.37042358,  1.60857405,\n         0.98429296,  0.56053927,  0.24800203,  0.26473814]])\n\n\n\nscalerX = StandardScaler().fit(X)\nstdX = scalerX.scale_\nmeanX = scalerX.mean_\nbetafinpls = bestpls.coef_[:,0]/stdX\nintercept = y.mean(axis=0) - meanX.dot(betafinpls.T)\nnp.append(intercept, betafinpls)\n\narray([7.95723726e+01, 8.99163083e-02, 8.43936432e-02, 1.65784166e-01,\n       1.98000066e-01, 2.80306490e-01, 1.88545955e-01, 1.71475028e-01,\n       3.09237815e-02, 1.68441318e-02])\n\n\n\n\n\n\n Retour au sommet",
    "crumbs": [
      "Codes",
      "III Réduction de dimension",
      "10 Régression sur composantes : PCR et PLS"
    ]
  },
  {
    "objectID": "codes/chap5.html",
    "href": "codes/chap5.html",
    "title": "5 Régression polynomiale et régression spline",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.interpolate import make_smoothing_spline\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\nfrom patsy import dmatrix \n\n\nRégression polynomiale\n\nozone = pd.read_csv(\"../donnees/ozone_simple.txt\", header=0, sep=\";\")\n\n\nfig = plt.figure()\nplt.plot(ozone.T12, ozone.O3, '.k')\nplt.ylabel('O3')\nplt.xlabel('T12')\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\ndef polyreg(donnee, d=3):\n    sigmax = donnee.T12.std()\n    mini = donnee.T12.min() -sigmax\n    maxi = donnee.T12.max()+sigmax\n    grillex = np.linspace(mini, maxi, 100)\n    aprevoir = pd.DataFrame({\"T12\": grillex})\n    formula = \"O3~1\"\n    for deg in range(d):\n        if deg&gt;0:\n            formula = formula + \"+ np.power(T12,\" + str(deg+1) + \")\"\n        else:\n            formula = formula + \"+ T12\"\n    repol = smf.ols(formula, donnee).fit()\n    prev = repol.predict(aprevoir)\n    return {\"grillex\": grillex, \"grilley\": prev}\n\n\nligne = ['-', '--', ':', '-.']\nfig, ax = plt.subplots(1, 1)\nfor iter, ii in enumerate([1,2,3,9]):\n    tmp = polyreg(ozone, d=ii)\n    ax.plot(tmp[\"grillex\"], tmp[\"grilley\"], ls=ligne[iter])\n\nax.set_xlabel(\"T12\")\nax.set_ylabel(\"O3\")\nax.set_xlim(0, 45)\nax.set_ylim(0, 150)\nax.legend(['d=1', 'd=2', 'd=3', 'd=9'])\nax.plot(ozone.T12, ozone.O3, \"k+\")\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\noz = ozone.copy()\noz.loc[oz.T12==7.9,\"T12\"] = 13\nligne = ['-', '--', ':', '-.']\nfig, (ax1, ax2) = plt.subplots(2, 1)\nfor iter, ii in enumerate([1,2,3,9]):\n    tmp = polyreg(ozone, d=ii)\n    ax1.plot(tmp[\"grillex\"], tmp[\"grilley\"], ls=ligne[iter])\n\nax1.set_ylabel(\"O3\")\nax1.set_xlim(5,35)\nax1.set_ylim(5, 150)\nax1.plot(ozone.T12, ozone.O3, \"k+\")\nfor iter, ii in enumerate([1,2,3,9]):\n    tmp = polyreg(oz, d=ii)\n    ax2.plot(tmp[\"grillex\"], tmp[\"grilley\"], ls=ligne[iter])\n\nax2.set_xlabel(\"T12\")\nax2.set_ylabel(\"O3\")\nax2.set_xlim(5,35)\nax2.set_ylim(5, 150)\nax2.plot(oz.T12, oz.O3, \"k+\")\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\noz = ozone.copy()\noz = oz.loc[oz.T12!=oz.T12.min(),:]\nligne = ['-', '--', ':', '-.']\nfig, (ax1, ax2) = plt.subplots(2, 1)\nfor iter, ii in enumerate([1,2,3,9]):\n    tmp = polyreg(ozone, d=ii)\n    ax1.plot(tmp[\"grillex\"], tmp[\"grilley\"], ls=ligne[iter])\n\nax1.set_ylabel(\"O3\")\nax1.set_xlim(5,35)\nax1.set_ylim(5, 150)\nax1.plot(ozone.T12, ozone.O3, \"k+\")\nfor iter, ii in enumerate([1,2,3,9]):\n    tmp = polyreg(oz, d=ii)\n    ax2.plot(tmp[\"grillex\"], tmp[\"grilley\"], ls=ligne[iter])\n\nax2.set_xlabel(\"T12\")\nax2.set_ylabel(\"O3\")\nax2.set_xlim(5,35)\nax2.set_ylim(5, 150)\nax2.plot(oz.T12, oz.O3, \"k+\")\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\n\nRégression spline\n\nind = ozone.T12 &lt; 23\nregd = smf.ols(\"O3~T12\", data=ozone.loc[ind,:]).fit()\nregf = smf.ols(\"O3~T12\", data=ozone.loc[~ind,:]).fit()\ngxd = np.linspace(3,23,21)\ngxf = np.linspace(23,30,8)\nprd = regd.params.iloc[0] +regd.params.iloc[1]*gxd\nprf = regf.params.iloc[0] +regf.params.iloc[1]*gxf\nfig, ax = plt.subplots(1, 1)\nax.plot(ozone.T12, ozone.O3, \"k+\", gxd, prd, \"k-\", gxf, prf, \"k-\")\nax.set_xlabel(\"T12\")\nax.set_ylabel(\"O3\")\nax.axvline(x=23, color=\"k\")\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nxi = [15, 23]\nBS = dmatrix(\"~ 1 + bs(T12, degree=2, knots=xi, include_intercept=False, lower_bound=5, upper_bound=32)\", ozone, return_type=\"dataframe\")\nBS.columns = [\"bs\" + str(i+1) for i in range(5)]\ndf = pd.concat([ozone.O3, BS.iloc[:,1:]], axis=1)\n\nregs = smf.ols(\"O3~1+bs2+bs3+bs4+bs5 \", df).fit()\nprint(regs.params.round(3))\n\nIntercept     51.102\nbs2           61.544\nbs3            5.562\nbs4           70.459\nbs5          106.712\ndtype: float64\n\n\n\nregs = smf.ols(\"O3~ 1 + bs(T12, degree=2, knots=xi, include_intercept=False, lower_bound=5, upper_bound=32)\", ozone).fit()\nprint(list(regs.params.round(3)))\n\n[51.102, 61.544, 5.562, 70.459, 106.712]\n\n\n\ndf = pd.DataFrame({\"T12\": np.linspace(5,32,100)})\nprev = regs.predict(df)\nfig, ax = plt.subplots(1, 1)\nax.plot(ozone.T12, ozone.O3, \"ko\" , df.T12, prev, \"-\")\nax.set_xlabel(\"T12\")\nax.set_ylabel(\"O3\")\nax.axvline(x=15, color=\"k\", ls=\"-\")\nax.axvline(x=xi[1], color=\"k\", ls=\"-\")\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nozdedup = ozone.groupby(\"T12\").mean()\nozdedup[\"w\"] = ozone.groupby(\"T12\").count()\nspl = make_smoothing_spline(ozdedup.index, ozdedup.O3, ozdedup.w, lam=10000)\nxi = np.linspace(ozdedup.index[0], ozdedup.index[-1], 150)\nyi = spl(xi)\nfig, ax = plt.subplots(1, 1)\nax.plot(ozone.T12, ozone.O3, \"ko\" , xi, yi, \"-\")\nax.set_xlabel(\"T12\")\nax.set_ylabel(\"O3\")\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nspl = make_smoothing_spline(ozdedup.index, ozdedup.O3, ozdedup.w, lam=1e-2)\nxi = np.linspace(ozdedup.index[0], ozdedup.index[-1], 150)\nyi = spl(xi)\nfig, ax = plt.subplots(1, 1)\nax.plot(ozone.T12, ozone.O3, \"ko\" , xi, yi, \"-\")\nax.set_xlabel(\"T12\")\nax.set_ylabel(\"O3\")\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nozdedup = ozone.groupby(\"T12\").mean()\nozdedup[\"w\"] = ozone.groupby(\"T12\").count()\nspl = make_smoothing_spline(ozdedup.index, ozdedup.O3, ozdedup.w)\nxi = np.linspace(ozdedup.index[0], ozdedup.index[-1], 150)\nyi = spl(xi)\nfig, ax = plt.subplots(1, 1)\nax.plot(ozone.T12, ozone.O3, \"ko\" , xi, yi, \"-\")\nax.set_xlabel(\"T12\")\nax.set_ylabel(\"O3\")\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\n Retour au sommet",
    "crumbs": [
      "Codes",
      "I Introduction au modèle linéaire",
      "5 Régression polynomiale et régression spline"
    ]
  },
  {
    "objectID": "codes/chap13.html",
    "href": "codes/chap13.html",
    "title": "13 Régression de Poisson",
    "section": "",
    "text": "import pandas as pd; import numpy as np\nimport matplotlib.pyplot as plt\nimport scipy as sp\nfrom scipy import stats\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\nimport statsmodels.regression.linear_model as smlm\nimport sys\nsys.path.append('../modules')\nimport choixglmstats\n\n\nLe modèle linéaire généralisé\n\n\nExemple : modélisation du nombre de visites\n\nMalaria = pd.read_csv(\"../donnees/poissonData3.csv\", header=0, sep=',')\nMalaria[\"Sexe\"] = Malaria[\"Sexe\"].astype(\"category\")\nMalaria[\"Prev\"] = Malaria[\"Prev\"].astype(\"category\")\nprint(Malaria.describe())\n\n               Age     Altitude        Duree     Nmalaria\ncount  1627.000000  1522.000000  1627.000000  1627.000000\nmean    419.359557  1294.714389   619.263061     4.687154\nstd     247.929838    44.198415   420.990175     4.153109\nmin      10.000000  1129.000000     0.000000     0.000000\n25%     220.000000  1266.000000   172.000000     1.000000\n50%     361.000000  1298.000000   721.000000     4.000000\n75%     555.000000  1320.000000  1011.000000     7.000000\nmax    1499.000000  1515.000000  1464.000000    26.000000\n\n\n\nprint(Malaria.isna().sum(axis=0))\n\nSexe          0\nAge           0\nAltitude    105\nPrev          0\nDuree         0\nNmalaria      0\ndtype: int64\n\n\n\nMalaria = Malaria[[\"Sexe\",\"Prev\",\"Duree\",\"Nmalaria\"]]\nplt.plot(Malaria.Duree, Malaria.Nmalaria, 'o')\n\n\n\n\n\n\n\n\n\nfig = plt.figure()\nmodele = smf.ols(\"Nmalaria ~ 1+ Duree\", data = Malaria).fit()\nplt.plot(Malaria.Duree, Malaria.Nmalaria, 'o')\ngrille = pd.DataFrame({'Duree': np.linspace(Malaria.Duree.min(), Malaria.Duree.max(), 2)})\ncalcprev = modele.get_prediction(grille)\nplt.plot(Malaria.Duree, Malaria.Nmalaria, 'o', grille.Duree,  calcprev.predicted_mean, '-')\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nmodP = smf.glm(\"Nmalaria ~ 1+ Duree\", data = Malaria, family=sm.families.Poisson()).fit()\nmodP.summary()\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\nNmalaria\nNo. Observations:\n1627\n\n\nModel:\nGLM\nDf Residuals:\n1625\n\n\nModel Family:\nPoisson\nDf Model:\n1\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-4060.3\n\n\nDate:\nTue, 04 Feb 2025\nDeviance:\n3325.2\n\n\nTime:\n14:27:57\nPearson chi2:\n3.17e+03\n\n\nNo. Iterations:\n5\nPseudo R-squ. (CS):\n0.7691\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n0.4295\n0.031\n13.853\n0.000\n0.369\n0.490\n\n\nDuree\n0.0015\n3.42e-05\n44.144\n0.000\n0.001\n0.002\n\n\n\n\n\n\nfig = plt.figure()\nplt.plot(Malaria.Duree, Malaria.Nmalaria, 'o')\ngrille2 = pd.DataFrame({'Duree': np.linspace(Malaria.Duree.min(), Malaria.Duree.max(), 1500)})\ncalcprev2 = modP.get_prediction(grille2)\nplt.plot(Malaria.Duree, Malaria.Nmalaria, 'o', grille.Duree,  calcprev.predicted_mean, '-', grille2.Duree,  calcprev2.predicted_mean, '--')\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\n\nRégression log-linéaire\n\nmodP3 = smf.glm(\"Nmalaria ~ 1+ Duree + Sexe + Prev\", data = Malaria, family=sm.families.Poisson()).fit()\nprint(modP3.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:               Nmalaria   No. Observations:                 1627\nModel:                            GLM   Df Residuals:                     1621\nModel Family:                 Poisson   Df Model:                            5\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -4056.3\nDate:                Tue, 04 Feb 2025   Deviance:                       3317.3\nTime:                        14:27:57   Pearson chi2:                 3.17e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.7703\nCovariance Type:            nonrobust                                         \n===========================================================================================\n                              coef    std err          z      P&gt;|z|      [0.025      0.975]\n-------------------------------------------------------------------------------------------\nIntercept                   0.1623      0.180      0.900      0.368      -0.191       0.516\nSexe[T.M]                   0.0551      0.023      2.398      0.016       0.010       0.100\nPrev[T.Moustiquaire]        0.2433      0.177      1.371      0.170      -0.105       0.591\nPrev[T.Rien]                0.2256      0.178      1.266      0.205      -0.124       0.575\nPrev[T.Serpentin/Spray]     0.2452      0.185      1.324      0.185      -0.118       0.608\nDuree                       0.0015   3.43e-05     44.031      0.000       0.001       0.002\n===========================================================================================\n\n\n\nmod3= smf.glm(\"Nmalaria ~ 1+ Duree + Sexe + C(Prev, Treatment('Rien'))\", data = Malaria, family=sm.families.Poisson()).fit()\nprint(mod3.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:               Nmalaria   No. Observations:                 1627\nModel:                            GLM   Df Residuals:                     1621\nModel Family:                 Poisson   Df Model:                            5\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -4056.3\nDate:                Tue, 04 Feb 2025   Deviance:                       3317.3\nTime:                        14:27:57   Pearson chi2:                 3.17e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.7703\nCovariance Type:            nonrobust                                         \n=================================================================================================================\n                                                    coef    std err          z      P&gt;|z|      [0.025      0.975]\n-----------------------------------------------------------------------------------------------------------------\nIntercept                                         0.3879      0.039      9.951      0.000       0.311       0.464\nSexe[T.M]                                         0.0551      0.023      2.398      0.016       0.010       0.100\nC(Prev, Treatment('Rien'))[T.Autre]              -0.2256      0.178     -1.266      0.205      -0.575       0.124\nC(Prev, Treatment('Rien'))[T.Moustiquaire]        0.0177      0.026      0.691      0.490      -0.032       0.068\nC(Prev, Treatment('Rien'))[T.Serpentin/Spray]     0.0196      0.059      0.333      0.739      -0.096       0.135\nDuree                                             0.0015   3.43e-05     44.031      0.000       0.001       0.002\n=================================================================================================================\n\n\n\nmodP2= smf.glm(\"Nmalaria ~ 1+ Duree + Sexe \", data = Malaria, family=sm.families.Poisson()).fit()\n\n\nsmlm.RegressionResults.compare_lr_test(modP3,modP2)\n\n(2.4488233421689074, 0.4846108842015914, 3)\n\n\n\nsp.stats.chi2.ppf(0.95,3)\n\n7.814727903251179\n\n\n\nprint(modP3.conf_int(alpha=0.05))\n\n                                0         1\nIntercept               -0.191170  0.515791\nSexe[T.M]                0.010071  0.100107\nPrev[T.Moustiquaire]    -0.104566  0.591101\nPrev[T.Rien]            -0.123561  0.574727\nPrev[T.Serpentin/Spray] -0.117722  0.608171\nDuree                    0.001443  0.001577\n\n\n\nMalaria = pd.read_csv(\"../donnees/poissonData.csv\", header=0, sep=\",\")\nMalaria.dropna(axis=0, inplace=True)\n\n\nform =\"Nmalaria ~ \" + \"+\".join(Malaria.columns[ :-1 ])\nmod_sel = choixglmstats.bestglm(Malaria, upper=form, family=sm.families.Poisson())\n\n\nprint(mod_sel.sort_values(by=[\"BIC\",\"nb_var\"]).iloc[:3,[1,3]])\n\n                       var_added          BIC\n13        (Altitude, Duree, Age)  7392.274194\n20             (Altitude, Duree)  7395.141592\n2   (Sexe, Altitude, Duree, Age)  7397.380825\n\n\n\nprint(mod_sel.sort_values(by=[\"AIC\",\"nb_var\"]).iloc[:3,[1,2]])\n\n                             var_added          AIC\n2         (Sexe, Altitude, Duree, Age)  7370.741923\n13              (Altitude, Duree, Age)  7370.963072\n0   (Sexe, Altitude, Duree, Prev, Age)  7374.648450\n\n\n\n\n\n\n Retour au sommet",
    "crumbs": [
      "Codes",
      "IV Le modèle linéaire généralisé",
      "13 Régression de Poisson"
    ]
  },
  {
    "objectID": "codes/chap9.html",
    "href": "codes/chap9.html",
    "title": "9 Régularisation des moindres carrés : ridge, lasso elastic net",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import Ridge, ElasticNet, Lasso\nfrom sklearn.linear_model import RidgeCV, ElasticNetCV, LassoCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import KFold\nfrom patsy import dmatrix\n\n\nIntroduction\n\n\nProblème du centrage-réduction\n\n\nPropriétés des estimateurs\n\n\nRégularisation avec le package scikitlearn\n\nozone = pd.read_csv(\"../donnees/ozone.txt\", header = 0, sep = \";\", index_col=0)\nX = ozone.iloc[:,1:10].to_numpy()\ny = ozone[\"O3\"].to_numpy()\n\n\ncr = StandardScaler()\nkf = KFold(n_splits=10, shuffle=True, random_state=0)\n\n\nlassocv = LassoCV(cv=kf)\nenetcv = ElasticNetCV(cv=kf)\npipe_lassocv = Pipeline(steps=[(\"cr\", cr), (\"lassocv\", lassocv)])\npipe_enetcv = Pipeline(steps=[(\"cr\", cr), (\"enetcv\", enetcv)])\n\n\npipe_lassocv.fit(X, y)\npipe_enetcv.fit(X, y)\n\nPipeline(steps=[('cr', StandardScaler()),\n                ('enetcv',\n                 ElasticNetCV(cv=KFold(n_splits=10, random_state=0, shuffle=True)))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('cr', StandardScaler()),\n                ('enetcv',\n                 ElasticNetCV(cv=KFold(n_splits=10, random_state=0, shuffle=True)))]) StandardScaler?Documentation for StandardScalerStandardScaler() ElasticNetCV?Documentation for ElasticNetCVElasticNetCV(cv=KFold(n_splits=10, random_state=0, shuffle=True)) \n\n\n\netape_lassocv = pipe_lassocv.named_steps[\"lassocv\"]\netape_enetcv = pipe_enetcv.named_steps[\"enetcv\"]\n\n\nalphasridge = etape_lassocv.alphas_ * 100\nridgecv = RidgeCV(cv=kf, alphas=alphasridge)\n\n\npipe_ridgecv = Pipeline(steps=[(\"cr\", cr), (\"ridgecv\", ridgecv)])\npipe_ridgecv.fit(X, y)\n\nPipeline(steps=[('cr', StandardScaler()),\n                ('ridgecv',\n                 RidgeCV(alphas=array([1.82214402e+03, 1.69933761e+03, 1.58480794e+03, 1.47799719e+03,\n       1.37838513e+03, 1.28548658e+03, 1.19884909e+03, 1.11805068e+03,\n       1.04269780e+03, 9.72423459e+02, 9.06885373e+02, 8.45764334e+02,\n       7.88762649e+02, 7.35602686e+02, 6.86025527e+02, 6.39789702e+02,\n       5.96670018e+02, 5.56456456e+02, 5.18953...\n       6.86025527e+00, 6.39789702e+00, 5.96670018e+00, 5.56456456e+00,\n       5.18953153e+00, 4.83977447e+00, 4.51358987e+00, 4.20938902e+00,\n       3.92569029e+00, 3.66111190e+00, 3.41436521e+00, 3.18424843e+00,\n       2.96964074e+00, 2.76949689e+00, 2.58284207e+00, 2.40876716e+00,\n       2.24642431e+00, 2.09502283e+00, 1.95382531e+00, 1.82214402e+00]),\n                         cv=KFold(n_splits=10, random_state=0, shuffle=True)))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('cr', StandardScaler()),\n                ('ridgecv',\n                 RidgeCV(alphas=array([1.82214402e+03, 1.69933761e+03, 1.58480794e+03, 1.47799719e+03,\n       1.37838513e+03, 1.28548658e+03, 1.19884909e+03, 1.11805068e+03,\n       1.04269780e+03, 9.72423459e+02, 9.06885373e+02, 8.45764334e+02,\n       7.88762649e+02, 7.35602686e+02, 6.86025527e+02, 6.39789702e+02,\n       5.96670018e+02, 5.56456456e+02, 5.18953...\n       6.86025527e+00, 6.39789702e+00, 5.96670018e+00, 5.56456456e+00,\n       5.18953153e+00, 4.83977447e+00, 4.51358987e+00, 4.20938902e+00,\n       3.92569029e+00, 3.66111190e+00, 3.41436521e+00, 3.18424843e+00,\n       2.96964074e+00, 2.76949689e+00, 2.58284207e+00, 2.40876716e+00,\n       2.24642431e+00, 2.09502283e+00, 1.95382531e+00, 1.82214402e+00]),\n                         cv=KFold(n_splits=10, random_state=0, shuffle=True)))]) StandardScaler?Documentation for StandardScalerStandardScaler() RidgeCV?Documentation for RidgeCVRidgeCV(alphas=array([1.82214402e+03, 1.69933761e+03, 1.58480794e+03, 1.47799719e+03,\n       1.37838513e+03, 1.28548658e+03, 1.19884909e+03, 1.11805068e+03,\n       1.04269780e+03, 9.72423459e+02, 9.06885373e+02, 8.45764334e+02,\n       7.88762649e+02, 7.35602686e+02, 6.86025527e+02, 6.39789702e+02,\n       5.96670018e+02, 5.56456456e+02, 5.18953153e+02, 4.83977447e+02,\n       4.51358987e+02, 4.20938902e+0...\n       6.86025527e+00, 6.39789702e+00, 5.96670018e+00, 5.56456456e+00,\n       5.18953153e+00, 4.83977447e+00, 4.51358987e+00, 4.20938902e+00,\n       3.92569029e+00, 3.66111190e+00, 3.41436521e+00, 3.18424843e+00,\n       2.96964074e+00, 2.76949689e+00, 2.58284207e+00, 2.40876716e+00,\n       2.24642431e+00, 2.09502283e+00, 1.95382531e+00, 1.82214402e+00]),\n        cv=KFold(n_splits=10, random_state=0, shuffle=True)) \n\n\n\netape_ridgecv = pipe_ridgecv.named_steps[\"ridgecv\"]\n\n\nalphas = pipe_lassocv.named_steps[\"lassocv\"].alphas_\ncoefs_lasso = []; coefs_enet = []; coefs_ridge = []\nXcr = StandardScaler().fit(X).transform(X)\nfor a in alphas:\n    ## lasso\n    lasso = Lasso(alpha=a, warm_start=True).fit(Xcr, y)\n    coefs_lasso.append(lasso.coef_)\n    ## enet\n    enet = ElasticNet(alpha=a*2, warm_start=True).fit(Xcr, y)\n    coefs_enet.append(enet.coef_)\n    ## ridge\n    ridge = Ridge(alpha=a*100).fit(Xcr, y)\n    coefs_ridge.append(ridge.coef_)\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3)\nax1.plot(np.log(alphas), coefs_lasso)\nax1.set_xlabel(r'$\\log(\\alpha)$')\nax1.set_ylabel('Coefficients')\nax2.plot(np.log(2*alphas), coefs_enet)\nax2.set_xlabel(r'$\\log(2\\alpha)$')\nax2.set_ylabel('Coefficients')\nax3.plot(np.log(100*alphas), coefs_ridge)\nax3.set_xlabel(r'$\\log(100\\alpha)$')\nax3.set_ylabel('Coefficients')\n\nText(0, 0.5, 'Coefficients')\n\n\n\n\n\n\n\n\n\n\nprint(\"Lasso: \", round(etape_lassocv.alpha_,4))\nprint(\"ElasticNet: \", round(etape_enetcv.alpha_,4))\nprint(\"Ridge: \", round(etape_ridgecv.alpha_,4))\n\nLasso:  0.7356\nElasticNet:  0.3399\nRidge:  9.0689\n\n\n\nozone = pd.read_csv(\"../donnees/ozone.txt\", header = 0, sep = \";\", index_col=0)\nXapp = ozone.iloc[0:45,1:10].to_numpy()\nXnew = ozone.iloc[45:50,1:10].to_numpy()\nyapp = np.ravel(ozone.iloc[0:45,:1])\nynew = np.ravel(ozone.iloc[45:50,:1])\n\n\nkf = KFold(n_splits=10, shuffle=True, random_state=0)\ncr = StandardScaler()\nlassocv = LassoCV(cv=kf)\npipe_lassocv = Pipeline(steps=[(\"cr\", cr), (\"lassocv\", lassocv)])\npipe_lassocv.fit(Xapp,yapp)\n\nPipeline(steps=[('cr', StandardScaler()),\n                ('lassocv',\n                 LassoCV(cv=KFold(n_splits=10, random_state=0, shuffle=True)))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFittedPipeline(steps=[('cr', StandardScaler()),\n                ('lassocv',\n                 LassoCV(cv=KFold(n_splits=10, random_state=0, shuffle=True)))]) StandardScaler?Documentation for StandardScalerStandardScaler() LassoCV?Documentation for LassoCVLassoCV(cv=KFold(n_splits=10, random_state=0, shuffle=True)) \n\n\n\npipe_lassocv.predict(Xnew)\n\narray([95.81725767, 63.675633  , 67.49560903, 67.57922292, 89.43629205])\n\n\n\n\nIntégration de variables qualitatives\n\nozone = pd.read_csv(\"../donnees/ozone.txt\", header=0, sep=\";\", index_col=0)\nozone[\"vent\"]=ozone[\"vent\"].astype(\"category\")\nozone[\"nebu\"]=ozone[\"nebu\"].astype(\"category\")\n\n\nkf = KFold(n_splits=10, shuffle=True, random_state=0)\ncr = StandardScaler()\nlassocv = LassoCV(cv=kf)\nenetcv = ElasticNetCV(cv=kf)\npipe_lassocv = Pipeline(steps=[(\"cr\", cr), (\"lassocv\", lassocv)])\n\n\nnoms = list(ozone.iloc[:, 1:].columns)\nformule = \"~\" + \"+\".join(noms[0:])\nXq = dmatrix(formule, ozone, return_type=\"dataframe\")\nXapp = Xq.iloc[0:45, 1:].to_numpy()\nXnew = Xq.iloc[45:50, 1:].to_numpy()\nyapp = np.ravel(ozone.iloc[0:45,:1])\nynew = np.ravel(ozone.iloc[45:50,:1])\n\n\npipe_lassocv.fit(Xapp, yapp)\npipe_lassocv.predict(Xnew).round(2)\n\narray([97.41, 63.72, 66.26, 66.76, 90.23])\n\n\n\nformule = \"~\" + \"+\".join(noms[0:]) + \"+C(vent,Treatment(1))\"\nXq = dmatrix(formule,ozone,return_type=\"dataframe\")\nXapp = Xq.iloc[0:45,1:].to_numpy()\nXnew = Xq.iloc[45:50,1:].to_numpy()\nyapp = np.ravel(ozone.iloc[0:45,:1])\nynew = np.ravel(ozone.iloc[45:50,:1])\npipe_lassocv.fit(Xapp,yapp)\npipe_lassocv.predict(Xnew).round(2)\n\narray([97.92, 63.63, 65.91, 66.42, 90.29])\n\n\n\nformule = \"~\" + \"+\".join(noms[0:-2]) + \\\n\"+ C(nebu, Sum) + C(vent, Sum)\"\nXq = dmatrix(formule,ozone,return_type=\"dataframe\")\nXapp = Xq.iloc[0:45,1:].to_numpy()\nXnew = Xq.iloc[45:50,1:].to_numpy()\nyapp = np.ravel(ozone.iloc[0:45,:1])\nynew = np.ravel(ozone.iloc[45:50,:1])\npipe_lassocv.fit(Xapp,yapp)\npipe_lassocv.predict(Xnew).round(2)\n\narray([99.49, 63.82, 65.65, 66.18, 90.31])\n\n\n\n\n\n\n Retour au sommet",
    "crumbs": [
      "Codes",
      "III Réduction de dimension",
      "9 Régularisation des moindres carrés : ridge, lasso elastic net"
    ]
  },
  {
    "objectID": "codes/chap11.html",
    "href": "codes/chap11.html",
    "title": "11 Comparaison des différentes méthodes, étude de cas réels",
    "section": "",
    "text": "import pandas as pd; import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LinearRegression, Ridge, \\\nElasticNet, Lasso\nfrom sklearn.linear_model import RidgeCV, ElasticNetCV, LassoCV\nfrom sklearn.cross_decomposition import PLSRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV, KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom patsy import dmatrix\n\nimport sys\nsys.path.append('../modules')\nimport ols_step_sk",
    "crumbs": [
      "Codes",
      "III Réduction de dimension",
      "11 Comparaison des différentes méthodes, étude de cas réels"
    ]
  },
  {
    "objectID": "codes/chap11.html#modèles-de-prévision-avec-interactions",
    "href": "codes/chap11.html#modèles-de-prévision-avec-interactions",
    "title": "11 Comparaison des différentes méthodes, étude de cas réels",
    "section": "Modèles de prévision avec interactions",
    "text": "Modèles de prévision avec interactions\n\nformuleI = \"1 + (\" + \"+\".join(nomsvar) + \")**2\"\nXinter = dmatrix(formuleI, don, return_type=\"dataframe\").\\\n    iloc[:,1:].to_numpy()\n\n\nformuleI = \"1 + (\" + \"+\".join(nomsvar) + \")**2\"\nXq = dmatrix(formuleI, don)\nXinter = np.asarray(Xq)[:,1:]\nXinter.shape\n\n(1366, 231)\n\n\n\nkfregul = KFold(n_splits=10, shuffle=True, random_state=0)\nkfaxes = KFold(n_splits=4, shuffle=True, random_state=0)\nnbaxes = 40\n# instanciation steps\ncr = StandardScaler()\nlassocv = LassoCV(cv=kfregul, n_jobs=3,max_iter=5000)\nenetcv = ElasticNetCV(cv=kfregul, n_jobs=3,max_iter=5000)\n# instanciation pipeline\npipe_lassocv = Pipeline(steps=[(\"cr\", cr), (\"lassocv\", lassocv)])\npipe_enetcv = Pipeline(steps=[(\"cr\", cr), (\"enetcv\", enetcv)])\n## ridge : path\netape_lasso = pipe_lassocv.named_steps[\"lassocv\"]\n# intanciations\nridge = Ridge()\npipe_ridge = Pipeline(steps=[(\"cr\", cr), (\"ridge\", ridge)])\nacp = PCA()\nreg = LinearRegression()\npipe_pcr = Pipeline(steps=[(\"cr\", cr), (\"acp\", acp), (\"reg\", reg)])\nregpls = PLSRegression()\n## grille composantes et decoupage VC\nparam_grid_pcr = { \"acp__n_components\" : list(range(1,nbaxes))}\nparam_grid_pls = { \"n_components\" : list(range(1,nbaxes))}\n \nfor i in np.arange(nb):\n    print(i)\n    Xapp = Xinter[bloc!=i,:]\n    Xtest = Xinter[bloc==i,:]\n    Yapp = don[bloc!=i][\"Y\"]\n    Ytest = don[bloc==i][\"Y\"]\n    #### reg\n    reg = LinearRegression()\n    reg.fit(Xapp,Yapp)\n    PREV.loc[PREV.bloc==i,\"MCO\"] = reg.predict(Xtest)\n    ### bic\n    inst_reg_bic = ols_step_sk.LinearRegressionSelectionFeatureIC(verbose=1,crit=\"bic\")\n    reg_bic = inst_reg_bic.fit(X=Xapp, y=Yapp)\n    PREV.loc[PREV.bloc==i,\"BIC\"] = reg_bic.predict(Xtest)\n    ### aic\n    inst_reg_aic = ols_step_sk.LinearRegressionSelectionFeatureIC(verbose=1,crit=\"aic\")\n    reg_aic = inst_reg_aic.fit(X=Xapp, y=Yapp)\n    PREV.loc[PREV.bloc==i,\"AIC\"] = reg_aic.predict(Xtest)\n    ## lasso\n    pipe_lassocv.fit(Xapp,Yapp)\n    PREV.loc[PREV.bloc==i,\"lasso\"] = pipe_lassocv.predict(Xtest)\n    ## elastic net\n    pipe_enetcv.fit(Xapp,Yapp)\n    PREV.loc[PREV.bloc==i,\"elast\"] = pipe_enetcv.predict(Xtest)    \n    ## params lambda\n    path_ridge = etape_lasso.alphas_ * 100    \n    param_grid_ridge = {\"ridge__alpha\": path_ridge}\n    ## GridSearchCV\n    cv_ridge = GridSearchCV(pipe_ridge, param_grid_ridge, cv=kfregul, scoring = \"neg_mean_squared_error\", n_jobs=3).fit(Xapp, Yapp)\n    PREV.loc[PREV.bloc==i,\"ridge\"] = cv_ridge.predict(Xtest)\n    ## gridsearch instanciation et fit\n    cv_pcr = GridSearchCV(pipe_pcr, param_grid_pcr, cv=kfaxes, scoring = \"neg_mean_squared_error\", n_jobs=3).fit(Xapp,Yapp)\n    cv_pls = GridSearchCV(regpls, param_grid_pls, cv=kfaxes, scoring = \"neg_mean_squared_error\", n_jobs=3).fit(Xapp,Yapp)\n    PREV.loc[PREV.bloc==i,\"pcr\"] = cv_pcr.predict(Xtest)\n    PREV.loc[PREV.bloc==i,\"pls\"] = cv_pls.predict(Xtest)\n\n0\nFinal: [5, 20, 21, 47, 98, 111, 125, 139, 142, 164, 185, 228]\nFinal: [1, 4, 5, 8, 13, 20, 21, 38, 41, 49, 62, 64, 82, 90, 98, 108, 110, 111, 115, 125, 127, 137, 139, 142, 143, 150, 152, 164, 165, 168, 192, 206, 221, 229]\n1\nFinal: [1, 5, 8, 20, 21, 98, 111, 125, 139, 142, 164, 185, 193]\nFinal: [1, 5, 7, 8, 9, 13, 20, 21, 22, 29, 38, 40, 41, 44, 46, 49, 73, 77, 98, 101, 108, 111, 114, 115, 125, 127, 137, 142, 143, 145, 150, 164, 165, 167, 183, 185, 193, 202, 206, 229]\n2\nFinal: [6, 10, 21, 40, 59, 95, 111, 139, 142, 153]\nFinal: [2, 4, 6, 8, 10, 11, 13, 20, 21, 38, 41, 42, 59, 62, 64, 75, 85, 90, 91, 98, 100, 109, 110, 111, 116, 118, 123, 126, 128, 137, 139, 140, 144, 149, 150, 153, 164, 166, 172, 183, 185, 194, 202, 205, 209, 221, 229]\n3\nFinal: [6, 9, 14, 20, 22, 53, 97, 111, 119, 164, 192]\nFinal: [6, 7, 8, 9, 13, 14, 20, 21, 41, 47, 53, 59, 61, 63, 64, 66, 85, 98, 101, 104, 111, 112, 119, 130, 137, 139, 143, 150, 158, 164, 168, 176, 192, 195, 221, 229, 230]\n\n\n4\nFinal: [7, 10, 20, 27, 41, 59, 77, 115, 125, 126, 164]\nFinal: [2, 4, 7, 13, 20, 24, 37, 41, 42, 43, 49, 59, 85, 90, 97, 101, 110, 115, 125, 126, 132, 137, 139, 142, 150, 154, 164, 183, 185, 190, 191, 193, 206, 209, 221, 229]\n5\nFinal: [6, 10, 20, 21, 59, 111, 113, 115]\nFinal: [4, 6, 8, 10, 11, 13, 20, 21, 22, 24, 41, 57, 59, 90, 97, 110, 111, 113, 115, 126, 128, 137, 142, 145, 150, 162, 164, 165, 183, 185, 194, 203, 209, 211, 221, 229]\n6\nFinal: [4, 6, 20, 24, 28, 41, 77, 97, 115, 125, 126, 164, 185, 192, 205]\nFinal: [4, 6, 13, 20, 21, 24, 28, 41, 49, 59, 77, 90, 97, 110, 111, 115, 117, 118, 125, 126, 128, 137, 144, 150, 164, 167, 183, 185, 190, 191, 194, 206, 209, 221, 226]\n7\nFinal: [5, 20, 21, 111, 125, 139, 142, 164, 185]\nFinal: [1, 4, 5, 8, 13, 20, 21, 41, 49, 77, 80, 90, 98, 101, 110, 111, 115, 116, 117, 125, 127, 130, 131, 137, 139, 149, 150, 154, 155, 156, 157, 161, 164, 167, 168, 185, 192, 201, 202, 221, 223, 227]\n8\nFinal: [4, 7, 10, 20, 21, 59, 96, 115, 125, 126, 164]\nFinal: [4, 5, 11, 13, 20, 21, 24, 29, 38, 41, 51, 59, 90, 96, 97, 110, 111, 115, 117, 125, 128, 130, 137, 139, 142, 150, 152, 157, 162, 164, 167, 168, 221, 226, 229]\n\n\n9\nFinal: [1, 5, 10, 20, 21, 111, 115, 125, 139, 164, 191, 206]\nFinal: [1, 2, 4, 5, 7, 13, 20, 21, 24, 37, 38, 41, 47, 62, 64, 73, 79, 81, 91, 95, 97, 110, 111, 115, 117, 122, 125, 127, 130, 131, 137, 139, 142, 145, 150, 166, 172, 192, 205, 229]\n\n\n\nprev = PREV.iloc[:,1:]\nround((prev.sub(PREV.Y, axis=0)**2).mean(),2)\n\nY          0.00\nMCO      187.73\nBIC      168.28\nAIC      168.38\nridge    165.13\nlasso    161.60\nelast    164.25\npls      168.91\npcr      173.75\ndtype: float64",
    "crumbs": [
      "Codes",
      "III Réduction de dimension",
      "11 Comparaison des différentes méthodes, étude de cas réels"
    ]
  },
  {
    "objectID": "codes/chap11.html#modèles-de-prévision-avec-des-polynômes",
    "href": "codes/chap11.html#modèles-de-prévision-avec-des-polynômes",
    "title": "11 Comparaison des différentes méthodes, étude de cas réels",
    "section": "Modèles de prévision avec des polynômes",
    "text": "Modèles de prévision avec des polynômes\n\nXcar = X**2\nXcub = X**3\nXpol = np.concatenate((X, Xcar, Xcub), axis=1)\nXpol.shape\n\n(1366, 63)\n\n\n\nkfregul = KFold(n_splits=10, shuffle=True, random_state=0)\nkfaxes = KFold(n_splits=4, shuffle=True, random_state=0)\nnbaxes = 40\n# instanciation steps\ncr = StandardScaler()\nlassocv = LassoCV(cv=kfregul, n_jobs=3,max_iter=3000)\nenetcv = ElasticNetCV(cv=kfregul, n_jobs=3,max_iter=3000)\n# instanciation pipeline\npipe_lassocv = Pipeline(steps=[(\"cr\", cr), (\"lassocv\", lassocv)])\npipe_enetcv = Pipeline(steps=[(\"cr\", cr), (\"enetcv\", enetcv)])\n## ridge : path\netape_lasso = pipe_lassocv.named_steps[\"lassocv\"]\n# intanciations\nridge = Ridge()\npipe_ridge = Pipeline(steps=[(\"cr\", cr), (\"ridge\", ridge)])\nacp = PCA()\nreg = LinearRegression()\npipe_pcr = Pipeline(steps=[(\"cr\", cr), (\"acp\", acp), (\"reg\", reg)])\nregpls = PLSRegression()\n## grille composantes et decoupage VC\nparam_grid_pcr = { \"acp__n_components\" : list(range(1,nbaxes))}\nparam_grid_pls = { \"n_components\" : list(range(1,nbaxes))}\n \nfor i in np.arange(nb):\n    print(i)\n    Xapp = Xpol[bloc!=i,:]\n    Xtest = Xpol[bloc==i,:]\n    Yapp = don[bloc!=i][\"Y\"]\n    Ytest = don[bloc==i][\"Y\"]\n    #### reg\n    reg = LinearRegression()\n    reg.fit(Xapp,Yapp)\n    PREV.loc[PREV.bloc==i,\"MCO\"] = reg.predict(Xtest)\n    ### bic\n    inst_reg_bic = ols_step_sk.LinearRegressionSelectionFeatureIC(verbose=1,crit=\"bic\")\n    reg_bic = inst_reg_bic.fit(X=Xapp, y=Yapp)\n    PREV.loc[PREV.bloc==i,\"BIC\"] = reg_bic.predict(Xtest)\n    ### aic\n    inst_reg_aic = ols_step_sk.LinearRegressionSelectionFeatureIC(verbose=1,crit=\"aic\")\n    reg_aic = inst_reg_aic.fit(X=Xapp, y=Yapp)\n    PREV.loc[PREV.bloc==i,\"AIC\"] = reg_aic.predict(Xtest)\n    ## lasso\n    pipe_lassocv.fit(Xapp,Yapp)\n    PREV.loc[PREV.bloc==i,\"lasso\"] = pipe_lassocv.predict(Xtest)\n    ## elastic net\n    pipe_enetcv.fit(Xapp,Yapp)\n    PREV.loc[PREV.bloc==i,\"elast\"] = pipe_enetcv.predict(Xtest)    \n    ## params lambda\n    path_ridge = etape_lasso.alphas_ * 100    \n    param_grid_ridge = {\"ridge__alpha\": path_ridge}\n    ## GridSearchCV\n    cv_ridge = GridSearchCV(pipe_ridge, param_grid_ridge, cv=kfregul, scoring = \"neg_mean_squared_error\", n_jobs=3).fit(Xapp, Yapp)\n    PREV.loc[PREV.bloc==i,\"ridge\"] = cv_ridge.predict(Xtest)\n    ## gridsearch instanciation et fit\n    cv_pcr = GridSearchCV(pipe_pcr, param_grid_pcr, cv=kfaxes, scoring = \"neg_mean_squared_error\", n_jobs=3).fit(Xapp,Yapp)\n    cv_pls = GridSearchCV(regpls, param_grid_pls, cv=kfaxes, scoring = \"neg_mean_squared_error\", n_jobs=3).fit(Xapp,Yapp)\n    PREV.loc[PREV.bloc==i,\"pcr\"] = cv_pcr.predict(Xtest)\n    PREV.loc[PREV.bloc==i,\"pls\"] = cv_pls.predict(Xtest)\n\n0\nFinal: [11, 20, 21, 26, 29, 43, 47]\nFinal: [0, 1, 4, 11, 19, 20, 21, 26, 27, 29, 31, 34, 35, 38, 42, 43, 47, 52, 53, 56]\n1\nFinal: [11, 20, 21, 26, 29, 43, 47, 53]\nFinal: [0, 1, 4, 8, 11, 12, 13, 19, 20, 21, 22, 26, 29, 31, 34, 35, 38, 42, 47, 48, 52, 53, 56]\n\n\n2\nFinal: [5, 11, 20, 29, 38, 42, 47, 48]\nFinal: [0, 1, 4, 5, 8, 11, 13, 19, 20, 21, 22, 29, 31, 34, 35, 38, 42, 47, 48, 52, 53, 56]\n3\nFinal: [5, 11, 20, 21, 29, 33, 38, 47, 48, 53]\nFinal: [1, 5, 10, 11, 12, 19, 20, 21, 29, 31, 33, 34, 35, 38, 43, 46, 47, 48, 53, 54, 56]\n4\nFinal: [11, 20, 21, 26, 29, 43, 47]\nFinal: [0, 1, 4, 8, 10, 11, 18, 19, 20, 21, 26, 27, 29, 31, 34, 35, 38, 42, 43, 47, 56, 60]\n\n\n5\nFinal: [5, 11, 20, 29, 38, 42, 47, 48, 53]\nFinal: [0, 1, 4, 5, 8, 10, 11, 19, 20, 21, 22, 29, 34, 35, 38, 41, 42, 47, 48, 53, 56, 62]\n\n\n6\nFinal: [5, 11, 20, 21, 29, 43, 47]\nFinal: [0, 5, 8, 11, 12, 19, 20, 21, 27, 29, 31, 34, 35, 38, 42, 43, 46, 47, 52, 53, 56]\n\n\n7\nFinal: [11, 20, 21, 26, 29, 43, 47]\nFinal: [1, 11, 12, 19, 20, 21, 26, 27, 29, 31, 34, 35, 38, 43, 46, 47, 52, 53, 56]\n\n\n8\nFinal: [5, 11, 20, 21, 29, 43, 47]\nFinal: [0, 1, 5, 11, 19, 20, 21, 22, 29, 33, 34, 35, 38, 42, 46, 47, 48, 52, 53, 56]\n9\nFinal: [0, 1, 5, 11, 16, 20, 29, 38, 43, 47, 53]\nFinal: [0, 1, 5, 8, 10, 11, 19, 20, 21, 22, 29, 31, 34, 35, 38, 42, 46, 47, 48, 53, 56]\n\n\n\nprev = PREV.iloc[:,1:]\nround((prev.sub(PREV.Y, axis=0)**2).mean(),2)\n\nY          0.00\nMCO      165.40\nBIC      167.61\nAIC      164.87\nridge    165.08\nlasso    163.90\nelast    164.76\npls      166.16\npcr      169.63\ndtype: float64",
    "crumbs": [
      "Codes",
      "III Réduction de dimension",
      "11 Comparaison des différentes méthodes, étude de cas réels"
    ]
  },
  {
    "objectID": "codes/chap11.html#modèles-de-prévision-des-splines",
    "href": "codes/chap11.html#modèles-de-prévision-des-splines",
    "title": "11 Comparaison des différentes méthodes, étude de cas réels",
    "section": "Modèles de prévision des splines",
    "text": "Modèles de prévision des splines\n\nXp = np.ones((X.shape[0],1))\nfor i in nomsvar:\n    xi = don.loc[:,i].quantile([0.25, 0.5, 0.75])\n    formule = \"-1 + bs(\" + i + \",knots=xi, degree=3)\"\n    BX = dmatrix(formule, don)\n    Xp = np.concatenate((Xp, BX), axis=1)\n\nXspline = Xp[:,1:]\n\n\nkfregul = KFold(n_splits=10, shuffle=True, random_state=0)\nkfaxes = KFold(n_splits=4, shuffle=True, random_state=0)\nnbaxes = 40\n# instanciation steps\ncr = StandardScaler()\nlassocv = LassoCV(cv=kfregul, n_jobs=3,max_iter=3000)\nenetcv = ElasticNetCV(cv=kfregul, n_jobs=3,max_iter=3000)\n# instanciation pipeline\npipe_lassocv = Pipeline(steps=[(\"cr\", cr), (\"lassocv\", lassocv)])\npipe_enetcv = Pipeline(steps=[(\"cr\", cr), (\"enetcv\", enetcv)])\n## ridge : path\netape_lasso = pipe_lassocv.named_steps[\"lassocv\"]\n# intanciations\nridge = Ridge()\npipe_ridge = Pipeline(steps=[(\"cr\", cr), (\"ridge\", ridge)])\nacp = PCA()\nreg = LinearRegression()\npipe_pcr = Pipeline(steps=[(\"cr\", cr), (\"acp\", acp), (\"reg\", reg)])\nregpls = PLSRegression()\n## grille composantes et decoupage VC\nparam_grid_pcr = { \"acp__n_components\" : list(range(1,nbaxes))}\nparam_grid_pls = { \"n_components\" : list(range(1,nbaxes))}\n \nfor i in np.arange(nb):\n    print(i)\n    Xapp = Xspline[bloc!=i,:]\n    Xtest = Xspline[bloc==i,:]\n    Yapp = don[bloc!=i][\"Y\"]\n    Ytest = don[bloc==i][\"Y\"]\n    #### reg\n    reg = LinearRegression()\n    reg.fit(Xapp,Yapp)\n    PREV.loc[PREV.bloc==i,\"MCO\"] = reg.predict(Xtest)\n    ### bic\n    inst_reg_bic = ols_step_sk.LinearRegressionSelectionFeatureIC(verbose=1,crit=\"bic\")\n    reg_bic = inst_reg_bic.fit(X=Xapp, y=Yapp)\n    PREV.loc[PREV.bloc==i,\"BIC\"] = reg_bic.predict(Xtest)\n    ### aic\n    inst_reg_aic = ols_step_sk.LinearRegressionSelectionFeatureIC(verbose=1,crit=\"aic\")\n    reg_aic = inst_reg_aic.fit(X=Xapp, y=Yapp)\n    PREV.loc[PREV.bloc==i,\"AIC\"] = reg_aic.predict(Xtest)\n    ## lasso\n    pipe_lassocv.fit(Xapp,Yapp)\n    PREV.loc[PREV.bloc==i,\"lasso\"] = pipe_lassocv.predict(Xtest)\n    ## elastic net\n    pipe_enetcv.fit(Xapp,Yapp)\n    PREV.loc[PREV.bloc==i,\"elast\"] = pipe_enetcv.predict(Xtest)    \n    ## params lambda\n    path_ridge = etape_lasso.alphas_ * 100    \n    param_grid_ridge = {\"ridge__alpha\": path_ridge}\n    ## GridSearchCV\n    cv_ridge = GridSearchCV(pipe_ridge, param_grid_ridge, cv=kfregul, scoring = \"neg_mean_squared_error\", n_jobs=3).fit(Xapp, Yapp)\n    PREV.loc[PREV.bloc==i,\"ridge\"] = cv_ridge.predict(Xtest)\n    ## gridsearch instanciation et fit\n    cv_pcr = GridSearchCV(pipe_pcr, param_grid_pcr, cv=kfaxes, scoring = \"neg_mean_squared_error\", n_jobs=3).fit(Xapp,Yapp)\n    cv_pls = GridSearchCV(regpls, param_grid_pls, cv=kfaxes, scoring = \"neg_mean_squared_error\", n_jobs=3).fit(Xapp,Yapp)\n    PREV.loc[PREV.bloc==i,\"pcr\"] = cv_pcr.predict(Xtest)\n    PREV.loc[PREV.bloc==i,\"pls\"] = cv_pls.predict(Xtest)\n\n0\nFinal: [3, 5, 29, 34, 35, 39, 51, 53, 67, 87, 120, 121, 124]\nFinal: [1, 3, 5, 7, 8, 23, 26, 28, 29, 34, 35, 39, 48, 51, 52, 53, 65, 67, 70, 73, 80, 87, 88, 92, 104, 107, 108, 120, 121, 123, 124, 125]\n1\nFinal: [3, 5, 34, 39, 41, 48, 51, 52, 87, 120, 121, 124]\nFinal: [0, 1, 3, 5, 7, 8, 23, 26, 28, 33, 34, 39, 40, 41, 46, 48, 49, 51, 52, 54, 59, 62, 65, 67, 70, 80, 82, 87, 88, 99, 102, 106, 120, 121, 123, 124, 125]\n2\nFinal: [1, 3, 5, 7, 34, 39, 41, 49, 52, 87, 120, 121, 124]\nFinal: [0, 1, 3, 5, 7, 12, 23, 26, 27, 28, 29, 33, 34, 39, 40, 41, 46, 49, 51, 52, 59, 65, 70, 82, 84, 87, 88, 92, 104, 107, 120, 121, 124]\n3\nFinal: [3, 5, 7, 28, 34, 41, 51, 52, 70, 77, 87, 88, 120, 121, 124]\nFinal: [0, 3, 5, 7, 8, 23, 26, 28, 31, 34, 38, 39, 40, 41, 44, 48, 49, 51, 52, 53, 56, 62, 67, 70, 73, 77, 80, 87, 88, 99, 103, 104, 122, 123, 124, 125]\n4\nFinal: [3, 5, 34, 39, 41, 51, 52, 87, 120, 121, 124]\nFinal: [0, 1, 3, 5, 7, 8, 23, 25, 26, 27, 28, 29, 33, 34, 35, 39, 40, 41, 45, 46, 49, 51, 52, 59, 65, 67, 70, 75, 80, 87, 88, 92, 102, 106, 120, 121, 123, 124]\n5\nFinal: [3, 5, 34, 39, 41, 48, 51, 52, 87, 104, 120, 121, 123, 124, 125]\nFinal: [0, 2, 4, 5, 7, 8, 23, 26, 28, 29, 34, 35, 39, 40, 41, 46, 48, 49, 51, 52, 59, 65, 67, 70, 73, 80, 87, 88, 92, 104, 122, 123, 124, 125]\n6\nFinal: [1, 3, 5, 7, 28, 34, 39, 40, 41, 49, 52, 87, 120, 121, 124]\nFinal: [0, 1, 3, 5, 7, 26, 28, 33, 34, 39, 40, 41, 46, 48, 49, 51, 52, 59, 65, 70, 80, 87, 88, 92, 104, 108, 109, 117, 122, 123, 124, 125]\n7\nFinal: [2, 7, 26, 28, 34, 39, 41, 49, 52, 65, 70, 87, 88, 120, 121, 124]\nFinal: [2, 7, 17, 23, 26, 27, 28, 29, 33, 34, 38, 39, 40, 41, 43, 46, 48, 49, 51, 52, 53, 65, 67, 70, 73, 80, 85, 87, 88, 92, 104, 106, 108, 122, 123, 124, 125]\n8\nFinal: [3, 5, 7, 34, 39, 40, 41, 51, 52, 63, 120, 121, 124]\nFinal: [0, 2, 3, 4, 5, 7, 8, 23, 26, 28, 33, 34, 39, 40, 41, 46, 48, 51, 52, 59, 63, 65, 70, 80, 87, 88, 89, 92, 104, 105, 122, 123, 124, 125]\n9\nFinal: [1, 3, 5, 7, 28, 34, 39, 41, 51, 53, 87, 104, 120, 121, 124]\nFinal: [0, 1, 2, 3, 5, 7, 8, 26, 28, 34, 39, 40, 41, 46, 48, 51, 52, 53, 63, 65, 70, 80, 87, 88, 89, 92, 104, 108, 122, 123, 124, 125]\n\n\n\nprev = PREV.iloc[:,1:]\nround((prev.sub(PREV.Y, axis=0)**2).mean(),2)\n\nY          0.00\nMCO      162.74\nBIC      163.77\nAIC      158.73\nridge    157.86\nlasso    155.43\nelast    156.55\npls      160.24\npcr      167.39\ndtype: float64",
    "crumbs": [
      "Codes",
      "III Réduction de dimension",
      "11 Comparaison des différentes méthodes, étude de cas réels"
    ]
  },
  {
    "objectID": "codes/chap11.html#modèles-de-prévision-avec-des-splines-et-des-interactions",
    "href": "codes/chap11.html#modèles-de-prévision-avec-des-splines-et-des-interactions",
    "title": "11 Comparaison des différentes méthodes, étude de cas réels",
    "section": "Modèles de prévision avec des splines et des interactions",
    "text": "Modèles de prévision avec des splines et des interactions\n\nXsplineinter = np.concatenate((Xinter[:,22:],Xspline),axis=1)\n\n\nkfregul = KFold(n_splits=10, shuffle=True, random_state=0)\nkfaxes = KFold(n_splits=4, shuffle=True, random_state=0)\nnbaxes = 40\n# instanciation steps\ncr = StandardScaler()\nlassocv = LassoCV(cv=kfregul, n_jobs=3,max_iter=3000)\nenetcv = ElasticNetCV(cv=kfregul, n_jobs=3,max_iter=3000)\n# instanciation pipeline\npipe_lassocv = Pipeline(steps=[(\"cr\", cr), (\"lassocv\", lassocv)])\npipe_enetcv = Pipeline(steps=[(\"cr\", cr), (\"enetcv\", enetcv)])\n## ridge : path\netape_lasso = pipe_lassocv.named_steps[\"lassocv\"]\n# intanciations\nridge = Ridge()\npipe_ridge = Pipeline(steps=[(\"cr\", cr), (\"ridge\", ridge)])\nacp = PCA()\nreg = LinearRegression()\npipe_pcr = Pipeline(steps=[(\"cr\", cr), (\"acp\", acp), (\"reg\", reg)])\nregpls = PLSRegression()\n## grille composantes et decoupage VC\nparam_grid_pcr = { \"acp__n_components\" : list(range(1,nbaxes))}\nparam_grid_pls = { \"n_components\" : list(range(1,nbaxes))}\n \nfor i in np.arange(nb):\n    print(i)\n    Xapp = Xsplineinter[bloc!=i,:]\n    Xtest = Xsplineinter[bloc==i,:]\n    Yapp = don[bloc!=i][\"Y\"]\n    Ytest = don[bloc==i][\"Y\"]\n    #### reg\n    reg = LinearRegression()\n    reg.fit(Xapp,Yapp)\n    PREV.loc[PREV.bloc==i,\"MCO\"] = reg.predict(Xtest)\n    ### bic\n    inst_reg_bic = ols_step_sk.LinearRegressionSelectionFeatureIC(verbose=1,crit=\"bic\")\n    reg_bic = inst_reg_bic.fit(X=Xapp, y=Yapp)\n    PREV.loc[PREV.bloc==i,\"BIC\"] = reg_bic.predict(Xtest)\n    ### aic\n    inst_reg_aic = ols_step_sk.LinearRegressionSelectionFeatureIC(verbose=1,crit=\"aic\")\n    reg_aic = inst_reg_aic.fit(X=Xapp, y=Yapp)\n    PREV.loc[PREV.bloc==i,\"AIC\"] = reg_aic.predict(Xtest)\n    ## lasso\n    pipe_lassocv.fit(Xapp,Yapp)\n    PREV.loc[PREV.bloc==i,\"lasso\"] = pipe_lassocv.predict(Xtest)\n    ## elastic net\n    pipe_enetcv.fit(Xapp,Yapp)\n    PREV.loc[PREV.bloc==i,\"elast\"] = pipe_enetcv.predict(Xtest)    \n    ## params lambda\n    path_ridge = etape_lasso.alphas_ * 100    \n    param_grid_ridge = {\"ridge__alpha\": path_ridge}\n    ## GridSearchCV\n    cv_ridge = GridSearchCV(pipe_ridge, param_grid_ridge, cv=kfregul, scoring = \"neg_mean_squared_error\", n_jobs=3).fit(Xapp, Yapp)\n    PREV.loc[PREV.bloc==i,\"ridge\"] = cv_ridge.predict(Xtest)\n    ## gridsearch instanciation et fit\n    cv_pcr = GridSearchCV(pipe_pcr, param_grid_pcr, cv=kfaxes, scoring = \"neg_mean_squared_error\", n_jobs=3).fit(Xapp,Yapp)\n    cv_pls = GridSearchCV(regpls, param_grid_pls, cv=kfaxes, scoring = \"neg_mean_squared_error\", n_jobs=3).fit(Xapp,Yapp)\n    PREV.loc[PREV.bloc==i,\"pcr\"] = cv_pcr.predict(Xtest)\n    PREV.loc[PREV.bloc==i,\"pls\"] = cv_pls.predict(Xtest)\n\n0\nFinal: [2, 79, 117, 121, 131, 211, 243, 244, 249, 313, 330]\nFinal: [2, 4, 16, 45, 76, 79, 117, 121, 128, 140, 142, 147, 158, 161, 171, 174, 181, 184, 207, 209, 216, 217, 232, 236, 243, 244, 249, 253, 257, 291, 294, 296, 297, 301, 308, 313, 329, 330, 332, 333, 334]\n1\nFinal: [5, 7, 19, 55, 76, 130, 131, 211, 243, 244, 296, 330]\nFinal: [5, 7, 8, 11, 16, 19, 25, 29, 55, 56, 66, 69, 76, 80, 86, 93, 97, 117, 118, 131, 136, 142, 143, 147, 163, 171, 174, 184, 185, 187, 211, 238, 241, 243, 245, 249, 252, 274, 276, 282, 291, 295, 296, 297, 298, 301, 308, 315, 329, 330, 333]\n2\nFinal: [6, 21, 79, 105, 117, 121, 211, 243, 250, 274, 291, 296, 297, 313, 330]\nFinal: [3, 11, 19, 37, 55, 56, 75, 95, 97, 106, 108, 109, 115, 117, 124, 128, 131, 133, 136, 142, 144, 145, 147, 171, 174, 184, 187, 204, 207, 209, 210, 211, 222, 232, 238, 241, 243, 248, 249, 250, 255, 258, 259, 266, 274, 281, 291, 293, 294, 296, 297, 298, 300, 308, 313, 330, 331, 333]\n3\nFinal: [5, 19, 25, 55, 76, 79, 108, 117, 131, 211, 243, 244, 286, 289, 296, 297, 313, 330]\nFinal: [5, 6, 16, 19, 22, 24, 36, 51, 55, 57, 59, 69, 74, 75, 94, 95, 102, 108, 109, 115, 124, 128, 130, 134, 136, 142, 145, 147, 153, 166, 170, 181, 183, 207, 209, 213, 216, 221, 223, 228, 238, 243, 245, 249, 250, 256, 261, 277, 280, 286, 291, 292, 294, 297, 298, 305, 315, 329, 330, 333]\n4\nFinal: [2, 19, 55, 79, 105, 117, 121, 211, 243, 244, 286, 289, 296, 297, 330]\nFinal: [2, 5, 15, 19, 20, 55, 63, 69, 79, 93, 115, 117, 120, 128, 136, 142, 147, 151, 165, 168, 169, 171, 184, 193, 209, 215, 216, 232, 238, 243, 244, 249, 255, 258, 259, 268, 276, 284, 286, 291, 293, 294, 296, 297, 301, 308, 311, 315, 329, 330, 333]\n5\nFinal: [6, 75, 93, 97, 105, 117, 171, 211, 216, 243, 289, 295, 297, 330]\nFinal: [2, 16, 18, 19, 21, 22, 23, 55, 56, 68, 74, 75, 81, 91, 93, 105, 115, 123, 124, 127, 128, 130, 131, 135, 142, 143, 147, 150, 155, 161, 174, 189, 196, 200, 207, 209, 213, 216, 221, 228, 232, 236, 243, 244, 248, 249, 255, 262, 267, 268, 270, 276, 289, 295, 297, 298, 301, 307, 309, 312, 313, 325, 329, 330, 333]\n6\nFinal: [19, 55, 75, 93, 97, 103, 142, 155, 211, 241, 242, 243, 289, 297, 330]\nFinal: [2, 3, 16, 19, 25, 29, 53, 55, 74, 75, 81, 86, 94, 97, 103, 130, 142, 164, 170, 184, 208, 209, 232, 238, 242, 243, 245, 248, 249, 255, 259, 274, 275, 280, 284, 291, 295, 297, 298, 301, 308, 313, 329, 330, 333]\n7\nFinal: [76, 78, 117, 120, 131, 174, 211, 216, 243, 249, 291, 296, 297, 313, 330]\nFinal: [0, 5, 31, 36, 42, 51, 55, 61, 69, 75, 81, 93, 115, 117, 120, 123, 128, 140, 142, 163, 170, 172, 174, 184, 208, 209, 216, 221, 224, 232, 238, 243, 244, 245, 249, 255, 257, 274, 289, 290, 294, 296, 297, 315, 325, 329, 330, 333]\n8\nFinal: [0, 55, 76, 93, 117, 131, 155, 211, 243, 244, 289, 296, 297, 313, 330]\nFinal: [0, 5, 6, 11, 28, 31, 34, 36, 55, 68, 76, 93, 117, 120, 122, 127, 128, 136, 142, 147, 150, 151, 164, 172, 174, 184, 187, 193, 199, 208, 209, 211, 216, 229, 232, 238, 243, 244, 245, 249, 255, 259, 274, 280, 281, 291, 295, 297, 298, 301, 309, 311, 313, 314, 329, 330, 333]\n\n\n9\nFinal: [0, 55, 76, 94, 105, 117, 155, 211, 243, 244, 296, 297, 313, 330]\nFinal: [5, 11, 12, 16, 19, 22, 24, 25, 55, 57, 59, 68, 76, 78, 93, 100, 114, 115, 117, 120, 123, 127, 128, 142, 150, 154, 160, 168, 169, 170, 174, 176, 177, 184, 199, 207, 209, 211, 216, 231, 243, 245, 249, 255, 256, 259, 273, 275, 280, 281, 291, 294, 296, 297, 298, 301, 309, 313, 314, 329, 330, 333, 334]\n\n\n\nprev = PREV.iloc[:,1:]\nround((prev.sub(PREV.Y, axis=0)**2).mean(),2)\n\nY          0.00\nMCO      193.29\nBIC      153.88\nAIC      164.73\nridge    156.45\nlasso    154.41\nelast    154.17\npls      160.23\npcr      164.32\ndtype: float64",
    "crumbs": [
      "Codes",
      "III Réduction de dimension",
      "11 Comparaison des différentes méthodes, étude de cas réels"
    ]
  },
  {
    "objectID": "codes/chap15.html",
    "href": "codes/chap15.html",
    "title": "15 Comparaison des différentes méthodes en classification supervisée, étude de cas réels",
    "section": "",
    "text": "import pandas as pd; import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression, \\\n    LogisticRegressionCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import StratifiedKFold\nimport sklearn.metrics as sklm\nfrom patsy import dmatrix\n\nimport sys\nsys.path.append('../modules')\nimport logistic_step_sk as lss",
    "crumbs": [
      "Codes",
      "IV Le modèle linéaire généralisé",
      "15 Comparaison des différentes méthodes en classification supervisée, étude de cas réels"
    ]
  },
  {
    "objectID": "codes/chap15.html#interactions",
    "href": "codes/chap15.html#interactions",
    "title": "15 Comparaison des différentes méthodes en classification supervisée, étude de cas réels",
    "section": "Interactions",
    "text": "Interactions\n\nformuleI = \"1 + (\" + \"+\".join(nomsvar) + \")**2\"\nPROB = pd.DataFrame({\"Y\":Y,\"log\":0.0,\"BIC\":0.0,\"AIC\":0.0,\n                    \"ridge\":0.0,\"lasso\":0.0,\"elast\":0.0})\n\n\nXinter = dmatrix(formuleI, don, return_type=\"dataframe\").iloc[:,1:].to_numpy()\nXinter.shape\n\n(462, 45)\n\n\n\ncr = StandardScaler()\nlassocv =  LogisticRegressionCV(cv=10, penalty=\"l1\", n_jobs=10, Cs=Cs_lasso,  solver=\"saga\", max_iter=2000)\nenetcv = LogisticRegressionCV(cv=10, penalty=\"elasticnet\", l1_ratios = [0.5], n_jobs=10,  Cs=Cs_enet, solver=\"saga\", max_iter=2000)\nridgecv = LogisticRegressionCV(cv=10, penalty=\"l2\", n_jobs=10, Cs=Cs_ridge,  max_iter=1000)\npipe_lassocv = Pipeline(steps=[(\"cr\", cr), (\"lassocv\", lassocv)])\npipe_enetcv = Pipeline(steps=[(\"cr\", cr), (\"enetcv\", enetcv)])\npipe_ridgecv = Pipeline(steps=[(\"cr\", cr), (\"ridgecv\", ridgecv)])\n\nnb=10\nskf = StratifiedKFold(n_splits=nb, shuffle=True, random_state=1234)\n\n\nfor app_index, val_index in skf.split(X,Y):\n    Xapp = Xinter[app_index,:]\n    Xtest = Xinter[val_index,:]\n    Yapp = Y[app_index]\n    ### log\n    log = LogisticRegression(penalty=None,solver=\"newton-cholesky\").fit(Xapp,Yapp)\n    PROB.loc[val_index,\"log\"] = log.predict_proba(Xtest)[:,1]\n    ### bic\n    choixbic = lss.LogisticRegressionSelectionFeatureIC(start=[], \\\n        direction=\"forward\",crit=\"bic\").fit(Xapp,Yapp)\n    PROB.loc[val_index, \"BIC\"] = choixbic.predict_proba(Xtest)[:,1]\n    ### aic\n    choixaic = lss.LogisticRegressionSelectionFeatureIC(start=[], \\\n        direction=\"forward\",crit=\"aic\").fit(Xapp,Yapp)\n    PROB.loc[val_index, \"AIC\"] = choixaic.predict_proba(Xtest)[:,1]\n    ### lasso\n    cr = StandardScaler()\n    Cs_lasso = grille(Xapp,Yapp, \"lasso\")\n    lassocv =  LogisticRegressionCV(cv=10, penalty=\"l1\", n_jobs=10,\\\n                 Cs=Cs_lasso,  solver=\"saga\", max_iter=2000)\n    pipe_lassocv = Pipeline(steps=[(\"cr\", cr), (\"lassocv\", lassocv)])\n    pipe_lassocv.fit(Xapp,Yapp)\n    PROB.loc[val_index,\"lasso\"] = pipe_lassocv.predict_proba(Xtest)[:,1]\n    ### elastic net\n    cr = StandardScaler()\n    Cs_enet = grille(Xapp,Yapp,\"enet\")\n    enetcv=LogisticRegressionCV(cv=10,penalty=\"elasticnet\",n_jobs=10,\\\n          l1_ratios=[0.5],Cs=Cs_enet,solver=\"saga\",max_iter=2000)\n    pipe_enetcv = Pipeline(steps=[(\"cr\", cr), (\"enetcv\", enetcv)])\n    pipe_enetcv.fit(Xapp,Yapp)\n    PROB.loc[val_index,\"elast\"] = pipe_enetcv.predict_proba(Xtest)[:,1] \n    ### ridge\n    cr = StandardScaler()\n    Cs_ridge = grille(Xapp,Yapp,\"ridge\")\n    ridgecv = LogisticRegressionCV(cv=10, penalty=\"l2\", \\\n            Cs=Cs_ridge,  max_iter=1000)\n    pipe_ridgecv = Pipeline(steps=[(\"cr\", cr), (\"ridgecv\", ridgecv)])\n    pipe_ridgecv.fit(Xapp,Yapp)\n    PROB.loc[val_index,\"ridge\"] = pipe_ridgecv.predict_proba(Xtest)[:,1]\n\n\nmc = pd.Series(0.0, index=PROB.columns[1:])\ns = 0.5\nfor i in range(mc.shape[0]):\n    mc.iloc[i] = sklm.zero_one_loss(PROB.Y, PROB.iloc[:,i+1]&gt;s)\n\nround(mc.sort_values(ascending=True),3)\n\nAIC      0.245\nBIC      0.251\nridge    0.260\nelast    0.264\nlasso    0.268\nlog      0.288\ndtype: float64",
    "crumbs": [
      "Codes",
      "IV Le modèle linéaire généralisé",
      "15 Comparaison des différentes méthodes en classification supervisée, étude de cas réels"
    ]
  },
  {
    "objectID": "codes/chap15.html#polynôme",
    "href": "codes/chap15.html#polynôme",
    "title": "15 Comparaison des différentes méthodes en classification supervisée, étude de cas réels",
    "section": "Polynôme",
    "text": "Polynôme\n\nXquanti = don.drop(columns=\"Y\").\\\n                    select_dtypes(include=[np.number]).to_numpy()\nXcar = Xquanti**2\nXcub = Xquanti**3\nformule = \"~\" + \"+\".join(nomsvar)\nX = dmatrix(formule, don, return_type=\"dataframe\").\\\n                                            iloc[:,1:].to_numpy()\nXpol = np.concatenate((X, Xcar, Xcub), axis=1)\nXpol.shape\n\n(462, 25)\n\n\n\ncr = StandardScaler()\nlassocv =  LogisticRegressionCV(cv=10, penalty=\"l1\", n_jobs=10, Cs=Cs_lasso,  solver=\"saga\", max_iter=2000)\nenetcv = LogisticRegressionCV(cv=10, penalty=\"elasticnet\", l1_ratios = [0.5], n_jobs=10,  Cs=Cs_enet, solver=\"saga\", max_iter=2000)\nridgecv = LogisticRegressionCV(cv=10, penalty=\"l2\", n_jobs=10, Cs=Cs_ridge,  max_iter=1000)\npipe_lassocv = Pipeline(steps=[(\"cr\", cr), (\"lassocv\", lassocv)])\npipe_enetcv = Pipeline(steps=[(\"cr\", cr), (\"enetcv\", enetcv)])\npipe_ridgecv = Pipeline(steps=[(\"cr\", cr), (\"ridgecv\", ridgecv)])\n\nnb=10\nskf = StratifiedKFold(n_splits=nb, shuffle=True, random_state=1234)\n\n\nfor app_index, val_index in skf.split(X,Y):\n    Xapp = Xpol[app_index,:]\n    Xtest = Xpol[val_index,:]\n    Yapp = Y[app_index]\n    ### log\n    log = LogisticRegression(penalty=None,solver=\"newton-cholesky\").fit(Xapp,Yapp)\n    PROB.loc[val_index,\"log\"] = log.predict_proba(Xtest)[:,1]\n    ### bic\n    choixbic = lss.LogisticRegressionSelectionFeatureIC(start=[], \\\n        direction=\"forward\",crit=\"bic\").fit(Xapp,Yapp)\n    PROB.loc[val_index, \"BIC\"] = choixbic.predict_proba(Xtest)[:,1]\n    ### aic\n    choixaic = lss.LogisticRegressionSelectionFeatureIC(start=[], \\\n        direction=\"forward\",crit=\"aic\").fit(Xapp,Yapp)\n    PROB.loc[val_index, \"AIC\"] = choixaic.predict_proba(Xtest)[:,1]\n    ### lasso\n    cr = StandardScaler()\n    Cs_lasso = grille(Xapp,Yapp, \"lasso\")\n    lassocv =  LogisticRegressionCV(cv=10, penalty=\"l1\", n_jobs=10,\\\n                 Cs=Cs_lasso,  solver=\"saga\", max_iter=2000)\n    pipe_lassocv = Pipeline(steps=[(\"cr\", cr), (\"lassocv\", lassocv)])\n    pipe_lassocv.fit(Xapp,Yapp)\n    PROB.loc[val_index,\"lasso\"] = pipe_lassocv.predict_proba(Xtest)[:,1]\n    ### elastic net\n    cr = StandardScaler()\n    Cs_enet = grille(Xapp,Yapp,\"enet\")\n    enetcv=LogisticRegressionCV(cv=10,penalty=\"elasticnet\",n_jobs=10,\\\n          l1_ratios=[0.5],Cs=Cs_enet,solver=\"saga\",max_iter=2000)\n    pipe_enetcv = Pipeline(steps=[(\"cr\", cr), (\"enetcv\", enetcv)])\n    pipe_enetcv.fit(Xapp,Yapp)\n    PROB.loc[val_index,\"elast\"] = pipe_enetcv.predict_proba(Xtest)[:,1] \n    ### ridge\n    cr = StandardScaler()\n    Cs_ridge = grille(Xapp,Yapp,\"ridge\")\n    ridgecv = LogisticRegressionCV(cv=10, penalty=\"l2\", \\\n            Cs=Cs_ridge,  max_iter=1000)\n    pipe_ridgecv = Pipeline(steps=[(\"cr\", cr), (\"ridgecv\", ridgecv)])\n    pipe_ridgecv.fit(Xapp,Yapp)\n    PROB.loc[val_index,\"ridge\"] = pipe_ridgecv.predict_proba(Xtest)[:,1]\n\n\nmc = pd.Series(0.0, index=PROB.columns[1:])\ns = 0.5\nfor i in range(mc.shape[0]):\n    mc.iloc[i] = sklm.zero_one_loss(PROB.Y, PROB.iloc[:,i+1]&gt;s)\n\nround(mc.sort_values(ascending=True),3)\n\nBIC      0.266\nlasso    0.266\nridge    0.271\nelast    0.277\nAIC      0.279\nlog      0.286\ndtype: float64",
    "crumbs": [
      "Codes",
      "IV Le modèle linéaire généralisé",
      "15 Comparaison des différentes méthodes en classification supervisée, étude de cas réels"
    ]
  },
  {
    "objectID": "codes/chap15.html#splines",
    "href": "codes/chap15.html#splines",
    "title": "15 Comparaison des différentes méthodes en classification supervisée, étude de cas réels",
    "section": "Splines",
    "text": "Splines\n\nnomsquanti = don.columns[np.isin(don.dtypes, [\"float64\", \\\n                          \"int64\"])].difference([\"Y\"])\nnomsquali = don.columns[np.isin(don.dtypes, [\"object\", \\\n                          \"category\"])].difference([\"Y\"])\nformule = \"~\" + \"+\".join(nomsquali)\nXspline = dmatrix(formule, don, return_type=\"dataframe\").\\\n                                           iloc[:,1:].to_numpy()\nfor i in nomsquanti:\n    xi = don.loc[:,i].quantile([0.25, 0.5, 0.75])\n    formule = \"-1 + bs(\" + i + \",knots=xi, degree=3)\"\n    BX = dmatrix(formule, don, return_type=\"dataframe\").to_numpy()\n    Xspline = np.concatenate((Xspline, BX), axis=1)\n\nXspline.shape\n\n(462, 49)\n\n\n\ncr = StandardScaler()\nlassocv =  LogisticRegressionCV(cv=10, penalty=\"l1\", n_jobs=10, Cs=Cs_lasso,  solver=\"saga\", max_iter=2000)\nenetcv = LogisticRegressionCV(cv=10, penalty=\"elasticnet\", l1_ratios = [0.5], n_jobs=10,  Cs=Cs_enet, solver=\"saga\", max_iter=2000)\nridgecv = LogisticRegressionCV(cv=10, penalty=\"l2\", n_jobs=10, Cs=Cs_ridge,  max_iter=1000)\npipe_lassocv = Pipeline(steps=[(\"cr\", cr), (\"lassocv\", lassocv)])\npipe_enetcv = Pipeline(steps=[(\"cr\", cr), (\"enetcv\", enetcv)])\npipe_ridgecv = Pipeline(steps=[(\"cr\", cr), (\"ridgecv\", ridgecv)])\n\nnb=10\nskf = StratifiedKFold(n_splits=nb, shuffle=True, random_state=1234)\n\n\nfor app_index, val_index in skf.split(X,Y):\n    Xapp = Xspline[app_index,:]\n    Xtest = Xspline[val_index,:]\n    Yapp = Y[app_index]\n    ### log\n    log = LogisticRegression(penalty=None,solver=\"newton-cholesky\").fit(Xapp,Yapp)\n    PROB.loc[val_index,\"log\"] = log.predict_proba(Xtest)[:,1]\n    ### bic\n    choixbic = lss.LogisticRegressionSelectionFeatureIC(start=[], \\\n        direction=\"forward\",crit=\"bic\").fit(Xapp,Yapp)\n    PROB.loc[val_index, \"BIC\"] = choixbic.predict_proba(Xtest)[:,1]\n    ### aic\n    choixaic = lss.LogisticRegressionSelectionFeatureIC(start=[], \\\n        direction=\"forward\",crit=\"aic\").fit(Xapp,Yapp)\n    PROB.loc[val_index, \"AIC\"] = choixaic.predict_proba(Xtest)[:,1]\n    ### lasso\n    cr = StandardScaler()\n    Cs_lasso = grille(Xapp,Yapp, \"lasso\")\n    lassocv =  LogisticRegressionCV(cv=10, penalty=\"l1\", n_jobs=10,\\\n                 Cs=Cs_lasso,  solver=\"saga\", max_iter=2000)\n    pipe_lassocv = Pipeline(steps=[(\"cr\", cr), (\"lassocv\", lassocv)])\n    pipe_lassocv.fit(Xapp,Yapp)\n    PROB.loc[val_index,\"lasso\"] = pipe_lassocv.predict_proba(Xtest)[:,1]\n    ### elastic net\n    cr = StandardScaler()\n    Cs_enet = grille(Xapp,Yapp,\"enet\")\n    enetcv=LogisticRegressionCV(cv=10,penalty=\"elasticnet\",n_jobs=10,\\\n          l1_ratios=[0.5],Cs=Cs_enet,solver=\"saga\",max_iter=2000)\n    pipe_enetcv = Pipeline(steps=[(\"cr\", cr), (\"enetcv\", enetcv)])\n    pipe_enetcv.fit(Xapp,Yapp)\n    PROB.loc[val_index,\"elast\"] = pipe_enetcv.predict_proba(Xtest)[:,1] \n    ### ridge\n    cr = StandardScaler()\n    Cs_ridge = grille(Xapp,Yapp,\"ridge\")\n    ridgecv = LogisticRegressionCV(cv=10, penalty=\"l2\", \\\n            Cs=Cs_ridge,  max_iter=1000)\n    pipe_ridgecv = Pipeline(steps=[(\"cr\", cr), (\"ridgecv\", ridgecv)])\n    pipe_ridgecv.fit(Xapp,Yapp)\n    PROB.loc[val_index,\"ridge\"] = pipe_ridgecv.predict_proba(Xtest)[:,1]\n\n\nmc = pd.Series(0.0, index=PROB.columns[1:])\ns = 0.5\nfor i in range(mc.shape[0]):\n    mc.iloc[i] = sklm.zero_one_loss(PROB.Y, PROB.iloc[:,i+1]&gt;s)\n\nround(mc.sort_values(ascending=True),3)\n\nridge    0.279\nAIC      0.286\nelast    0.290\nlog      0.292\nlasso    0.297\nBIC      0.305\ndtype: float64",
    "crumbs": [
      "Codes",
      "IV Le modèle linéaire généralisé",
      "15 Comparaison des différentes méthodes en classification supervisée, étude de cas réels"
    ]
  },
  {
    "objectID": "correction/chap12.html",
    "href": "correction/chap12.html",
    "title": "Régression logistique",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom statsmodels.stats.anova import anova_lm\nimport matplotlib.pyplot as plt\nfrom scipy.stats import chi2,norm\nfrom scipy import interpolate\n\n\nExercice 1 (Questions de cours)  \n\nA\nA\nB\nA\nA\nA\nB\nA\n\n\n\nExercice 2 (Interprétation des coefficients)  \n\nOn génère l’échantillon.\n\nn = 100\nnp.random.seed(48967365)\nX = np.random.choice(['A', 'B', 'C'], size=n, replace=True)\nY = np.zeros(n, dtype=int)\nnp.random.seed(487365)\nY[X == 'A'] = np.random.binomial(1, 0.95, size=np.sum(X == 'A'))\nnp.random.seed(4878365)\nY[X == 'B'] = np.random.binomial(1, 0.95, size=np.sum(X == 'B'))\nnp.random.seed(4653965)\nY[X == 'C'] = np.random.binomial(1, 0.05, size=np.sum(X == 'C'))\ndonnees = pd.DataFrame({'Y': Y, 'X': X})\nprint(donnees.head())\n\n   Y  X\n0  1  A\n1  0  C\n2  1  A\n3  1  B\n4  0  A\n\n\nOn ajuste le modèle avec les contraintes par défaut.\n\nmodel1 = smf.logit('Y ~ X', data=donnees).fit()\nprint(model1.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.297451\n         Iterations 7\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                      Y   No. Observations:                  100\nModel:                          Logit   Df Residuals:                       97\nMethod:                           MLE   Df Model:                            2\nDate:                Tue, 04 Feb 2025   Pseudo R-squ.:                  0.5521\nTime:                        12:22:32   Log-Likelihood:                -29.745\nconverged:                       True   LL-Null:                       -66.406\nCovariance Type:            nonrobust   LLR p-value:                 1.197e-16\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      1.9459      0.478      4.070      0.000       1.009       2.883\nX[T.B]         0.5798      0.877      0.661      0.508      -1.138       2.298\nX[T.C]        -4.6868      0.872     -5.373      0.000      -6.396      -2.977\n==============================================================================\n\n\nOn obtient les résultats du test de Wald sur la nullité des paramètres \\(\\beta_0,\\beta_2\\) et \\(\\beta_3\\).\nOn change la modalité de référence.\n\nmodel2 = smf.logit('Y ~ C(X, Treatment(reference=\"C\"))', data=donnees).fit()\nprint(model2.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.297451\n         Iterations 7\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                      Y   No. Observations:                  100\nModel:                          Logit   Df Residuals:                       97\nMethod:                           MLE   Df Model:                            2\nDate:                Tue, 04 Feb 2025   Pseudo R-squ.:                  0.5521\nTime:                        12:22:32   Log-Likelihood:                -29.745\nconverged:                       True   LL-Null:                       -66.406\nCovariance Type:            nonrobust   LLR p-value:                 1.197e-16\n=======================================================================================================\n                                          coef    std err          z      P&gt;|z|      [0.025      0.975]\n-------------------------------------------------------------------------------------------------------\nIntercept                              -2.7408      0.730     -3.757      0.000      -4.171      -1.311\nC(X, Treatment(reference=\"C\"))[T.A]     4.6868      0.872      5.373      0.000       2.977       6.396\nC(X, Treatment(reference=\"C\"))[T.B]     5.2666      1.035      5.086      0.000       3.237       7.296\n=======================================================================================================\n\n\nOn obtient les résultats du test de Wald sur la nullité des paramètres \\(\\beta_0,\\beta_1\\) et \\(\\beta_2\\).\nOn remarque que dans model1 on accepte la nullité de \\(\\beta_2\\) alors qu’on la rejette dans model2. Ceci est logique dans la mesure où ces tests dépendent de la contrainte identifiante choisie. Dans model1 le test de nullité de \\(\\beta_2\\) permet de vérifier si \\(B\\) à un effet similaire à \\(A\\) sur \\(Y\\). Dans model2, on compare l’effet de \\(B\\) à celui de \\(C\\). On peut donc conclure \\(A\\) et \\(B\\) ont des effets proches sur \\(Y\\) alors que \\(B\\) et \\(C\\) ont un impact différent. Ceci est logique vu la façon dont les données ont été générées.\nTester l’effet global de \\(X\\) sur \\(Y\\) revient à tester si les coefficients \\(\\beta_1,\\beta_2\\) et \\(\\beta_3\\) sont égaux, ce qui, compte tenu des contraintes revient à considérer les hypothèses nulles :\n\n\\(\\beta_2=\\beta_3=0\\) dans model1 ;\n\\(\\beta_1=\\beta_2=0\\) dans model2.\n\nOn peut effectuer les tests de Wald ou du rapport de vraisemblance. On obtient les résultats du rapport de vraisemblance avec :\n\nlr_test_model1 = model1.llr\nlr_test_model2 = model2.llr\n\nprint(\"Test de rapport de vraisemblance pour model1:\")\nprint(f\"LR stat: {lr_test_model1:.4f}, p-value: {model1.llr_pvalue:.4f}\")\n\nprint(\"\\nTest de rapport de vraisemblance pour model2:\")\nprint(f\"LR stat: {lr_test_model2:.4f}, p-value: {model2.llr_pvalue:.4f}\")\n\nTest de rapport de vraisemblance pour model1:\nLR stat: 73.3227, p-value: 0.0000\n\nTest de rapport de vraisemblance pour model2:\nLR stat: 73.3227, p-value: 0.0000\n\n\nOn remarque ici que ces deux tests sont identiques : ils ne dépendent pas de la contrainte identifiante choisie.\n\n\n\nExercice 3 (Séparabilité)  \n\nOn génère l’échantillon demandé.\n\nnp.random.seed(1234)\nX = np.concatenate([np.random.uniform(-1, 0, 50), np.random.uniform(0, 1, 50)])\nY = np.concatenate([np.zeros(50), np.ones(50)])\ndf = pd.DataFrame({'X': X, 'Y': Y})\nprint(df.head())\n\n          X    Y\n0 -0.808481  0.0\n1 -0.377891  0.0\n2 -0.562272  0.0\n3 -0.214641  0.0\n4 -0.220024  0.0\n\n\nLe graphe s’obtient avec :\n\nbeta = np.arange(0, 100, 0.01)\ndef log_vrais(X, Y, beta):\n    LV = np.zeros(len(beta))\n    for i in range(len(beta)):\n        Pbeta = np.exp(beta[i] * X) / (1 + np.exp(beta[i] * X))\n        LV[i] = np.sum(Y * X * beta[i] - np.log(1 + np.exp(X * beta[i])))\n    return LV\nLL = log_vrais(df['X'], df['Y'], beta)\n\n\nplt.plot(beta,LL)\nplt.xlabel('beta')\nplt.ylabel('Log-vraisemblance')\nplt.title('Log-vraisemblance en fonction de beta')\nplt.show()\n\n\n\n\n\n\n\n\nOn obtient un avertissement qui nous dit que l’algorithme d’optimisation n’a pas convergé.\n\nmodel = smf.logit('Y ~ X - 1', data=df).fit()\nprint(model.params)\n\nWarning: Maximum number of iterations has been exceeded.\n         Current function value: 0.000000\n         Iterations: 35\nX    1801.824972\ndtype: float64\n\n\n/opt/miniconda3/lib/python3.12/site-packages/statsmodels/discrete/discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n  return 1/(1+np.exp(-X))\n/opt/miniconda3/lib/python3.12/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n  warnings.warn(\"Maximum Likelihood optimization failed to \"\n\n\nLe changement proposé supprime la séparabilité des données. On obtient bien un maximum fini pour cette nouvelle vraisemblance.\n\nY1 = Y.copy()\nY1[0] = 1\nLL1 = log_vrais(X, Y1, beta)\nplt.plot(beta, LL1)\nplt.xlabel('beta')\nplt.ylabel('Log-vraisemblance')\nplt.title('Log-vraisemblance en fonction de beta pour Y1')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nExercice 4 (Matrice hessienne) Le gradient de la log-vraisemblance en \\(\\beta\\) est donné par \\(\\nabla \\mathcal L(Y,\\beta)=X'(Y-P_\\beta)\\). Sa \\(j\\)ème composante vaut \\[\\frac{\\partial\\mathcal L}{\\partial\\beta_j}(\\beta)=\\sum_{i=1}^nx_{ij}(y_i-p_\\beta(x_i)).\\]\nOn peut donc calculer la drivée par rapport à \\(\\beta_\\ell\\) : \\[\\begin{align*}\n\\frac{\\partial\\mathcal L}{\\partial\\beta_j\\partial\\beta_\\ell}(\\beta)= & \\frac{\\partial}{\\partial\\beta_\\ell}\\left[\n\\sum_{i=1}^nx_{ij}\\left(y_i-\\frac{\\exp(x_i'\\beta)}{1+\\exp(x_i'\\beta)}\\right)\\right] \\\\\n=& -\\sum_{i=1}^nx_{ij}x_{i\\ell}\\frac{\\exp(x_i'\\beta)}{[1+\\exp(x_i'\\beta)]^2} \\\\\n=& -\\sum_{i=1}^nx_{ij}x_{i\\ell}p_\\beta(x_i)(1-p_\\beta(x_i)).\n\\end{align*}\\] Matriciellement on déduit donc que la hessienne vaut \\[\\nabla^2\\mathcal L(Y,\\beta)=-X'W_\\beta X,\\] où \\(W_\\beta\\) est la matrice \\(n\\times n\\) diagonale dont le \\(i\\)ème terme de la diagonale vaut \\(p_\\beta(x_i)(1-p_\\beta(x_i))\\). Par ailleurs, comme pour tout \\(i=1,\\dots,n\\), on a \\(p_\\beta(x_i)(1-p_\\beta(x_i))&gt;0\\) et que \\(X\\) est de plein rang, on déduit que \\(X'W_\\beta X\\) est définie positive et par conséquent que la hessienne est définie négative.\n\n\nExercice 5 (Modèles avec R) On importe les données :\n\npanne = pd.read_csv(\"../donnees/panne.txt\", sep=\" \")\nprint(panne.head())\n\n   etat  age marque\n0     0    4      A\n1     0    2      C\n2     0    3      C\n3     0    9      B\n4     0    7      B\n\n\n\nLa commande\n\nmodel = smf.logit('etat ~ age+marque', data=panne).fit()\n\nOptimization terminated successfully.\n         Current function value: 0.659124\n         Iterations 5\n\n\najuste le modèle \\[\\log\\left(\\frac{p_\\beta(x)}{1-p_\\beta(x)}\\right)=\\beta_0+\\beta_1x_1+\\beta_2\\mathsf{1}_{x_2=B}+\\beta_3\\mathsf{1}_{x_2=C}\\] où \\(x_1\\) et \\(x_2\\) désigne respectivement les variables age et marque. On obtient les estimateurs avec\n\nprint(model.summary())\n\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                   etat   No. Observations:                   33\nModel:                          Logit   Df Residuals:                       29\nMethod:                           MLE   Df Model:                            3\nDate:                Tue, 04 Feb 2025   Pseudo R-squ.:                 0.04845\nTime:                        12:22:41   Log-Likelihood:                -21.751\nconverged:                       True   LL-Null:                       -22.859\nCovariance Type:            nonrobust   LLR p-value:                    0.5290\n===============================================================================\n                  coef    std err          z      P&gt;|z|      [0.025      0.975]\n-------------------------------------------------------------------------------\nIntercept       0.4781      0.833      0.574      0.566      -1.155       2.111\nmarque[T.B]    -0.4194      0.814     -0.515      0.607      -2.015       1.177\nmarque[T.C]    -1.4561      1.054     -1.382      0.167      -3.521       0.609\nage             0.0139      0.094      0.148      0.883      -0.170       0.198\n===============================================================================\n\n\nIl s’agit des tests de Wald pour tester l’effet des variables dans le modèle. Pour l’effet de marque, on va par exemple tester \\[H_0:\\beta_2=\\beta_3=0\\quad\\text{contre}\\quad H_1:\\beta_2\\neq 0\\text{ ou }\\beta_3\\neq 0.\\] Sous \\(H_0\\) la statistique de Wald suit une loi du \\(\\chi^2\\) à 4-2=2 degrés de liberté. Pour le test de la variable age le nombre de degrés de liberté manquant est 1. On retrouve cela dans la sortie\n\nwald_tests = model.wald_test_terms()\nprint(\"Tests de Wald pour chaque coefficient:\")\nprint(wald_tests)\n\nTests de Wald pour chaque coefficient:\n                              chi2              P&gt;chi2  df constraint\nIntercept   [[0.3293833811483461]]  0.5660224013616977              1\nmarque      [[1.9306493813691281]]  0.3808595181236274              2\nage        [[0.02182485519152971]]  0.8825539788968159              1\n\n\n\n\n\n\nLe modèle s’écrit \\[\\log\\left(\\frac{p_\\beta(x)}{1-p_\\beta(x)}\\right)=\\beta_0+\\beta_1\\mathsf{1}_{x_2=A}+\\beta_2\\mathsf{1}_{x_2=B}.\\]\nLe modèle ajusté ici est \\[\n\\log\\left(\\frac{p_\\beta(x)}{1-p_\\beta(x)}\\right)=\\gamma_0+\\gamma_1\\mathsf{1}_{x_2=B}+\\gamma_2\\mathsf{1}_{x_2=C}.\n\\] Par identification on a \\[\n\\begin{cases}\n    \\beta_0+\\beta_1=\\gamma_0 \\\\\n    \\beta_0+\\beta_2=\\gamma_0+\\gamma_1 \\\\\n    \\beta_0=\\gamma_0+\\gamma_2 \\\\\n    \\end{cases}\n    \\Longleftrightarrow\n    \\begin{cases}\n    \\beta_0=\\gamma_0+\\gamma_2 \\\\\n    \\beta_1=-\\gamma_2 \\\\\n    \\beta_2=\\gamma_1-\\gamma_2 \\\\\n    \\end{cases}\n    \\Longrightarrow\n    \\begin{cases}\n    \\widehat\\beta_0=-0.92 \\\\\n    \\widehat\\beta_1=1.48 \\\\\n    \\widehat\\beta_2=1.05 \\\\\n    \\end{cases}\n\\] On peut retrouver ces résultats avec\n\nmodel1 = smf.logit('etat ~ C(marque, Treatment(reference=\"C\"))', data=panne).fit()\nmodel1.summary()\n\nOptimization terminated successfully.\n         Current function value: 0.659456\n         Iterations 5\n\n\n\nLogit Regression Results\n\n\nDep. Variable:\netat\nNo. Observations:\n33\n\n\nModel:\nLogit\nDf Residuals:\n30\n\n\nMethod:\nMLE\nDf Model:\n2\n\n\nDate:\nTue, 04 Feb 2025\nPseudo R-squ.:\n0.04798\n\n\nTime:\n12:22:41\nLog-Likelihood:\n-21.762\n\n\nconverged:\nTrue\nLL-Null:\n-22.859\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\n0.3340\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-0.9163\n0.837\n-1.095\n0.273\n-2.556\n0.724\n\n\nC(marque, Treatment(reference=\"C\"))[T.A]\n1.4759\n1.045\n1.412\n0.158\n-0.573\n3.525\n\n\nC(marque, Treatment(reference=\"C\"))[T.B]\n1.0498\n0.984\n1.067\n0.286\n-0.878\n2.978\n\n\n\n\n\n\nIl y a interaction si l’age agit différemment sur la panne en fonction de la marque.\nLe modèle ajusté sur est \\[\n\\log\\left(\\frac{p_\\beta(x)}{1-p_\\beta(x)}\\right)=\\delta_0+\\delta_1\\mathsf{1}_{x_2=B}+\\delta_2\\mathsf{1}_{x_2=C}+\\delta_3x_1+\\delta_4x_1\\mathsf{1}_{x_2=B}+\\delta_5x_1\\mathsf{1}_{x_2=C}.\n\\]\nOn obtient ainsi par identification : \\[\n\\begin{cases}\n\\alpha_0=\\delta_0\\\\\n\\alpha_1=\\delta_3\\\\\n\\beta_0=\\delta_0+\\delta_1\\\\\n\\beta_1=\\delta_3+\\delta_4\\\\\n\\gamma_0=\\delta_0+\\delta_2\\\\\n\\gamma_1=\\delta_3+\\delta_5\n\\end{cases}\n\\]\nOn déduit les valeurs des estimateurs que l’on peut retrouver avec la commande :\n\nmodel2 = smf.logit('etat ~ -1 + marque + marque:age', data=panne).fit()\nprint(model2.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.645772\n         Iterations 5\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                   etat   No. Observations:                   33\nModel:                          Logit   Df Residuals:                       27\nMethod:                           MLE   Df Model:                            5\nDate:                Tue, 04 Feb 2025   Pseudo R-squ.:                 0.06773\nTime:                        12:22:41   Log-Likelihood:                -21.310\nconverged:                       True   LL-Null:                       -22.859\nCovariance Type:            nonrobust   LLR p-value:                    0.6851\n=================================================================================\n                    coef    std err          z      P&gt;|z|      [0.025      0.975]\n---------------------------------------------------------------------------------\nmarque[A]         0.2351      1.047      0.225      0.822      -1.817       2.288\nmarque[B]         0.4337      0.889      0.488      0.626      -1.308       2.176\nmarque[C]        -2.1963      2.008     -1.094      0.274      -6.131       1.739\nmarque[A]:age     0.0564      0.150      0.377      0.706      -0.237       0.350\nmarque[B]:age    -0.0555      0.133     -0.416      0.677      -0.317       0.206\nmarque[C]:age     0.2723      0.367      0.742      0.458      -0.447       0.992\n=================================================================================\n\n\n\n\n\nExercice 6 (Interprétation)  \n\ndf = pd.read_csv(\"../donnees/logit_exo6.csv\")\n\n# Ajuster le modèle de régression logistique avec toutes les variables explicatives\nmod = smf.logit('Y ~ X1+X2', data=df).fit()\n\n# Ajuster le modèle de régression logistique avec seulement la variable X1\nmod1 = smf.logit('Y ~ X1', data=df).fit()\n\n# Afficher le résumé des modèles\nprint(mod.summary())\nprint(mod1.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.618257\n         Iterations 6\nOptimization terminated successfully.\n         Current function value: 0.692991\n         Iterations 3\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                      Y   No. Observations:                  500\nModel:                          Logit   Df Residuals:                      497\nMethod:                           MLE   Df Model:                            2\nDate:                Tue, 04 Feb 2025   Pseudo R-squ.:                  0.1080\nTime:                        12:22:41   Log-Likelihood:                -309.13\nconverged:                       True   LL-Null:                       -346.57\nCovariance Type:            nonrobust   LLR p-value:                 5.469e-17\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -0.5787      0.119     -4.852      0.000      -0.812      -0.345\nX1            -0.1947      0.066     -2.970      0.003      -0.323      -0.066\nX2             0.3190      0.044      7.244      0.000       0.233       0.405\n==============================================================================\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                      Y   No. Observations:                  500\nModel:                          Logit   Df Residuals:                      498\nMethod:                           MLE   Df Model:                            1\nDate:                Tue, 04 Feb 2025   Pseudo R-squ.:               0.0002259\nTime:                        12:22:41   Log-Likelihood:                -346.50\nconverged:                       True   LL-Null:                       -346.57\nCovariance Type:            nonrobust   LLR p-value:                    0.6923\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.0011      0.089      0.012      0.990      -0.174       0.177\nX1            -0.0205      0.052     -0.396      0.692      -0.122       0.081\n==============================================================================\n\n\nOn remarque que la nullité du paramètre associé à X1 est accepté dans le modèle avec uniquement X1 alors qu’elle est refusée lorsqu’on considère X1 et X2 dans le modèle.\n\n\nExercice 7 (Tests à la main)  \n\nLe modèle s’écrit \\[log\\left(\\frac{p_\\beta(x)}{1-p_\\beta(x)}\\right)=\\beta_0+\\beta_1x.\\]\nLa log vraisemblance s’obtient avec\n\np = np.array([0.76, 0.4, 0.6, 0.89, 0.35])\nY = np.array([1, 0, 0, 1, 1])\nL1 = np.log(np.prod(p**Y * (1-p)**(1-Y)))\nprint(L1)\n\n-2.867909142096535\n\n\n\nOn calcule les écart-type des estimateurs\n\np = np.array([0.76, 0.4, 0.6, 0.89, 0.35])\nX1 = np.array([0.47, -0.55, -0.01, 1.07, -0.71])\nX = np.column_stack((np.ones(5), X1))\nW = np.diag(p * (1 - p))\nSIG = np.linalg.inv(X.T @ W @ X)\nsig = np.sqrt(np.diag(SIG))\nprint(sig)\n\n[1.0232524  1.74493535]\n\n\nOn en déduit les statistiques de test :\n\nbeta = np.array([0.4383, 1.5063])\nresult = beta / sig\nprint(result)\n\n[0.42834006 0.86324115]\n\n\nOn peut faire le test de Wald et du rapport de vraisemblance.\nLa statistique de test vaut 0.8632411, on obtient donc la probabilité critique\n\n2 * (1 - norm.cdf(0.8632411))\n\n0.3880049206652636\n\n\nOn peut également effectuer un test du rapport de vraisemblance. Le modèle null sans X1 a pour log-vraisemblance\n\np0 = 3 / 5\nY = np.array([1, 0, 0, 1, 1])\nL0 = np.log(np.prod(p0**Y * (1-p0)**(1-Y)))\nprint(L0)\n\n-3.365058335046282\n\n\nLa statistique de test vaut donc\n\n2*(L1-L0)\n\n0.9942983858994943\n\n\net la probabilité critique est égale à\n\n1 - chi2.cdf(2*(L1-L0), df=1)\n\n0.31869407584847365\n\n\nOn peut retrouver (aux arrondis près) les résultats de l’exercice avec\n\nX = [0.47, -0.55, -0.01, 1.07, -0.71]\nY = [1, 0, 0, 1, 1]\ndf = pd.DataFrame({'X': X, 'Y': Y})\nmodel = smf.logit('Y ~ X', data=df).fit()\nlog_likelihood = model.llf\nprint(log_likelihood)\n\nOptimization terminated successfully.\n         Current function value: 0.579753\n         Iterations 6\n-2.898765148750008\n\n\n\nwald_test = model.wald_test_terms()\nprint(wald_test)\n\n                             chi2               P&gt;chi2  df constraint\nIntercept  [[0.1845505636560877]]   0.6674913759958387              1\nX           [[0.755071501627431]]  0.38487529817766963              1\n\n\n\nlr_test_model = model.llr\nprint(f\"LR stat: {lr_test_model:.4f}, p-value: {model.llr_pvalue:.4f}\")\n\nLR stat: 0.9326, p-value: 0.3342\n\n\n\n\n\n\nExercice 8 (Vraisemblance du modèle saturé)  \n\nLes variables \\((y_t,t=1,\\dots,y_T)\\) étant indépendantes et de loi binomiales \\(B(n_t,p_t)\\), la log-vraisemblance est donnée par \\[\\begin{align*}\n\\mathcal L_{\\text{sat}}(Y,p)= & \\log\\left(\\prod_{t=1}^T\n\\begin{pmatrix}\nn_t\\\\\n\\tilde y_t\n\\end{pmatrix}\np_t^{\\tilde y_t}(1-p_t)^{n_t-\\tilde y_t}\\right) \\\\\n= &\n\\sum_{t=1}^T\\left(\\log\n\\begin{pmatrix}\nn_t\\\\\n\\tilde y_t\n\\end{pmatrix}\n+\\tilde y_t\\log(p_t)+(n_t-\\tilde y_t)\\log(1-p_t)\\right)\n\\end{align*}\\]\nLa dérivée de la log-vraisemblance par rapport à \\(p_t\\) s’écrit \\[\\frac{\\tilde y_t}{p_t}-\\frac{n_t-\\tilde y_t}{1-p_t}.\\] Cette dérivée s’annule pour \\[\\widehat p_t=\\frac{\\tilde y_t}{n_t}.\\]\nOn note \\(\\widehat \\beta\\) l’EMV du modèle logistique et \\(p_{\\widehat\\beta}\\) le vecteur qui contient les valeurs ajustées \\(p_{\\widehat\\beta}(x_t),t=1,\\dots,T\\). On a pour tout \\(\\beta\\in\\mathbb R^p\\) : \\[\\mathcal L(Y,\\beta)\\leq\\mathcal L(Y,\\widehat\\beta)=\\mathcal L_{\\text{sat}}(Y,p_{\\widehat\\beta})\\leq L_{\\text{sat}}(Y,\\widehat p_t).\\]\n\n\n\nExercice 9 (Résidus partiels)  \n\n\nartere = pd.read_csv('../donnees/artere.txt', header=0, index_col=0, sep=' ')\nmodele = smf.glm('chd~age', data=artere, family=sm.families.Binomial()).fit()\nB0 = modele.params\nOriginalDeviance = modele.deviance\n\n\nalpha=0.05\n\n::: {#496f8de4 .cell execution_count=31} {.python .cell-code}   stderr = modele.cov_params().iloc[1,1]**0.5   from scipy import stats   delta = stats.chi2.ppf(1-alpha/4, df=1)**0.5*stderr/5   grille = B0[1] + np.arange(-10,11)*delta :::\nOn a \\[\\begin{align*}\n\\mathcal D_1&=-2(\\mathcal L(Y,\\hat\\beta)-\\mathcal L_{sat})\n  \\end{align*}\\] Pour celle avec l’offset \\(K_i=x_i\\beta_2^*\\) elle vaut \\[\\begin{align*}\n\\mathcal D_o&=-2(\\mathcal L(Y,K,\\hat\\beta_1)-\\mathcal L_{sat})\n  \\end{align*}\\] où \\(\\hat \\beta_1\\) maximise \\(\\mathcal L(Y,K,\\hat\\beta_1)\\) c’est à dire \\(\\mathcal L(Y,K,\\hat\\beta_1)=l(\\beta_2^*)\\) et nous avons donc \\[\\begin{align*}\n\\mathcal D_o - \\mathcal D_1= 2(\\mathcal L(Y,\\hat\\beta)-\\mathcal L(Y,K,\\beta_1)= 2(\\mathcal L(Y,\\hat\\beta)-l(\\beta_2^*))=P(\\beta_2^*).\n  \\end{align*}\\]\n\nprofil2 = []\nfor valgrille in grille:\n    modeleo = smf.glm('chd~1', data=artere, offset=artere.age*valgrille, family=sm.families.Binomial()).fit()\n    if (modeleo.deviance -  OriginalDeviance)&lt;0:\n        profil2.append(0)\n    else:\n        profil2.append(modeleo.deviance -  OriginalDeviance)    \n\n\nprofil = np.sign(np.arange(-10,11))*np.sqrt(profil2)\n\n::: {#79a2dfd6 .cell execution_count=34} {.python .cell-code}   f = interpolate.interp1d(profil, grille)   xnew = [-np.sqrt(stats.chi2.ppf(1-alpha, df=1)),           np.sqrt(stats.chi2.ppf(1-alpha, df=1)) ]   print(f(xnew))\n::: {.cell-output .cell-output-stdout} [0.06697424 0.16204287] ::: :::\n\nprint(modele.conf_int().iloc[1,:])\n\n0    0.063765\n1    0.158078\nName: age, dtype: float64\n\n\n\n\n\n\n\n Retour au sommet",
    "crumbs": [
      "Corrections",
      "IV Le modèle linéaire généralisé",
      "Régression logistique"
    ]
  }
]