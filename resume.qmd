---
title: "Résumé"
---

Le livre se décompose en **quatre parties**, chacune constituée de *deux à cinq chapitres*.

1.  La première pose les fondamentaux du **problème de régression** et montre, à travers quelques
exemples, comment on peut l'aborder à l'aide d'un *modèle linéaire simple* d'abord, puis *multiple*.
Les problèmes d'estimation ainsi que la géométrie associée à la méthode des *moindres carrés*
sont proposés dans les deux premiers chapitres de cette partie.
Le troisième chapitre propose les principaux diagnostics qui permettent de s'assurer de la validité du modèle
tandis que le dernier présente quelques stratégies à envisager lorsque les hypothèses classiques
du modèle linéaire ne sont pas vérifiées. Les quatrième et cinquième chapitre abordent des extensions
du modèle linéaire comme la corrélation des erreurs et les régressions polynomiales et splines.

2.  La seconde partie aborde la partie **inférentielle**. Il s'agit d'une des parties les plus techniques 
et calculatoires de l'ouvrage. Cette partie permet, entre autres, d'exposer précisément les
procédures de *tests* et de *construction d'intervalles de confiance* dans le modèle linéaire. Elle
décrit également les spécificités engendrées par l'utilisation de variables qualitatives dans ce modèle.


3.  La troisième partie est consacrée à un problème désormais courant en régression : la
**réduction de la dimension**. En effet, face à l'augmentation conséquente des données,
nous sommes de plus en plus confrontés à des problèmes où le nombre de variables est (très) grand.
Les techniques standards appliquées à ce type de données se révèlent souvent peu performantes et
il est nécessaire de trouver des alternatives. Nous présentons tout d'abord les techniques classiques
de choix de variables qui consistent à se donner un critère de performance et à rechercher
à l'aide de *procédures exhaustives* ou *pas à pas* le sous-groupe de variables qui optimise le critère donné.
Nous présentons ensuite les *approches régularisées* de type Ridge-Lasso qui consistent à
trouver les estimateurs qui optimisent le critère des *moindres carrés pénalisés* par une fonction de la norme des
paramètres. Le troisième chapitre propose de faire la régression non pas sur les variables initiales mais
sur des *combinaisons linéaires* de celles-ci. Nous insistons sur la *régression sur composantes principales (PCR)* et la *régression Partial Least Square (PLS)*.
A ce stade, nous disposons de plusieurs
algorithmes qui répondent à un même problème de régression. Il devient
important de se donner une méthode qui permette d'en *choisir un automatiquement$
(on ne laisse pas l'utilisateur décider, ce sont les données qui doivent choisir).
Nous proposons un protocole basé sur la minimisation de risques empiriques
calculés par des algorithmes de type validation croisée qui permet de choisir
l'algorithme le plus approprié pour un problème donné. 

4.  Dans la quatrième partie,  nous présentons
le **modèle linéaire généralisé**. Cette partie généralise les modèles
initiaux, qui permettaient de traiter uniquement le cas d'une variable à expliquer
continue, à des variables à expliquer *binaire
(régression logistique)* ou de *comptage (régression de Poisson)*.
Nous insistons uniquement sur les spécificités associées à ces types de variables,
la plupart des concepts étudiés précédemment s'adaptent directement à ces cas nouveaux.


